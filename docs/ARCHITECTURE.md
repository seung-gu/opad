# ì•„í‚¤í…ì²˜ ë¬¸ì„œ: 3-Service ë¶„ë¦¬

## ğŸ¯ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### System Overview

```mermaid
graph TB
    Web[Web<br/>Next.js] -->|HTTP| API[API<br/>FastAPI]
    API -->|RPUSH| Redis[(Redis<br/>Queue + Status)]
    Redis -->|BLPOP| Worker[Worker<br/>Python]
    Worker -->|Execute| CrewAI[CrewAI]
    Worker -->|Save| MongoDB[(MongoDB<br/>Article + Vocabulary)]
    
    API -.->|SET/GET| Redis
    Worker -.->|SET| Redis
    
    API -.->|utils/llm.py<br/>utils/prompts.py| OpenAI
    
    Web -->|HTTP<br/>Proxy| API
    API -->|Save/Query| MongoDB
    
    style Web fill:#2196F3
    style API fill:#2196F3
    style Worker fill:#2196F3
    style CrewAI fill:#10a37f
    style Redis fill:#dc382d
    style MongoDB fill:#13aa52
    style OpenAI fill:#10a37f
    
    linkStyle 0 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 1 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 2 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 5 stroke:#ff9500,stroke-width:2px,color:#ff9500
    linkStyle 6 stroke:#13aa52,stroke-width:2px,color:#13aa52
    linkStyle 7 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
    linkStyle 8 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
```

### Article Generation íë¦„

```mermaid
sequenceDiagram
    participant Web
    participant API
    participant Redis
    participant Worker
    participant CrewAI
    participant MongoDB
    
    Web->>API: POST /articles/generate
    API->>MongoDB: Check duplicates
    API->>MongoDB: Save article metadata
    API->>Redis: RPUSH job
    API-->>Web: Return job_id + article_id
    
    Worker->>Redis: BLPOP
    Redis-->>Worker: job_data
    
    Worker->>CrewAI: Execute crew.kickoff()
    CrewAI-->>Worker: Return article
    
    Worker->>MongoDB: Save article content
    Worker->>Redis: Update status (completed)
```

**íŠ¹ì§•:**
- **ì‹¤ì‹œê°„ ì‘ë‹µ**: ì‚¬ìš©ìê°€ ë‹¨ì–´ë¥¼ í´ë¦­í•˜ë©´ ì¦‰ì‹œ ì •ì˜ ë°˜í™˜ (ë¹„ë™ê¸° í ì‚¬ìš© ì•ˆ í•¨)
- **í”„ë¡ì‹œ íŒ¨í„´**: Next.js API routeê°€ FastAPIë¡œ ìš”ì²­ì„ í”„ë¡ì‹œ
- **ê³µí†µ ìœ í‹¸ ì‚¬ìš©**: `utils/llm.py`ì™€ `utils/prompts.py`ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
- **ì—ëŸ¬ ì²˜ë¦¬**: `get_llm_error_response()`ë¡œ ì¼ê´€ëœ ì—ëŸ¬ ì‘ë‹µ

### ì„œë¹„ìŠ¤ ê°„ í†µì‹ 

| From | To | Method | Purpose |
|------|-----|--------|---------|
| **Web** | **API** | HTTP | Article ìƒì„±, Job enqueue, Token usage ì¡°íšŒ |
| **Web** | **Next.js API** | HTTP | Dictionary API ìš”ì²­ (í”„ë¡ì‹œ), Vocabulary CRUD ìš”ì²­ (í”„ë¡ì‹œ), Dictionary Stats ìš”ì²­ (í”„ë¡ì‹œ) |
| **Next.js API** | **API** | HTTP | Dictionary API í”„ë¡ì‹œ ìš”ì²­, Vocabulary CRUD í”„ë¡ì‹œ ìš”ì²­, Dictionary Stats í”„ë¡ì‹œ ìš”ì²­ |
| **API** | **MongoDB** | (via utils.mongodb) | ì¤‘ë³µ ì²´í¬, Article metadata ì €ì¥/ì¡°íšŒ, Vocabulary ì €ì¥/ì¡°íšŒ, Token usage ì €ì¥/ì¡°íšŒ |
| **API** | **Redis** | `RPUSH` | Jobì„ íì— ì¶”ê°€ |
| **API** | **Redis** | `SET/GET` | Job ìƒíƒœ ì €ì¥/ì¡°íšŒ (ê³µí†µ ëª¨ë“ˆ `api.job_queue` ì‚¬ìš©) |
| **API** | **LLM** | HTTP (via utils.llm) | Dictionary APIìš© LLM í˜¸ì¶œ (lemma, definition, related_words) + Token tracking |
| **API** | **API** | Internal | Token usage endpoints (`/usage/me`, `/usage/articles/{id}`) |
| **Worker** | **Redis** | `BLPOP` | Jobì„ íì—ì„œ êº¼ëƒ„ (blocking) |
| **Worker** | **Redis** | `SET` | Job ìƒíƒœ ì—…ë°ì´íŠ¸ (ê³µí†µ ëª¨ë“ˆ `api.job_queue` ì‚¬ìš©) |
| **Worker** | **CrewAI** | Function Call | Article ìƒì„± |
| **Worker** | **MongoDB** | (via utils.mongodb) | Article content ì €ì¥, Token usage ì €ì¥ |

**ì°¸ê³ **: APIì™€ Worker ëª¨ë‘ `api.job_queue` ëª¨ë“ˆì„ í†µí•´ Redisì— ì ‘ê·¼í•©ë‹ˆë‹¤. MongoDB ì ‘ê·¼ì€ `utils.mongodb` ëª¨ë“ˆì„ í†µí•´ í•©ë‹ˆë‹¤.

### Redis ë°ì´í„° êµ¬ì¡°

#### 1. Job Queue (List) - `opad:jobs`

**ìš©ë„**: Workerê°€ ì²˜ë¦¬í•  jobë“¤ì„ FIFO ìˆœì„œë¡œ ì €ì¥

```
Queue: opad:jobs (List)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [oldest] â† ... â† [newest]       â”‚
â”‚    â†‘                    â†‘       â”‚
â”‚  BLPOP              RPUSH       â”‚
â”‚ (Worker)             (API)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë°ì´í„° í˜•ì‹**:
```json
{
  "job_id": "uuid",
  "article_id": "uuid",
  "inputs": {
    "language": "Korean",
    "level": "B1",
    "length": "300",
    "topic": "Climate Change"
  },
  "created_at": "2025-01-08T12:34:56.789Z"
}
```

#### 2. Job Status (String) - `opad:job:{job_id}`

**ìš©ë„**: ê° jobì˜ í˜„ì¬ ìƒíƒœì™€ ì§„í–‰ë¥  ì¶”ì 

**TTL**: 24ì‹œê°„ (ìë™ ì‚­ì œ)

**ë°ì´í„° í˜•ì‹**:
```json
{
  "id": "job-uuid",
  "article_id": "article-uuid",
  "status": "running",
  "progress": 45,
  "message": "Adapting article...",
  "error": null,
  "created_at": "2025-01-08T12:34:56.789Z",
  "updated_at": "2025-01-08T12:35:12.345Z"
}
```

**ì ‘ê·¼ íŒ¨í„´**:
- **API**: ìƒíƒœ ì´ˆê¸°í™” (queued), ì¡°íšŒ (GET)
- **Worker**: ìƒíƒœ ì—…ë°ì´íŠ¸ (running, completed, failed)
- **Progress Listener**: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (0-100%) - CrewAI ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

---

## ğŸ”‘ í•µì‹¬ ê°œë…

### 1. **ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ (Async Job Processing)**
- **ë¬¸ì œ**: CrewAI ì‹¤í–‰ì€ 2-5ë¶„ ê±¸ë¦¼ â†’ HTTP ìš”ì²­ì´ íƒ€ì„ì•„ì›ƒ
- **í•´ê²°**: Job Queue íŒ¨í„´
  - ìš”ì²­ ì¦‰ì‹œ `jobId` ë°˜í™˜
  - ì‹¤ì œ ì‘ì—…ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
  - í´ë¼ì´ì–¸íŠ¸ëŠ” job ìƒíƒœë¥¼ í´ë§

### 2. **ì„œë¹„ìŠ¤ ë¶„ë¦¬ (Service Separation)**
- **ì›ì¹™**: "í•œ ì»¨í…Œì´ë„ˆ = í•œ ì—­í• "
- **ì¥ì **:
  - ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ (workerë§Œ ëŠ˜ë¦¬ë©´ ë¨)
  - ì¥ì•  ê²©ë¦¬ (worker ì£½ì–´ë„ web/apiëŠ” ì •ìƒ)
  - ë°°í¬ ë¶„ë¦¬ (apië§Œ ìˆ˜ì •í•´ë„ worker ì˜í–¥ ì—†ìŒ)

### 3. **Job Queue (Redis)**
- **ì—­í• **: ì‘ì—… ìš”ì²­ì„ íì— ë„£ê³ , workerê°€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
- **ìƒíƒœ**: `queued` â†’ `running` â†’ `completed` / `failed`
- **ì¥ì **: ë¶€í•˜ ë¶„ì‚°, ì¬ì‹œë„ ê°€ëŠ¥, ìš°ì„ ìˆœìœ„ ì„¤ì • ê°€ëŠ¥

### 4. **ë°ì´í„° ì €ì¥ì†Œ**

#### MongoDB: Article Storage
- **Article metadata ë° content ì €ì¥** (`articles` ì»¬ë ‰ì…˜)
  - ì¤‘ë³µ ì²´í¬ (24ì‹œê°„ ë‚´ ë™ì¼ ì…ë ¥ íŒŒë¼ë¯¸í„°)
  - Article ì¡°íšŒ ë° ë¦¬ìŠ¤íŠ¸

- **Vocabulary ì €ì¥** (`vocabularies` ì»¬ë ‰ì…˜)
  - ë‹¨ì–´, lemma, ì •ì˜, ë¬¸ì¥ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
  - `related_words` ë°°ì—´ í¬í•¨ (ë¶„ë¦¬ ë™ì‚¬ ë“± ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡° ì§€ì›)
  - Articleë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê´€ë¦¬

- **Token Usage ì¶”ì ** (`token_usage` ì»¬ë ‰ì…˜)
  - LLM API í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì 
  - ì‚¬ìš©ìë³„, ì‘ì—…ë³„ (dictionary_search, article_generation) ì§‘ê³„
  - ì¼ë³„ ì‚¬ìš©ëŸ‰ í†µê³„ ë° ë¹„ìš© ë¶„ì„
  
**Article Status** (MongoDB, ì˜êµ¬ ì €ì¥):
- `running`: Article ìƒì„± ì‹œ ì´ˆê¸° ìƒíƒœ (ì²˜ë¦¬ ì¤‘)
- `completed`: Article ìƒì„± ì™„ë£Œ
- `failed`: Article ìƒì„± ì‹¤íŒ¨
- `deleted`: Article ì‚­ì œ (soft delete)

**Status Flow:**
```
ìƒì„± ì‹œ: running
   â†“
ì™„ë£Œ: completed
ì‹¤íŒ¨: failed
```

#### Redis: Job Queue & Status
- **Queue**: `opad:jobs` (List) - Workerê°€ ì²˜ë¦¬í•  jobë“¤ì„ FIFO ìˆœì„œë¡œ ì €ì¥
- **Status**: `opad:job:{job_id}` (String, 24h TTL) - Jobì˜ ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì 

**Job Status** (Redis, 24ì‹œê°„ TTL):
- `queued`: Jobì´ íì— ì¶”ê°€ë¨ (Workerê°€ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
- `running`: Workerê°€ Jobì„ ì²˜ë¦¬ ì¤‘
- `completed`: Job ì²˜ë¦¬ ì™„ë£Œ
- `failed`: Job ì²˜ë¦¬ ì‹¤íŒ¨

**Status Flow:**
```
queued â†’ running â†’ completed / failed
```

**Article Status vs Job Status:**
- **Article Status (MongoDB)**: Articleì˜ ìµœì¢… ìƒíƒœ (ì˜êµ¬ ì €ì¥)
- **Job Status (Redis)**: Job ì²˜ë¦¬ì˜ ì‹¤ì‹œê°„ ìƒíƒœ (24ì‹œê°„ í›„ ìë™ ì‚­ì œ)
- Articleì€ `running` ìƒíƒœë¡œ ìƒì„±ë˜ê³ , Jobì´ ì™„ë£Œë˜ë©´ `completed` ë˜ëŠ” `failed`ë¡œ ì—…ë°ì´íŠ¸ë¨

---

## ğŸ’° Token Usage Tracking

### Overview
The system tracks LLM API token usage and costs for all API calls, enabling cost monitoring, user billing, and usage analytics.

### Architecture

#### 1. LLM Utility Module (`utils/llm.py`)
Provider-agnostic LLM API calls using LiteLLM with automatic token tracking.

**Functions**:
- `call_llm_with_tracking()`: Makes LLM API calls and returns content + token statistics
- `parse_json_from_content()`: Parses JSON from LLM responses (handles markdown code blocks)
- `get_llm_error_response()`: Converts LLM exceptions to HTTP status codes

**TokenUsageStats Dataclass**:
```python
@dataclass
class TokenUsageStats:
    model: str              # Model name (e.g., "gpt-4.1-mini")
    prompt_tokens: int      # Input tokens
    completion_tokens: int  # Output tokens
    total_tokens: int       # Total tokens used
    estimated_cost: float   # Cost in USD (calculated by LiteLLM)
    provider: str | None    # Provider name (openai, anthropic, google)
```

**Supported Providers** (via LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

**Example Usage**:
```python
from utils.llm import call_llm_with_tracking, TokenUsageStats

content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# stats.model = "gpt-4.1-mini"
# stats.prompt_tokens = 8
# stats.completion_tokens = 12
# stats.estimated_cost = 0.000015
```

#### 2. MongoDB Storage (`utils/mongodb.py`)

**save_token_usage()**: Save token usage record
```python
def save_token_usage(
    user_id: str,
    operation: str,  # "dictionary_search" | "article_generation"
    model: str,
    prompt_tokens: int,
    completion_tokens: int,
    estimated_cost: float,
    article_id: Optional[str] = None,
    metadata: Optional[dict] = None
) -> Optional[str]:
    """Save token usage record to MongoDB."""
```

**get_user_token_summary()**: Get user's token usage summary
```python
def get_user_token_summary(user_id: str, days: int = 30) -> dict:
    """
    Returns:
    {
        'total_tokens': int,
        'total_cost': float,
        'by_operation': {
            'operation_type': {'tokens': int, 'cost': float, 'count': int}
        },
        'daily_usage': [
            {'date': 'YYYY-MM-DD', 'tokens': int, 'cost': float}
        ]
    }
    """
```

**get_article_token_usage()**: Get token usage for specific article
```python
def get_article_token_usage(article_id: str) -> list[dict]:
    """Returns all token usage records for an article."""
```

#### 3. Token Usage Collection Schema (MongoDB)

```json
{
  "_id": "uuid",
  "user_id": "uuid",
  "operation": "dictionary_search | article_generation",
  "model": "string",
  "prompt_tokens": 100,
  "completion_tokens": 50,
  "total_tokens": 150,
  "estimated_cost": 0.00025,
  "article_id": "uuid (optional)",
  "metadata": {
    "query": "...",
    "language": "..."
  },
  "created_at": "datetime"
}
```

**Indexes**:
- `(user_id, created_at)`: User usage queries (descending)
- `article_id`: Article-specific queries (sparse)
- `created_at`: Time-based queries (descending)
- `(operation, created_at)`: Operation-type queries

### Integration

#### Dictionary API (`src/api/routes/dictionary.py`)

```python
@router.post("/search", response_model=SearchResponse)
async def search_word(request: SearchRequest, current_user: User = Depends(get_current_user_required)):
    # Build prompt
    prompt = build_word_definition_prompt(
        language=request.language,
        sentence=request.sentence,
        word=request.word
    )

    # Call LLM with tracking
    content, stats = await call_llm_with_tracking(
        messages=[{"role": "user", "content": prompt}],
        model="gpt-4.1-mini",
        max_tokens=200
    )

    # Log token usage
    logger.info("Token usage for dictionary search", extra=stats.to_dict())

    # Save to database (Phase 2)
    save_token_usage(
        user_id=current_user.id,
        operation="dictionary_search",
        model=stats.model,
        prompt_tokens=stats.prompt_tokens,
        completion_tokens=stats.completion_tokens,
        estimated_cost=stats.estimated_cost,
        metadata={"query": request.word, "language": request.language}
    )

    # Parse and return response
    result = parse_json_from_content(content)
    return SearchResponse(**result)
```

#### Token Usage API Endpoints (`src/api/routes/usage.py`)

**GET /usage/me**: Get current user's token usage summary
```python
@router.get("/me", response_model=TokenUsageSummary)
async def get_my_usage(
    days: int = Query(default=30, ge=1, le=365),
    current_user: User = Depends(get_current_user_required)
):
    # Get aggregated summary from MongoDB
    summary = get_user_token_summary(user_id=current_user.id, days=days)

    # Convert to response models
    by_operation = {
        op_name: OperationUsage(**op_data)
        for op_name, op_data in summary.get('by_operation', {}).items()
    }
    daily_usage = [
        DailyUsage(**day) for day in summary.get('daily_usage', [])
    ]

    return TokenUsageSummary(
        total_tokens=summary.get('total_tokens', 0),
        total_cost=summary.get('total_cost', 0.0),
        by_operation=by_operation,
        daily_usage=daily_usage
    )
```

**GET /usage/articles/{article_id}**: Get token usage for specific article
```python
@router.get("/articles/{article_id}", response_model=list[TokenUsageRecord])
async def get_article_usage(
    article_id: str,
    current_user: User = Depends(get_current_user_required)
):
    # Verify article ownership
    article = get_article(article_id)
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    if article.get('user_id') != current_user.id:
        raise HTTPException(status_code=403, detail="You don't have permission")

    # Get all usage records for article
    usage_records = get_article_token_usage(article_id)

    return [TokenUsageRecord(**record) for record in usage_records]
```

### Token Usage Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant WebUI as Web UI
    participant API as FastAPI
    participant Redis
    participant Worker
    participant CrewAI
    participant LiteLLM
    participant MongoDB
    participant LLM as LLM Provider

    Note over User,LLM: Dictionary Search with Token Tracking
    User->>WebUI: Click word
    WebUI->>API: POST /dictionary/search
    API->>LLM: Call LLM API (via utils/llm.py)
    LLM-->>API: Response + usage stats
    API->>MongoDB: save_token_usage()
    API-->>WebUI: Definition response

    Note over User,MongoDB: Article Generation with Token Tracking
    User->>WebUI: Generate article
    WebUI->>API: POST /articles/generate
    API->>Redis: Enqueue job
    Redis-->>Worker: Dequeue job
    Worker->>CrewAI: run_crew()
    CrewAI->>LLM: Multiple API calls (agents execute)
    LLM-->>CrewAI: Responses (usage tracked internally)
    CrewAI-->>Worker: CrewResult (with crew_instance)
    Worker->>Worker: result.get_agent_usage()
    Worker->>Worker: calculate_cost() per agent
    Worker->>MongoDB: save_crew_token_usage() (per agent)
    Worker->>MongoDB: Save article

    Note over User,MongoDB: Usage Summary Retrieval
    User->>WebUI: View usage page
    WebUI->>API: GET /usage/me?days=30
    API->>MongoDB: get_user_token_summary()
    MongoDB-->>API: Aggregated summary
    API-->>WebUI: TokenUsageSummary
    WebUI-->>User: Display usage charts

    Note over User,MongoDB: Article Usage Details
    User->>WebUI: View article usage
    WebUI->>API: GET /usage/articles/{id}
    API->>MongoDB: get_article_token_usage()
    MongoDB-->>API: Usage records (all LLM calls)
    API-->>WebUI: List of TokenUsageRecord
    WebUI-->>User: Display usage details
```

---

## ğŸ”§ Worker Token Tracking

### Overview

The Worker service tracks token usage during CrewAI article generation using **CrewAI's built-in token tracking**, which provides reliable per-agent usage metrics.

### Architecture

**Data Flow:**
```
Worker â†’ process_job() â†’ run_crew() â†’ CrewAI agents execute
                                            â†“
                                     CrewAI tracks usage internally
                                            â†“
                                     CrewResult.get_agent_usage()
                                            â†“
                                     calculate_cost() (LiteLLM pricing)
                                            â†“
                                     save_crew_token_usage() â†’ MongoDB
```

**Key Design Decision:**
CrewAI manages LLM calls internally through its agent.llm instances. Each agent has a separate LLM instance with independent usage tracking via `agent.llm.get_token_usage_summary()`. This approach is preferred over LiteLLM callbacks because:
- CrewAI's internal tracking is more reliable for per-agent metrics
- No need to intercept LLM calls at the LiteLLM layer
- Simpler implementation without callback lifecycle management

### Token Usage Module

**File**: `src/utils/token_usage.py`

**Purpose**: Token usage utilities for cost calculation and tracking. Provides functions to calculate LLM costs using LiteLLM's pricing database and save CrewAI agent token usage to MongoDB.

**Functions**:

#### calculate_cost()

```python
def calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    """Calculate estimated cost using LiteLLM's pricing data.

    Note: LiteLLM pricing may become outdated. Costs are estimates only.

    Args:
        model: Model name (e.g., 'gpt-4.1', 'gpt-4.1-mini')
        prompt_tokens: Number of input tokens
        completion_tokens: Number of output tokens

    Returns:
        Estimated cost in USD, or 0.0 if pricing unavailable
    """
```

**Error Handling**:
- Returns 0.0 if model not in LiteLLM pricing database
- Gracefully handles KeyError, ValueError, AttributeError
- Logs unexpected errors at debug level

#### save_crew_token_usage()

```python
def save_crew_token_usage(
    result: CrewResult,
    user_id: str,
    article_id: str | None,
    job_id: str
) -> None:
    """Save token usage for each CrewAI agent to MongoDB.

    Uses CrewAI's built-in token tracking (agent.llm.get_token_usage_summary())
    to get per-agent, per-model usage metrics.
    """
```

**Behavior**:
- Iterates through all agents in CrewResult
- Skips agents with zero token usage
- Calculates cost using `calculate_cost()` with LiteLLM pricing
- Saves each agent's usage as separate MongoDB record
- Non-fatal: failures don't crash job processing

**Data Saved per Agent**:
```json
{
  "user_id": "user-uuid",
  "operation": "article_generation",
  "model": "gpt-4.1",
  "prompt_tokens": 2000,
  "completion_tokens": 1500,
  "estimated_cost": 0.0525,
  "article_id": "article-uuid",
  "metadata": {"job_id": "job-uuid"}
}
```

---

### CrewResult Class

**File**: `src/crew/main.py`

**Purpose**: Container for crew execution result with usage metrics extraction.

```python
class CrewResult:
    """Container for crew execution result and usage metrics."""

    def __init__(self, result, crew_instance):
        self.raw = result.raw
        self.result = result
        self.crew_instance = crew_instance

    def get_agent_usage(self) -> list[dict]:
        """Get token usage per agent with model info.

        Returns:
            List of dicts with agent_role, model, prompt_tokens,
            completion_tokens, total_tokens, successful_requests
        """
```

**Usage Example**:
```python
result = run_crew(inputs=ctx.inputs)
agent_usage = result.get_agent_usage()
# [
#   {'agent_role': 'News Researcher', 'model': 'gpt-4.1', 'prompt_tokens': 500, ...},
#   {'agent_role': 'Content Writer', 'model': 'gpt-4.1', 'prompt_tokens': 2000, ...},
#   ...
# ]
```

**Why CrewAI Built-in Tracking?**
- Each CrewAI agent has its own LLM instance with independent usage tracking
- `agent.llm.get_token_usage_summary()` provides accurate per-agent metrics
- More reliable than intercepting LLM calls at LiteLLM callback layer
- Simpler implementation without callback lifecycle management

---

## ğŸ“š Vocabulary-Aware Article Generation

### Overview
The system now supports vocabulary-aware article generation, where CrewAI adjusts content difficulty based on words the user has already learned.

### Vocabulary Features

#### 1. Dictionary API - Word Definition
- **POST /dictionary/search**: Get word definition and lemma using OpenAI API
- **Returns**: lemma, definition, and related_words (for complex structures like separable verbs)
- **Auth**: Required (JWT) to prevent API abuse

#### 2. Vocabulary Storage
- **POST /dictionary/vocabulary**: Add a word to user's vocabulary
- **GET /dictionary/vocabularies**: Get aggregated vocabulary grouped by lemma with counts
- **DELETE /dictionary/vocabularies/{id}**: Delete a vocabulary word
- **Auth**: All vocabulary operations require authentication and are user-specific

#### 3. Article-Specific Vocabularies
- **GET /articles/{article_id}/vocabularies**: Get all vocabularies for a specific article
- **Response**: List of VocabularyResponse objects with word, lemma, definition, context, and metadata
- **Auth**: Users can only access vocabularies from their own articles

### Data Model

#### Vocabulary Collection (MongoDB)
```json
{
  "_id": "ObjectId",
  "article_id": "uuid",
  "user_id": "uuid",
  "word": "string",              // Original word clicked
  "lemma": "string",             // Dictionary form
  "definition": "string",        // Word definition
  "sentence": "string",          // Sentence context
  "language": "string",
  "related_words": ["string"],   // All forms in sentence (e.g., verbs with particles)
  "span_id": "string",           // Span ID from markdown for linking
  "created_at": "datetime",
  "pos": "string",               // Part of speech (noun, verb, adjective, etc.)
  "gender": "string",            // Grammatical gender (der/die/das for German, le/la for French, etc.)
  "conjugations": {              // Verb conjugations (null for non-verbs)
    "present": "string",
    "past": "string",
    "perfect": "string"
  },
  "level": "string"              // CEFR level (A1, A2, B1, B2, C1, C2)
}
```

**New Grammatical Metadata Fields:**
- `pos`: Part of speech classification (noun, verb, adjective, adverb, preposition, etc.)
- `gender`: Grammatical gender for nouns in gendered languages (German: der/die/das, French: le/la, Spanish: el/la). Null for non-gendered languages.
- `conjugations`: Verb conjugation forms across tenses (present, past, perfect). Null for non-verbs.
- `level`: CEFR difficulty level (A1-C2) for vocabulary tracking and adaptive learning.

#### VocabularyCount Model (Aggregated Response)
- Groups vocabularies by lemma
- Returns count of how many times a lemma appears across articles
- Includes most recent definition and example sentence
- Lists all article_ids where lemma appears
- Includes grammatical metadata (pos, gender, conjugations, level) from most recent entry

#### API Model Enhancements

**Conjugations.__bool__()** (`src/api/models.py:18-20`):
- Enables truthiness checking: `if conjugations:` returns False when all fields (present, past, perfect) are None
- Simplifies validation logic by treating empty Conjugations as falsy
- Backend can check conjugation presence without explicit null checks

**VocabularyRequest.field_validator** (`src/api/models.py:106-116`):
- Automatic conversion from Conjugations model to dict before database storage
- Returns None if conjugations object is empty (using `__bool__` check)
- Handles both dict and Conjugations input types
- Prevents storing empty conjugation objects in MongoDB

### Vocabulary-Aware Generation Flow
1. User saves vocabulary words from articles (POST /dictionary/vocabulary)
2. Words stored with article context (sentence, span_id)
3. When generating new article, worker retrieves user's vocabulary list
4. CrewAI receives vocabulary list as constraint for content difficulty
5. Generated article uses different words/complexity for learned vocabulary
6. User can access article-specific vocabularies (GET /articles/{id}/vocabularies)

### Authentication
All vocabulary endpoints require JWT authentication. Users can only:
- Add/delete their own vocabulary
- View their own vocabulary lists
- Access vocabularies from their own articles

### CEFR Vocabulary Level Filtering

**Function**: `get_allowed_vocab_levels(target_level, max_above=1)`

**File**: `src/utils/mongodb.py`

**Purpose**: Filters vocabulary words to only include those appropriate for the target CEFR level when generating articles.

**Parameters**:
- `target_level`: Target CEFR level (A1, A2, B1, B2, C1, C2)
- `max_above`: Maximum levels above target to allow (default: 1)

**Returns**: List of allowed CEFR levels

**Examples**:
```python
get_allowed_vocab_levels('A2', max_above=1)  # Returns: ['A1', 'A2', 'B1']
get_allowed_vocab_levels('B1', max_above=1)  # Returns: ['A1', 'A2', 'B1', 'B2']
get_allowed_vocab_levels('C2', max_above=1)  # Returns: ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
```

**Usage in Worker**:
When generating articles, the worker fetches user vocabulary filtered by target level:
```python
vocab = get_user_vocabulary_for_generation(
    user_id=ctx.user_id,
    language=ctx.inputs['language'],
    target_level=ctx.inputs.get('level'),  # Filters vocab by CEFR level
    limit=50
)
```

**Benefits**:
- Prevents too-difficult vocabulary from appearing in beginner articles
- Ensures vocabulary reinforcement matches the target difficulty
- Allows slight challenge with `max_above=1` parameter

---

## ğŸ¤– CrewAI Pipeline

### Agent Configuration

The CrewAI pipeline uses four specialized agents for article generation. Each agent is configured with a specific role, goal, and LLM model.

**Agents** (`src/crew/config/agents.yaml`):

| Agent | Role | LLM Model |
|-------|------|-----------|
| `article_finder` | Searches for recent news articles matching topic, language, and difficulty | `openai/gpt-4.1-mini` |
| `article_picker` | Evaluates and selects the best article from candidates | `openai/gpt-4.1` |
| `article_rewriter` | Adapts the article to target CEFR level with vocabulary reinforcement | `anthropic/claude-sonnet-4-20250514` |
| `article_reviewer` | Reviews for natural language quality and level appropriateness | `anthropic/claude-sonnet-4-20250514` |

### Task Pipeline

Tasks execute sequentially, with each task building on the previous output:

```mermaid
graph LR
    A[find_news_articles] --> B[pick_best_article]
    B --> C[adapt_news_article]
    C --> D[review_article_quality]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#e8f5e9
    style D fill:#e8f5e9
```

**Tasks** (`src/crew/config/tasks.yaml`):

#### 1. find_news_articles
- **Agent**: `article_finder`
- **Description**: Searches for 2-3 recent news articles matching the topic in the target language
- **Output**: `NewsArticleList` (JSON with articles array)
- **Guardrail**: `repair_json_output` for JSON validation

#### 2. pick_best_article
- **Agent**: `article_picker`
- **Description**: Evaluates candidates and selects the best article based on topic relevance, difficulty level, length, and educational value
- **Context**: `find_news_articles`
- **Output**: `SelectedArticle` (JSON with article and selection_rationale)
- **Guardrail**: `repair_json_output` for JSON validation

#### 3. adapt_news_article
- **Agent**: `article_rewriter`
- **Description**: Rewrites the selected article to match the target CEFR level
- **Context**: `pick_best_article`
- **Features**:
  - Vocabulary reinforcement using user's learned words
  - Source attribution (name, URL, date, author)
  - Markdown formatting without word highlighting
- **Output**: Markdown text

#### 4. review_article_quality
- **Agent**: `article_reviewer`
- **Description**: Reviews the adapted article for natural language quality and level appropriateness
- **Context**: `adapt_news_article`
- **Output**: `ReviewedArticle` (JSON with article_content and replaced_sentences)
- **Guardrail**: `repair_json_output` for JSON validation

### Pydantic Output Models

**File**: `src/crew/models.py`

#### ReviewedArticle
Final output from the review task:
```python
class ReviewedArticle(BaseModel):
    """A reviewed news article with review rationale"""
    article_content: str = Field(description="The final polished article in markdown format")
    replaced_sentences: list[ReplacedSentence] = Field(
        description="List of sentences that were replaced during review",
        default=[]
    )
```

#### ReplacedSentence
Tracks modifications made during review:
```python
class ReplacedSentence(BaseModel):
    """Replaced sentence information"""
    original: str = Field(description="The original sentence before replacement")
    replaced: str = Field(description="The sentence after replacement")
    rationale: str = Field(description="Reason for the replacement")
```

### Worker Integration

**File**: `src/worker/processor.py`

The worker processes the CrewAI result and extracts the reviewed article:

```python
# Log replaced sentences from review
reviewed = result.pydantic
if isinstance(reviewed, ReviewedArticle) and reviewed.replaced_sentences:
    for change in reviewed.replaced_sentences:
        logger.info(
            f"Sentence replaced: '{change.original}' â†’ '{change.replaced}' ({change.rationale})",
            extra=ctx.log_extra
        )

# Save to MongoDB
content = reviewed.article_content if isinstance(reviewed, ReviewedArticle) else result.raw
save_article(ctx.article_id, content, ctx.started_at)
```

**Benefits of the Review Step**:
- Catches unnatural expressions that slip through the rewriting phase
- Ensures vocabulary matches the target CEFR level
- Improves sentence flow and readability
- Logs all changes for quality tracking and debugging

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
opad/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # API ì„œë¹„ìŠ¤ (FastAPI)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py       # FastAPI ì•± ì§„ì…ì 
â”‚   â”‚   â”œâ”€â”€ models.py     # Pydantic ëª¨ë¸ (Article, Job)
â”‚   â”‚   â”œâ”€â”€ routes/       # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ articles.py
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints.py
â”‚   â”‚   â”‚   â”œâ”€â”€ stats.py
â”‚   â”‚   â”‚   â””â”€â”€ dictionary.py  # Dictionary API (word definition)
â”‚   â”‚   â””â”€â”€ job_queue.py  # Redis í ê´€ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ worker/           # Worker ì„œë¹„ìŠ¤ (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py       # Worker ì§„ì…ì 
â”‚   â”‚   â”œâ”€â”€ processor.py  # Job ì²˜ë¦¬ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ context.py    # JobContext helper
â”‚   â”‚   â””â”€â”€ tests/        # Worker tests
â”‚   â”‚
â”‚   â”œâ”€â”€ web/              # Web ì„œë¹„ìŠ¤ (Next.js)
â”‚   â”‚   â”œâ”€â”€ app/          # Next.js App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ api/      # API Routes (í”„ë¡ì‹œ)
â”‚   â”‚   â”‚   â”œâ”€â”€ articles/ # Article pages
â”‚   â”‚   â”‚   â”œâ”€â”€ vocabulary/ # Vocabulary pages
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx  # ë©”ì¸ í˜ì´ì§€
â”‚   â”‚   â”œâ”€â”€ components/   # React ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ ArticleCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EmptyState.tsx      # Reusable empty state
â”‚   â”‚   â”‚   â”œâ”€â”€ ErrorAlert.tsx      # Reusable error alert
â”‚   â”‚   â”‚   â”œâ”€â”€ MarkdownViewer.tsx
â”‚   â”‚   â”‚   â””â”€â”€ VocabularyList.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/        # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useAsyncFetch.ts    # Generic fetch with loading/error
â”‚   â”‚   â”‚   â”œâ”€â”€ usePagination.ts    # Pagination calculations
â”‚   â”‚   â”‚   â”œâ”€â”€ useStatusPolling.ts # Job status polling
â”‚   â”‚   â”‚   â””â”€â”€ useVocabularyDelete.ts # Vocabulary deletion
â”‚   â”‚   â”œâ”€â”€ lib/          # Frontend utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts           # fetchWithAuth, parseErrorResponse
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts          # Auth utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ formatters.ts    # Date formatting utilities
â”‚   â”‚   â”‚   â””â”€â”€ styleHelpers.ts  # CEFR color/label helpers
â”‚   â”‚   â”œâ”€â”€ types/        # TypeScript type definitions
â”‚   â”‚   â”œâ”€â”€ tailwind.config.ts # Tailwind config with safelist
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”‚
â”‚   â”œâ”€â”€ crew/             # CrewAI ë¡œì§ (ê³µìœ )
â”‚   â”‚   â”œâ”€â”€ crew.py       # ReadingMaterialCreator í´ë˜ìŠ¤ (agents + tasks)
â”‚   â”‚   â”œâ”€â”€ main.py       # run() í•¨ìˆ˜ (CrewAI ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸)
â”‚   â”‚   â”œâ”€â”€ models.py     # Pydantic ëª¨ë¸ (NewsArticle, ReviewedArticle ë“±)
â”‚   â”‚   â”œâ”€â”€ guardrails.py # JSON ì¶œë ¥ ë³µêµ¬ guardrail
â”‚   â”‚   â”œâ”€â”€ progress_listener.py  # JobProgressListener (ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ)
â”‚   â”‚   â””â”€â”€ config/       # YAML ì„¤ì •
â”‚   â”‚       â”œâ”€â”€ agents.yaml  # ì—ì´ì „íŠ¸ ì •ì˜ (article_finder, article_picker, article_rewriter, article_reviewer)
â”‚   â”‚       â””â”€â”€ tasks.yaml   # íƒœìŠ¤í¬ ì •ì˜ (find_news_articles, pick_best_article, adapt_news_article, review_article_quality)
â”‚   â”‚
â”‚   â””â”€â”€ utils/            # ê³µí†µ ìœ í‹¸ë¦¬í‹° (ê³µìœ )
â”‚       â”œâ”€â”€ mongodb.py    # MongoDB ì—°ê²° ë° ì‘ì—…
â”‚       â”œâ”€â”€ logging.py    # Structured logging ì„¤ì •
â”‚       â”œâ”€â”€ llm.py        # OpenAI API ê³µí†µ í•¨ìˆ˜
â”‚       â”œâ”€â”€ prompts.py    # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚       â””â”€â”€ token_usage.py # Token usage calculation and tracking
â”‚
â””â”€â”€ Dockerfile.*          # ì„œë¹„ìŠ¤ë³„ Dockerfile (ì´ìŠˆ #9)
```

### ì„œë¹„ìŠ¤ êµ¬ë¶„
| í´ë” | ì—­í•  | ëŸ°íƒ€ì„ | í¬íŠ¸ |
|------|------|--------|------|
| `src/api/` | CRUD + Job enqueue + Dictionary API | Python (FastAPI) | 8001 (default) |
| `src/worker/` | CrewAI ì‹¤í–‰ + Job/Token Tracking | Python | - |
| `src/web/` | UI | Node.js (Next.js) | 3000 |
| `src/crew/` | CrewAI ë¡œì§ (ê³µìœ ) | - | - |
| `src/utils/` | ê³µí†µ ìœ í‹¸ (ê³µìœ ) | - | - |

### Worker ëª¨ë“ˆ êµ¬ì„±
| íŒŒì¼ | ì—­í•  | ì˜ì¡´ì„± |
|------|------|--------|
| `worker/main.py` | Worker ì§„ì…ì  (infinite loop) | `processor.py` |
| `worker/processor.py` | Job ì²˜ë¦¬ ë¡œì§ (process_job) | `crew/main.py`, `utils/token_usage.py` |
| `worker/context.py` | JobContext helper (job data validation) | `api/job_queue.py` |
| `crew/progress_listener.py` | JobProgressListener (CrewAI event listener) | `api/job_queue.py` |

### CrewAI ëª¨ë“ˆ êµ¬ì„±
| íŒŒì¼ | ì—­í•  | ì¶œë ¥ ëª¨ë¸ |
|------|------|----------|
| `crew/crew.py` | ReadingMaterialCreator í´ë˜ìŠ¤ (agents + tasks ì •ì˜) | - |
| `crew/main.py` | `run()` í•¨ìˆ˜ - CrewAI ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ | `CrewResult` |
| `crew/models.py` | Pydantic ì¶œë ¥ ëª¨ë¸ ì •ì˜ | `NewsArticleList`, `SelectedArticle`, `ReviewedArticle` |
| `crew/guardrails.py` | JSON ì¶œë ¥ ë³µêµ¬ guardrail | - |
| `crew/config/agents.yaml` | ì—ì´ì „íŠ¸ ì„¤ì • (role, goal, backstory, llm) | - |
| `crew/config/tasks.yaml` | íƒœìŠ¤í¬ ì„¤ì • (description, expected_output, context) | - |

---

## ğŸ”‘ ê³µí†µ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

### LLM ìœ í‹¸ë¦¬í‹° (`utils/llm.py`)
Provider-agnostic LLM API í˜¸ì¶œ ë° í† í° ì¶”ì ì„ ìœ„í•œ ê³µí†µ í•¨ìˆ˜ë“¤ (LiteLLM ê¸°ë°˜):

- **`call_llm_with_tracking()`**: LLM API í˜¸ì¶œ + í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (ë²”ìš© í•¨ìˆ˜)
  - ë°˜í™˜ê°’: `(content: str, stats: TokenUsageStats)`
  - OpenAI, Anthropic, Google ë“± ë‹¤ì–‘í•œ í”„ë¡œë°”ì´ë” ì§€ì›
  - ìë™ ë¹„ìš© ê³„ì‚° (LiteLLM ë‚´ì¥ ê°€ê²© ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
- **`TokenUsageStats`**: í† í° ì‚¬ìš©ëŸ‰ í†µê³„ dataclass
  - í•„ë“œ: `model`, `prompt_tokens`, `completion_tokens`, `total_tokens`, `estimated_cost`, `provider`
- **`parse_json_from_content()`**: LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹± (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
  - ì¼ë°˜ JSON, ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ (```json), í…ìŠ¤íŠ¸ ë‚´ JSON ì¶”ì¶œ ì§€ì›
- **`get_llm_error_response()`**: LLM ê´€ë ¨ ì˜ˆì™¸ë¥¼ HTTP ìƒíƒœ ì½”ë“œë¡œ ë³€í™˜

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from utils.llm import call_llm_with_tracking, parse_json_from_content

# LLM í˜¸ì¶œ + í† í° ì¶”ì 
content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
logger.info("Token usage", extra=stats.to_dict())

# JSON íŒŒì‹±
result = parse_json_from_content(content)
```

**ì§€ì› í”„ë¡œë°”ì´ë”** (LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (`utils/prompts.py`)
ì¬ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡¬í”„íŠ¸ ë¹Œë” í•¨ìˆ˜ë“¤:

- **`build_word_definition_prompt()`**: Dictionary APIìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (lemma ë° definition ì¶”ì¶œ)

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from utils.prompts import build_word_definition_prompt

prompt = build_word_definition_prompt(
    language="German",
    sentence="Diese groÃŸe Spanne hÃ¤ngt von mehreren Faktoren ab.",
    word="hÃ¤ngt"
)
```

---

## ğŸ“¡ Dynamic Endpoint Discovery

### Tag-Based Endpoint Grouping

The `/endpoints` endpoint dynamically generates an HTML page listing all registered API routes, grouped by their tags. This system requires no code changes when new routes are addedâ€”they automatically appear in the listing.

**Endpoint**: `GET /endpoints`

**File**: `src/api/routes/endpoints.py`

**How It Works**:

1. **Route Introspection**: Scans `app.routes` to collect all routes with methods and paths
2. **Tag Extraction**: Reads tags from route definitions (e.g., `tags=["articles"]`, `tags=["usage"]`)
3. **Dynamic Grouping**: Groups endpoints by tag using `group_endpoints_by_tag()`
4. **HTML Generation**: Renders grouped endpoints as styled HTML page

**Helper Functions**:

- **`group_endpoints_by_tag(endpoints)`**: Groups endpoints by tag (first tag if multiple), returns `dict[tag, list[endpoints]]`
  - Uses `defaultdict(list)` to avoid KeyError
  - Filters out `EXCLUDED_TAGS` (e.g., "meta")
  - Sorts endpoints within each group by `(path, method)`
  - Assigns "other" tag if endpoint has no tags

- **`get_sorted_tags(grouped)`**: Returns alphabetically sorted tag list with "other" at end

- **`format_endpoint(ep)`**: Formats single endpoint as HTML with method badge, path, and summary

- **`format_tag_title(tag)`**: Converts tag name to display title (e.g., "articles" â†’ "Articles Endpoints")

**Configuration**:

```python
# src/api/routes/endpoints.py:13-14
EXCLUDED_TAGS = {"meta"}  # Tags to hide from listing
EXCLUDED_PATHS = {"/docs", "/openapi.json", "/redoc"}  # Paths to skip
```

**Example Route Definition**:

```python
# src/api/routes/usage.py
router = APIRouter(tags=["usage"])  # Tag used for grouping

@router.get("/me", summary="Get current user's token usage summary")
async def get_my_usage(...):
    """Detailed description..."""
```

**Benefits**:

- **Zero-maintenance**: New routes automatically appear in listing
- **Tag-based organization**: Routes grouped by domain (articles, usage, dictionary)
- **Clean HTML output**: Color-coded HTTP methods (GET=blue, POST=green, DELETE=red)
- **No hardcoding**: Eliminates manual endpoint lists

**Usage**:
Visit `/endpoints` in browser to see all available API routes grouped by tag.

---

## ğŸ“¡ Dictionary API

### Word Definition Endpoint

**Endpoint**: `POST /dictionary/search`

**ëª©ì **: ë¬¸ì¥ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ì˜ lemma, ì •ì˜ ë° ë¬¸ë²•ì  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ

**ìš”ì²­:**
```json
{
  "word": "hÃ¤ngt",
  "sentence": "Diese groÃŸe Spanne hÃ¤ngt von mehreren Faktoren ab.",
  "language": "German"
}
```

**ì‘ë‹µ:**
```json
{
  "lemma": "abhÃ¤ngen",
  "definition": "ì˜ì¡´í•˜ë‹¤, ~ì— ë‹¬ë ¤ìˆë‹¤",
  "related_words": ["hÃ¤ngt", "ab"],
  "pos": "verb",
  "gender": null,
  "conjugations": {
    "present": "hÃ¤ngt ab",
    "past": "hing ab",
    "perfect": "hat abgehangen"
  },
  "level": "B1"
}
```

**íŠ¹ì§•:**
- **ë¶„ë¦¬ë™ì‚¬ ì²˜ë¦¬**: ë…ì¼ì–´ ë“±ì—ì„œ ë™ì‚¬ê°€ ë¶„ë¦¬ëœ ê²½ìš° ì „ì²´ lemma ë°˜í™˜ (ì˜ˆ: `hÃ¤ngt ... ab` â†’ `abhÃ¤ngen`)
- **ë³µí•©ì–´ ì²˜ë¦¬**: ë‹¨ì–´ê°€ ë³µí•©ì–´ì˜ ì¼ë¶€ì¸ ê²½ìš° ì „ì²´ í˜•íƒœ ë°˜í™˜
- **related_words**: ë¬¸ì¥ì—ì„œ ê°™ì€ lemmaì— ì†í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ë“¤ì„ ë°°ì—´ë¡œ ë°˜í™˜ (ì˜ˆ: ë¶„ë¦¬ ë™ì‚¬ì˜ ê²½ìš° ëª¨ë“  ë¶€ë¶„ í¬í•¨)
- **ë¬¸ë²•ì  ë©”íƒ€ë°ì´í„°**: í’ˆì‚¬(pos), ì„±(gender), ë™ì‚¬ í™œìš©í˜•(conjugations), CEFR ë ˆë²¨(level) ìë™ ì¶”ì¶œ
- **ê³µí†µ ìœ í‹¸ ì‚¬ìš©**: `utils/llm.py`ì˜ `call_openai_chat()` í•¨ìˆ˜ í™œìš©
- **í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬**: `utils/prompts.py`ì˜ `build_word_definition_prompt()` ì‚¬ìš©
- **ë³´ì•ˆ**: Regex injection ë°©ì§€ë¥¼ ìœ„í•œ `re.escape()` ì ìš©

**íë¦„:**

```mermaid
sequenceDiagram
    participant Frontend as Frontend<br/>(MarkdownViewer)
    participant NextAPI as Next.js API<br/>(/api/dictionary/search)
    participant FastAPI as FastAPI<br/>(/dictionary/search)
    participant Utils as Utils<br/>(prompts.py + llm.py)
    participant OpenAI as OpenAI API

    Frontend->>NextAPI: POST /api/dictionary/search<br/>{word, sentence, language}
    NextAPI->>FastAPI: POST /dictionary/search<br/>{word, sentence, language}

    FastAPI->>Utils: build_word_definition_prompt()
    Utils-->>FastAPI: prompt string

    FastAPI->>Utils: call_openai_chat(prompt)
    Utils->>OpenAI: POST /v1/chat/completions
    OpenAI-->>Utils: {lemma, definition, related_words, pos, gender, conjugations, level}
    Utils->>Utils: parse_json_from_content()
    Utils-->>FastAPI: {lemma, definition, related_words, pos, gender, conjugations, level}

    FastAPI-->>NextAPI: SearchResponse<br/>{lemma, definition, related_words, pos, gender, conjugations, level}
    NextAPI-->>Frontend: {lemma, definition, related_words, pos, gender, conjugations, level}
```

---

## ğŸ¨ Frontend Architecture

### Code Organization

The frontend follows a modular architecture with clear separation of concerns:

```
src/web/
â”œâ”€â”€ app/              # Next.js App Router (pages)
â”œâ”€â”€ components/       # Reusable React components
â”œâ”€â”€ hooks/            # Custom React hooks
â”œâ”€â”€ lib/              # Utility functions
â””â”€â”€ types/            # TypeScript type definitions
```

### Utility Modules

#### API Client (`lib/api.ts`)
Centralized API client utilities for consistent request handling:
- `fetchWithAuth()`: Automatic JWT token injection
- `parseErrorResponse()`: Consistent error message extraction

**Benefits**:
- DRY principle: Authentication logic in one place
- Consistent error handling across all API calls
- Easy to add global request interceptors

#### Date Formatters (`lib/formatters.ts`)
Reusable date formatting functions using Intl.DateTimeFormat:
- `formatDate()`: Customizable date formatting
- `formatDateShort()`: Short date format
- `formatDateTime()`: Date with time

**Benefits**:
- Consistent date display across UI
- Locale-aware formatting
- Single source of truth for date formats

#### Style Helpers (`lib/styleHelpers.ts`)
CEFR level badge styling utilities:
- `getLevelColor()`: Tailwind classes for level badges
- `getLevelLabel()`: Human-readable level labels

**Benefits**:
- Consistent color scheme across UI
- Dynamic class generation for Tailwind
- Easy to update color scheme globally

**Important**: CEFR level colors are safelisted in `tailwind.config.ts` to prevent Tailwind's tree-shaking from removing dynamically-generated classes.

### Custom Hooks

#### useAsyncFetch
Generic hook for async data fetching with automatic state management:
- Loading state
- Error handling
- Automatic 401 redirect
- Type-safe data state

**Use Cases**:
- Fetching article lists
- Loading article details
- Any API data fetching

#### usePagination
Pagination calculations and state management:
- Current page calculation
- Total pages calculation
- Next/previous page navigation
- Skip value computation

**Use Cases**:
- Article list pagination
- Vocabulary list pagination

#### useStatusPolling
Job status polling with automatic interval management:
- Configurable polling interval
- Automatic cleanup on completion/error
- Progress state management
- Callbacks for status changes

**Use Cases**:
- Article generation progress tracking
- Any long-running job monitoring

#### useVocabularyDelete
Vocabulary deletion with error handling:
- DELETE request to API
- Detailed error messages
- Throws errors for caller to handle

**Use Cases**:
- Deleting vocabulary entries from vocabulary list

### Reusable Components

#### ErrorAlert (`src/web/components/ErrorAlert.tsx`)
Consistent error message display:
- Red background with border (`bg-red-50 border-red-200`)
- Optional retry button with hover effect
- Automatic hiding when error is null
- Accessible error messaging

**Props**:
- `error` (string | null): Error message to display
- `onRetry` (optional): Callback function for retry button
- `className` (optional): Additional CSS classes

#### EmptyState (`src/web/components/EmptyState.tsx`)
Consistent empty state display:
- Centered layout with white card background
- Optional icon (emoji or Unicode character)
- Optional action button with blue styling
- Title and description text with gray tones

**Props**:
- `title` (string): Main heading text
- `description` (string): Descriptive subtitle
- `icon` (optional): Emoji or icon character
- `action` (optional): Object with `label` and `onClick` for action button
- `className` (optional): Additional CSS classes

**Benefits**:
- Consistent UX across all pages
- Reduces code duplication
- Easy to update design globally
- Improves maintainability with single source of truth

### Refactoring Impact

**Before Refactoring**:
- Duplicate fetch logic in every page
- Inconsistent error handling
- Duplicate date formatting code
- Duplicate pagination calculations
- Duplicate empty state styling

**After Refactoring**:
- Single source of truth for common operations
- Consistent error handling with `useAsyncFetch`
- Reusable date formatters
- Reusable pagination hook
- Reusable UI components (`ErrorAlert`, `EmptyState`)

**Code Reduction**:
- Article detail page: 166 lines reduced
- Articles list page: 27 lines reduced
- Vocabulary page: 43 lines reduced
- Total: 236 lines of code removed through refactoring

### Security Improvements

#### XSS Prevention in MarkdownViewer (`src/web/components/MarkdownViewer.tsx`)

**Issue**: Previous implementation used `innerHTML` to inject vocabulary buttons, creating XSS vulnerability.

**Security Measures Implemented**:

1. **HTML Escaping** (lines 92-96):
   - `escapeHtml()` utility converts text to DOM text node then reads innerHTML
   - Prevents script injection in user-provided content (word, lemma, definition, sentence)
   - Applied to all vocabulary data before rendering

2. **DOM API Methods** (lines 663-713):
   - Replaced `innerHTML` with DOM manipulation (`createElement`, `textContent`)
   - Creates definition spans using `document.createElement()` instead of string templates
   - Uses `textContent` for user data instead of `innerHTML`
   - Parses button HTML in temporary container, then extracts element reference

3. **Event Delegation** (lines 514-534):
   - Single event listener on container instead of per-word listeners
   - Prevents stale closure issues with dynamic content
   - Ref-based callback storage (`handleWordClickRef`) avoids outdated state

4. **Data Attribute Escaping** (lines 112-131):
   - All data attributes escaped before setting: `data-word="${wordEscaped}"`
   - JSON strings escaped with `.replace(/"/g, '&quot;')` for attribute safety
   - Prevents attribute injection attacks

**Before (Vulnerable)**:
```typescript
// Direct innerHTML injection - XSS risk
defSpan.innerHTML = `<strong>${lemma}</strong>: ${meaning} <button>...</button>`
```

**After (Secure)**:
```typescript
// DOM API - XSS safe
const strong = document.createElement('strong')
strong.textContent = displayLemma
defSpan.appendChild(strong)
defSpan.appendChild(document.createTextNode(': ' + meaning))
```

**Impact**: Prevents malicious script execution from vocabulary data, protects against DOM-based XSS attacks.

#### React Component Remounting Pattern

**Purpose**: Prevent React hydration mismatches when article content changes.

**Implementation** (`src/web/app/articles/[id]/page.tsx:266`):
```typescript
<MarkdownViewer
  key={`${articleId}-${content.length}`}
  content={content}
  language={article?.language}
  articleId={articleId}
  vocabularies={vocabularies}
  onAddVocabulary={handleAddVocabulary}
/>
```

**Key Prop Strategy**:
- Pattern: `${articleId}-${content.length}`
- Forces complete component remount when content changes
- Triggers reset of `data-processed` attribute (line 456)
- Clears all previous DOM state and event listeners

**Processing State Check** (`src/web/components/MarkdownViewer.tsx:456-458`):
```typescript
// Skip if already processed (component remounts on content change via key prop)
if (containerRef.current.getAttribute('data-processed') === 'true') {
  return
}
```

**Why This Pattern**:
- Without key: React reuses DOM nodes, causing hydration mismatches
- Manual cleanup: Error-prone and complex to maintain
- useEffect on content: Risk of double-processing
- Remounting: Clean state guaranteed, simple lifecycle

**Benefits**:
- Eliminates React DOM mismatch errors
- Prevents stale event listeners
- Simplifies component update logic
- Ensures consistent behavior across content changes

### Bug Fixes

#### 1. Conjugations Type Conversion
**Issue**: Frontend expected conjugations as object, but backend returned null for non-verbs, causing type mismatches.

**Fix**: Ensure conjugations field is properly typed and handled as nullable in TypeScript types.

#### 2. Tailwind Safelist for Dynamic Classes
**Issue**: CEFR level color classes (generated dynamically by `getLevelColor()`) were being purged by Tailwind's tree-shaking.

**Fix**: Added safelist to `tailwind.config.ts` to preserve dynamic color classes:
```typescript
safelist: [
  'bg-gray-100', 'text-gray-600',   // Unknown level
  'bg-green-100', 'text-green-700', // A1-A2
  'bg-yellow-100', 'text-yellow-700', // B1-B2
  'bg-red-100', 'text-red-700',     // C1-C2
]
```

**Impact**: CEFR level badges now display correctly with proper colors

### Token Usage Display

The article detail page (`src/web/app/articles/[id]/page.tsx`) displays token usage with smart aggregation and auto-refresh functionality.

#### TokenUsageSection Component

**Location**: `src/web/app/articles/[id]/page.tsx:46-181`

**Purpose**: Displays token usage breakdown for an article with aggregation logic.

**Features**:
- Displays total tokens, prompt tokens, completion tokens, and estimated cost
- Shows detailed breakdown per operation with agent names
- Aggregates dictionary searches into cumulative totals
- Uses expandable details for viewing individual operations

#### Aggregation Logic

```typescript
// dictionary_search: aggregate by operation+model
// article_generation: keep separate using record id
const key = record.operation === 'dictionary_search'
  ? `op:dictionary_search:${record.model}`
  : `id:${record.id}`
```

**Rationale**:
- **Article generation records**: Kept separate to show individual agent contributions (Article Search, Article Selection, Article Rewrite)
- **Dictionary search records**: Aggregated into cumulative totals per model since individual lookups are less meaningful

#### Agent Names

Article generation records include `agent_name` in metadata:
- `Article Search`: Research agent that finds relevant articles
- `Article Selection`: Agent that selects the best article for adaptation
- `Article Rewrite`: Agent that rewrites content for the target level

The `formatOperationName()` helper prioritizes `agent_name` over the raw operation type for display.

#### Auto-Refresh Mechanism

**Flow**:
```
User clicks word in MarkdownViewer
    |
    v
Dictionary API call (POST /dictionary/search)
    |
    v
Token usage record saved to MongoDB
    |
    v
onTokenUsageUpdate callback triggered
    |
    v
fetchTokenUsage(true) called with isRefresh=true
    |
    v
Token usage section updates without loading spinner
```

**Implementation** (`src/web/app/articles/[id]/page.tsx`):

1. **MarkdownViewer receives callback**: `onTokenUsageUpdate?: () => void`
2. **After dictionary search**: Callback is invoked in `handleWordClick` (line 328)
3. **Article page passes handler**: `onTokenUsageUpdate={() => fetchTokenUsage(true)}`
4. **Silent refresh**: `isRefresh=true` skips loading state to prevent UI flicker

```typescript
const fetchTokenUsage = useCallback(async (isRefresh = false) => {
  if (article?.status !== 'completed') return

  if (!isRefresh) {
    setTokenUsageLoading(true)  // Only show loading on initial fetch
  }
  // ... fetch logic
}, [article?.status, articleId])
```

**Benefits**:
- Users see updated token costs immediately after word lookups
- No full page reload required
- Silent refresh prevents disruptive UI changes

### Vocabulary Management Endpoints

**Endpoints:**
- `POST /dictionary/vocabularies` - Add vocabulary
- `GET /dictionary/vocabularies` - Get vocabulary list (optionally filtered by article_id)
- `DELETE /dictionary/vocabularies/{id}` - Delete vocabulary
- `GET /dictionary/stats` - Get vocabulary statistics (word counts by language)

**Vocabulary ì €ì¥:**
- MongoDB `vocabularies` ì»¬ë ‰ì…˜ì— ì €ì¥
- `related_words` ë°°ì—´ í¬í•¨ (ë¶„ë¦¬ ë™ì‚¬ ë“± ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡° ì§€ì›)
- Articleë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê´€ë¦¬

**Vocabulary í‘œì‹œ:**
- ì €ì¥ëœ ë‹¨ì–´ëŠ” ì´ˆë¡ìƒ‰ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸
- `related_words`ì— í¬í•¨ëœ ë‹¨ì–´ë“¤ë„ í•¨ê»˜ ì´ˆë¡ìƒ‰ í‘œì‹œ
- ì˜ˆ: "hÃ¤ngt" ì €ì¥ ì‹œ "ab"ë„ ìë™ìœ¼ë¡œ ì´ˆë¡ìƒ‰ í‘œì‹œ

---

## ğŸ§ª Testing Infrastructure

### Web Testing (Vitest)

**Configuration**: `src/web/vitest.config.ts`

**Test Environment**:
- **Framework**: Vitest 4.0.18 with jsdom for DOM simulation
- **UI**: @vitest/ui for interactive test running
- **Testing Library**: @testing-library/react for component testing
- **Test Matchers**: @testing-library/jest-dom for DOM assertions

**Coverage Settings**:
- Provider: v8 (Node.js native coverage)
- Reporters: text, json, html
- Thresholds: 80% for lines, functions, branches, statements
- Excludes: node_modules, test files, test directories

**Test Location**:
- Pattern: `**/__tests__/**/*.test.ts` and `**/__tests__/**/*.test.tsx`
- Current test files:
  - `hooks/__tests__/usePagination.test.ts`
  - `hooks/__tests__/useStatusPolling.test.ts`
  - `lib/__tests__/api.test.ts`
  - `lib/__tests__/formatters.test.ts`
  - `lib/__tests__/styleHelpers.test.ts`

**Run Commands** (`src/web/package.json`):
```bash
npm test         # Run all tests once
npm run test:watch   # Watch mode for development
npm run test:ui      # Interactive UI for test exploration
```

**Alias Resolution**:
- `@` alias resolves to `src/web/` directory
- Matches Next.js path configuration for consistency

**Benefits**:
- Fast test execution with Vitest (ESM-native)
- Interactive UI for debugging
- Coverage reporting for quality assurance
- Type-safe testing with TypeScript
