# Architecture

> This document uses the [C4 model](https://c4model.com/) to structure architecture diagrams.
> C4 defines four zoom levels: **Context** (L1) ‚Üí **Container** (L2) ‚Üí **Component** (L3) ‚Üí **Code** (L4).
> This document covers **L1** and **L2**. Component-level details (L3) are described in the subsequent sections (e.g., Dictionary Lookup Pipeline, CrewAI Pipeline). L4 is the source code itself.

## System Context Diagram (C4 Level 1)

The Context diagram shows OPAD as a single box and every person or external system it interacts with. Internal details (services, databases) are hidden at this level.

```mermaid
%%{init: {"flowchart": {"curve": "linear"}} }%%
graph TB
    learner(["üë§ Language Learner"])
    admin(["üë§ Admin"])

    opad["OPAD"]

    llm["LLM API"]
    freedict["Free Dictionary API"]
    serper["Serper API"]
    news["News Websites"]

    learner -->|reads articles, looks up words| opad
    admin -->|views token usage| opad

    opad -->|generates articles, extracts lemmas| llm
    opad -->|looks up definitions, forms| freedict
    opad -->|searches news| serper
    opad -->|scrapes articles| news
```

| Element | Type | Description |
|---------|------|-------------|
| **Language Learner** | Person | Reads adapted articles and looks up unknown words |
| **Admin** | Person | Monitors token usage and system health |
| **OPAD** | System (ours) | Transforms news ‚Üí educational reading materials; context-aware dictionary |
| **LLM API** | External System | OpenAI / Anthropic ‚Äî article generation, lemma extraction, sense selection |
| **Free Dictionary API** | External System | Definitions, IPA pronunciation, grammatical forms |
| **Serper API** | External System | Google News search for article discovery |
| **News Websites** | External System | Source article content via web scraping |

---

## Container Diagram (C4 Level 2)

### System Overview

```mermaid
graph TB
    Web[Web<br/>Next.js] -->|HTTP/Proxy| API[API<br/>FastAPI]
    API -->|RPUSH| Redis[(Redis<br/>Queue + Status)]
    Redis -->|BLPOP| Worker[Worker<br/>Python]
    Worker -->|Execute| CrewAI[CrewAI]
    Worker -->|Save| MongoDB[(MongoDB<br/>Article + Vocabulary)]

    API -.->|SET/GET| Redis
    Worker -.->|SET| Redis

    API -.->|Dictionary Lookup| LLM[LLM Provider]
    CrewAI -.->|Article Generation| LLM

    API -->|Save/Query| MongoDB

    style Web fill:#2196F3
    style API fill:#2196F3
    style Worker fill:#2196F3
    style CrewAI fill:#10a37f
    style Redis fill:#dc382d
    style MongoDB fill:#13aa52
    style LLM fill:#10a37f

    linkStyle 0 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 1 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 2 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 3 stroke:#10a37f,stroke-width:2px,color:#10a37f
    linkStyle 4 stroke:#13aa52,stroke-width:2px,color:#13aa52
    linkStyle 5 stroke:#ff9500,stroke-width:2px,color:#ff9500
    linkStyle 6 stroke:#ff9500,stroke-width:2px,color:#ff9500
    linkStyle 7 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
    linkStyle 8 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
    linkStyle 9 stroke:#13aa52,stroke-width:2px,color:#13aa52
```

### Article Generation ÌùêÎ¶Ñ

```mermaid
sequenceDiagram
    participant Web
    participant API
    participant Redis
    participant Worker
    participant CrewAI
    participant MongoDB
    
    Web->>API: POST /articles/generate
    API->>MongoDB: Check duplicates
    API->>MongoDB: Save article metadata
    API->>Redis: RPUSH job
    API-->>Web: Return job_id + article_id
    
    Worker->>Redis: BLPOP
    Redis-->>Worker: job_data
    
    Worker->>CrewAI: Execute crew.kickoff()
    CrewAI-->>Worker: Return article
    
    Worker->>MongoDB: Save article content
    Worker->>Redis: Update status (completed)
```

**ÌäπÏßï:**
- **Ïã§ÏãúÍ∞Ñ ÏùëÎãµ**: ÏÇ¨Ïö©ÏûêÍ∞Ä Îã®Ïñ¥Î•º ÌÅ¥Î¶≠ÌïòÎ©¥ Ï¶âÏãú Ï†ïÏùò Î∞òÌôò (ÎπÑÎèôÍ∏∞ ÌÅê ÏÇ¨Ïö© Ïïà Ìï®)
- **ÌîÑÎ°ùÏãú Ìå®ÌÑ¥**: Next.js API routeÍ∞Ä FastAPIÎ°ú ÏöîÏ≤≠ÏùÑ ÌîÑÎ°ùÏãú
- **Í≥µÌÜµ Ïú†Ìã∏ ÏÇ¨Ïö©**: `utils/lemma_extraction.py`, `utils/sense_selection.py`, `utils/llm.py`Î°ú Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Íµ¨Ï°∞
- **Stanza NLP**: ÎèÖÏùºÏñ¥ lemma Ï∂îÏ∂úÏóê Stanza NLP ÏÇ¨Ïö© (Î°úÏª¨ Ï≤òÎ¶¨, ~51ms), Í∏∞ÌÉÄ Ïñ∏Ïñ¥Îäî LLM ÏÇ¨Ïö©
- **ÏóêÎü¨ Ï≤òÎ¶¨**: `get_llm_error_response()`Î°ú ÏùºÍ¥ÄÎêú ÏóêÎü¨ ÏùëÎãµ

### ÏÑúÎπÑÏä§ Í∞Ñ ÌÜµÏã†

| From | To | Method | Purpose |
|------|-----|--------|---------|
| **Web** | **API** | HTTP | Article ÏÉùÏÑ±, Job enqueue, Token usage Ï°∞Ìöå |
| **Web** | **Next.js API** | HTTP | Dictionary API ÏöîÏ≤≠ (ÌîÑÎ°ùÏãú), Vocabulary CRUD ÏöîÏ≤≠ (ÌîÑÎ°ùÏãú), Dictionary Stats ÏöîÏ≤≠ (ÌîÑÎ°ùÏãú) |
| **Next.js API** | **API** | HTTP | Dictionary API ÌîÑÎ°ùÏãú ÏöîÏ≤≠, Vocabulary CRUD ÌîÑÎ°ùÏãú ÏöîÏ≤≠, Dictionary Stats ÌîÑÎ°ùÏãú ÏöîÏ≤≠ |
| **API** | **MongoDB** | (via Repository adapters) | Ï§ëÎ≥µ Ï≤¥ÌÅ¨, Article metadata Ï†ÄÏû•/Ï°∞Ìöå (ArticleRepository), Vocabulary Ï†ÄÏû•/Ï°∞Ìöå (VocabularyRepository), Token usage Ï†ÄÏû•/Ï°∞Ìöå (TokenUsageRepository), User Ïù∏Ï¶ù/Ï°∞Ìöå (UserRepository) |
| **API** | **Redis** | `RPUSH` | JobÏùÑ ÌÅêÏóê Ï∂îÍ∞Ä |
| **API** | **Redis** | `SET/GET` | Job ÏÉÅÌÉú Ï†ÄÏû•/Ï°∞Ìöå (Í≥µÌÜµ Î™®Îìà `api.job_queue` ÏÇ¨Ïö©) |
| **API** | **Stanza NLP** | Local (via utils.lemma_extraction) | German lemma extraction (Î°úÏª¨ NLP, ~51ms) |
| **API** | **LLM** | HTTP (via utils.llm) | Dictionary APIÏö© LLM Ìò∏Ï∂ú (non-German lemma extraction + CEFR estimation + entry/sense selection) + Token tracking |
| **API** | **API** | Internal | Token usage endpoints (`/usage/me`, `/usage/articles/{id}`) |
| **Worker** | **Redis** | `BLPOP` | JobÏùÑ ÌÅêÏóêÏÑú Í∫ºÎÉÑ (blocking) |
| **Worker** | **Redis** | `SET` | Job ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ (Í≥µÌÜµ Î™®Îìà `api.job_queue` ÏÇ¨Ïö©) |
| **Worker** | **CrewAI** | Function Call | Article ÏÉùÏÑ± |
| **Worker** | **MongoDB** | (via Repository adapters) | Article content Ï†ÄÏû• (ArticleRepository), Token usage Ï†ÄÏû• (TokenUsageRepository) |

**Ï∞∏Í≥†**: APIÏôÄ Worker Î™®Îëê `api.job_queue` Î™®ÎìàÏùÑ ÌÜµÌï¥ RedisÏóê Ï†ëÍ∑ºÌï©ÎãàÎã§. Î™®Îì† MongoDB Ï†ëÍ∑ºÏùÄ hexagonal architectureÏùò Repository Ïñ¥ÎåëÌÑ∞Î•º ÌÜµÌï¥ Ìï©ÎãàÎã§: `ArticleRepository`, `VocabularyRepository`, `TokenUsageRepository`, `UserRepository`. Í∞Å RepositoryÎäî `api/dependencies.py`(Composition Root)ÏóêÏÑú ÏÉùÏÑ±ÎêòÏñ¥ FastAPI `Depends()`Î°ú Ï£ºÏûÖÎê©ÎãàÎã§.

### Redis Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞

#### 1. Job Queue (List) - `opad:jobs`

**Ïö©ÎèÑ**: WorkerÍ∞Ä Ï≤òÎ¶¨Ìï† jobÎì§ÏùÑ FIFO ÏàúÏÑúÎ°ú Ï†ÄÏû•

```
Queue: opad:jobs (List)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [oldest] ‚Üê ... ‚Üê [newest]       ‚îÇ
‚îÇ    ‚Üë                    ‚Üë       ‚îÇ
‚îÇ  BLPOP              RPUSH       ‚îÇ
‚îÇ (Worker)             (API)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Îç∞Ïù¥ÌÑ∞ ÌòïÏãù**:
```json
{
  "job_id": "uuid",
  "article_id": "uuid",
  "inputs": {
    "language": "Korean",
    "level": "B1",
    "length": "300",
    "topic": "Climate Change"
  },
  "created_at": "2025-01-08T12:34:56.789Z"
}
```

#### 2. Job Status (String) - `opad:job:{job_id}`

**Ïö©ÎèÑ**: Í∞Å jobÏùò ÌòÑÏû¨ ÏÉÅÌÉúÏôÄ ÏßÑÌñâÎ•† Ï∂îÏ†Å

**TTL**: 24ÏãúÍ∞Ñ (ÏûêÎèô ÏÇ≠Ï†ú)

**Îç∞Ïù¥ÌÑ∞ ÌòïÏãù**:
```json
{
  "id": "job-uuid",
  "article_id": "article-uuid",
  "status": "running",
  "progress": 45,
  "message": "Adapting article...",
  "error": null,
  "created_at": "2025-01-08T12:34:56.789Z",
  "updated_at": "2025-01-08T12:35:12.345Z"
}
```

**Ï†ëÍ∑º Ìå®ÌÑ¥**:
- **API**: ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî (queued), Ï°∞Ìöå (GET)
- **Worker**: ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ (running, completed, failed)
- **Progress Listener**: ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏ (0-100%) - CrewAI Ïù¥Î≤§Ìä∏ Î¶¨Ïä§ÎÑàÎ•º ÌÜµÌï¥ Ïã§ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏

---

## üîë ÌïµÏã¨ Í∞úÎÖê

### 1. **ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Ï≤òÎ¶¨ (Async Job Processing)**
- **Î¨∏Ï†ú**: CrewAI Ïã§ÌñâÏùÄ 2-5Î∂Ñ Í±∏Î¶º ‚Üí HTTP ÏöîÏ≤≠Ïù¥ ÌÉÄÏûÑÏïÑÏõÉ
- **Ìï¥Í≤∞**: Job Queue Ìå®ÌÑ¥
  - ÏöîÏ≤≠ Ï¶âÏãú `jobId` Î∞òÌôò
  - Ïã§Ï†ú ÏûëÏóÖÏùÄ Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ï≤òÎ¶¨
  - ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Îäî job ÏÉÅÌÉúÎ•º Ìè¥ÎßÅ

### 2. **ÏÑúÎπÑÏä§ Î∂ÑÎ¶¨ (Service Separation)**
- **ÏõêÏπô**: "Ìïú Ïª®ÌÖåÏù¥ÎÑà = Ìïú Ïó≠Ìï†"
- **Ïû•Ï†ê**:
  - ÎèÖÎ¶ΩÏ†Å Ïä§ÏºÄÏùºÎßÅ (workerÎßå ÎäòÎ¶¨Î©¥ Îê®)
  - Ïû•Ïï† Í≤©Î¶¨ (worker Ï£ΩÏñ¥ÎèÑ web/apiÎäî Ï†ïÏÉÅ)
  - Î∞∞Ìè¨ Î∂ÑÎ¶¨ (apiÎßå ÏàòÏ†ïÌï¥ÎèÑ worker ÏòÅÌñ• ÏóÜÏùå)

### 3. **Job Queue (Redis)**
- **Ïó≠Ìï†**: ÏûëÏóÖ ÏöîÏ≤≠ÏùÑ ÌÅêÏóê ÎÑ£Í≥†, workerÍ∞Ä ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ï≤òÎ¶¨
- **ÏÉÅÌÉú**: `queued` ‚Üí `running` ‚Üí `completed` / `failed`
- **Ïû•Ï†ê**: Î∂ÄÌïò Î∂ÑÏÇ∞, Ïû¨ÏãúÎèÑ Í∞ÄÎä•, Ïö∞ÏÑ†ÏàúÏúÑ ÏÑ§Ï†ï Í∞ÄÎä•

### 4. **Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå**

#### MongoDB: Article Storage
- **Article metadata Î∞è content Ï†ÄÏû•** (`articles` Ïª¨Î†âÏÖò)
  - Ï§ëÎ≥µ Ï≤¥ÌÅ¨ (24ÏãúÍ∞Ñ ÎÇ¥ ÎèôÏùº ÏûÖÎ†• ÌååÎùºÎØ∏ÌÑ∞)
  - Article Ï°∞Ìöå Î∞è Î¶¨Ïä§Ìä∏

- **Vocabulary Ï†ÄÏû•** (`vocabularies` Ïª¨Î†âÏÖò)
  - Îã®Ïñ¥, lemma, Ï†ïÏùò, Î¨∏Ïû• Ïª®ÌÖçÏä§Ìä∏ Ï†ÄÏû•
  - `related_words` Î∞∞Ïó¥ Ìè¨Ìï® (Î∂ÑÎ¶¨ ÎèôÏÇ¨ Îì± Î≥µÏû°Ìïú Ïñ∏Ïñ¥ Íµ¨Ï°∞ ÏßÄÏõê)
  - ArticleÎ≥ÑÎ°ú Í∑∏Î£πÌôîÌïòÏó¨ Í¥ÄÎ¶¨

- **Token Usage Ï∂îÏ†Å** (`token_usage` Ïª¨Î†âÏÖò)
  - LLM API Ìò∏Ï∂ú Ïãú ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Î∞è ÎπÑÏö© Ï∂îÏ†Å
  - ÏÇ¨Ïö©ÏûêÎ≥Ñ, ÏûëÏóÖÎ≥Ñ (dictionary_search, article_generation) ÏßëÍ≥Ñ
  - ÏùºÎ≥Ñ ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ Î∞è ÎπÑÏö© Î∂ÑÏÑù
  
**Article Status** (MongoDB, ÏòÅÍµ¨ Ï†ÄÏû•):
- `running`: Article ÏÉùÏÑ± Ïãú Ï¥àÍ∏∞ ÏÉÅÌÉú (Ï≤òÎ¶¨ Ï§ë)
- `completed`: Article ÏÉùÏÑ± ÏôÑÎ£å
- `failed`: Article ÏÉùÏÑ± Ïã§Ìå®
- `deleted`: Article ÏÇ≠Ï†ú (soft delete)

**Status Flow:**
```
ÏÉùÏÑ± Ïãú: running
   ‚Üì
ÏôÑÎ£å: completed
Ïã§Ìå®: failed
```

#### Redis: Job Queue & Status
- **Queue**: `opad:jobs` (List) - WorkerÍ∞Ä Ï≤òÎ¶¨Ìï† jobÎì§ÏùÑ FIFO ÏàúÏÑúÎ°ú Ï†ÄÏû•
- **Status**: `opad:job:{job_id}` (String, 24h TTL) - JobÏùò Ïã§ÏãúÍ∞Ñ ÏÉÅÌÉú Ï∂îÏ†Å

**Job Status** (Redis, 24ÏãúÍ∞Ñ TTL):
- `queued`: JobÏù¥ ÌÅêÏóê Ï∂îÍ∞ÄÎê® (WorkerÍ∞Ä ÏïÑÏßÅ Ï≤òÎ¶¨ÌïòÏßÄ ÏïäÏùå)
- `running`: WorkerÍ∞Ä JobÏùÑ Ï≤òÎ¶¨ Ï§ë
- `completed`: Job Ï≤òÎ¶¨ ÏôÑÎ£å
- `failed`: Job Ï≤òÎ¶¨ Ïã§Ìå®

**Status Flow:**
```
queued ‚Üí running ‚Üí completed / failed
```

**Article Status vs Job Status:**
- **Article Status (MongoDB)**: ArticleÏùò ÏµúÏ¢Ö ÏÉÅÌÉú (ÏòÅÍµ¨ Ï†ÄÏû•)
- **Job Status (Redis)**: Job Ï≤òÎ¶¨Ïùò Ïã§ÏãúÍ∞Ñ ÏÉÅÌÉú (24ÏãúÍ∞Ñ ÌõÑ ÏûêÎèô ÏÇ≠Ï†ú)
- ArticleÏùÄ `running` ÏÉÅÌÉúÎ°ú ÏÉùÏÑ±ÎêòÍ≥†, JobÏù¥ ÏôÑÎ£åÎêòÎ©¥ `completed` ÎòêÎäî `failed`Î°ú ÏóÖÎç∞Ïù¥Ìä∏Îê®

---

## Hexagonal Architecture (Ports and Adapters)

### Overview

The project uses hexagonal architecture (ports and adapters) for all database entities. This pattern decouples business logic from infrastructure concerns (MongoDB, Redis) by introducing explicit boundaries between layers. All MongoDB access goes through Protocol-based repository interfaces, with concrete adapters in `adapter/mongodb/`.

**Motivation:**
- Enable unit testing without MongoDB (swap in `FakeArticleRepository`)
- Make infrastructure decisions (MongoDB vs PostgreSQL) implementation details, not architectural commitments
- Enforce consistent data access patterns across API and Worker services

**Dependency flow:**
```
api/worker (Driving) --> services --> domain <-- ports <-- adapters (Driven)
```

### Layer Structure

```mermaid
graph TB
    subgraph "Composition Root"
        APIRoot["api/dependencies.py<br/>(FastAPI Depends)"]
        WorkerRoot["worker/main.py<br/>(parameter injection)"]
    end

    subgraph "Service Layer"
        VocabService["vocabulary_service.py"]
        DictService["dictionary_service.py"]
    end

    subgraph "Domain Layer"
        Article["Article"]
        User["User"]
        Vocabulary["Vocabulary<br/>GrammaticalInfo<br/>VocabularyCount"]
        TokenUsage["TokenUsage<br/>TokenUsageSummary"]
    end

    subgraph "Port Layer"
        ArticlePort["ArticleRepository<br/>(Protocol)"]
        UserPort["UserRepository<br/>(Protocol)"]
        VocabPort["VocabularyRepository<br/>(Protocol)"]
        TokenPort["TokenUsageRepository<br/>(Protocol)"]
    end

    subgraph "Adapter Layer (adapter/mongodb/)"
        MongoArticle["MongoArticleRepository"]
        MongoUser["MongoUserRepository"]
        MongoVocab["MongoVocabularyRepository"]
        MongoToken["MongoTokenUsageRepository"]
        FakeArticle["FakeArticleRepository<br/>(in-memory, testing)"]
        Indexes["indexes.py<br/>(ensure_all_indexes)"]
        Stats["stats.py<br/>(get_database_stats)"]
    end

    subgraph "Consumers (Driving Side)"
        ArticlesRoute["api/routes/articles.py"]
        AuthRoute["api/routes/auth.py"]
        DictRoute["api/routes/dictionary.py"]
        UsageRoute["api/routes/usage.py"]
        StatsRoute["api/routes/stats.py"]
        Processor["worker/processor.py"]
    end

    APIRoot --> ArticlePort
    APIRoot --> UserPort
    APIRoot --> VocabPort
    APIRoot --> TokenPort
    WorkerRoot --> ArticlePort

    ArticlesRoute --> ArticlePort
    AuthRoute --> UserPort
    DictRoute --> VocabPort
    DictRoute --> TokenPort
    UsageRoute --> ArticlePort
    UsageRoute --> TokenPort
    Processor --> ArticlePort

    VocabService --> VocabPort
    DictService --> VocabPort

    ArticlePort -.->|implements| MongoArticle
    ArticlePort -.->|implements| FakeArticle
    UserPort -.->|implements| MongoUser
    VocabPort -.->|implements| MongoVocab
    TokenPort -.->|implements| MongoToken

    MongoArticle --> Article
    MongoUser --> User
    MongoVocab --> Vocabulary
    MongoToken --> TokenUsage

    style Article fill:#e3f2fd
    style User fill:#e3f2fd
    style Vocabulary fill:#e3f2fd
    style TokenUsage fill:#e3f2fd
    style ArticlePort fill:#fff3e0
    style UserPort fill:#fff3e0
    style VocabPort fill:#fff3e0
    style TokenPort fill:#fff3e0
    style MongoArticle fill:#e8f5e9
    style MongoUser fill:#e8f5e9
    style MongoVocab fill:#e8f5e9
    style MongoToken fill:#e8f5e9
    style FakeArticle fill:#e8f5e9
    style Indexes fill:#e8f5e9
    style Stats fill:#e8f5e9
    style APIRoot fill:#fce4ec
    style WorkerRoot fill:#fce4ec
    style VocabService fill:#f3e5f5
    style DictService fill:#f3e5f5
```

### Domain Models (`src/domain/model/`)

All domain models are plain Python dataclasses with no database dependencies.

#### Article (`domain/model/article.py`)

```python
class ArticleStatus(str, Enum):
    RUNNING = 'running'
    COMPLETED = 'completed'
    FAILED = 'failed'
    DELETED = 'deleted'

@dataclass(frozen=True)
class ArticleInputs:
    language: str
    level: str
    length: str
    topic: str

@dataclass
class Article:
    id: str
    inputs: ArticleInputs
    status: ArticleStatus
    created_at: datetime
    updated_at: datetime
    user_id: str | None = None
    job_id: str | None = None
    content: str | None = None
    started_at: datetime | None = None
```

#### User (`domain/model/user.py`)

```python
@dataclass
class User:
    id: str
    name: str
    email: str
    created_at: datetime
    updated_at: datetime
    last_login: datetime | None = None
    password_hash: str | None = None
    provider: str = 'email'
```

#### Vocabulary (`domain/model/vocabulary.py`)

```python
@dataclass
class GrammaticalInfo:
    pos: str | None = None
    gender: str | None = None
    phonetics: str | None = None
    conjugations: dict | None = None
    level: str | None = None
    examples: list[str] | None = None

@dataclass
class Vocabulary:
    id: str
    article_id: str
    word: str
    lemma: str
    definition: str
    sentence: str
    language: str
    created_at: datetime
    related_words: list[str] | None = None
    span_id: str | None = None
    user_id: str | None = None
    grammar: GrammaticalInfo = field(default_factory=GrammaticalInfo)

@dataclass
class VocabularyCount:
    vocabulary: Vocabulary
    count: int
    article_ids: list[str] = field(default_factory=list)
```

#### Token Usage (`domain/model/token_usage.py`)

```python
@dataclass
class TokenUsage:
    id: str
    user_id: str
    operation: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    estimated_cost: float
    created_at: datetime
    article_id: str | None = None
    metadata: dict = field(default_factory=dict)

@dataclass
class TokenUsageSummary:
    total_tokens: int
    total_cost: float
    by_operation: dict[str, OperationUsage] = field(default_factory=dict)
    daily_usage: list[DailyUsage] = field(default_factory=list)
```

**Design decisions:**
- `ArticleInputs` is `frozen=True` (immutable) because inputs never change after creation
- `ArticleStatus` extends `str` for JSON serialization compatibility
- `Article` is a mutable dataclass (status, content change during lifecycle)
- `GrammaticalInfo` groups optional grammatical metadata for vocabulary entries
- `VocabularyCount` is a composite model combining a vocabulary entry with its aggregated count

### Ports (`src/port/`)

Each port defines the contract for a domain entity's persistence using Python's `Protocol`:

#### ArticleRepository (`port/article_repository.py`)

```python
class ArticleRepository(Protocol):
    def save_metadata(self, article_id, inputs, status, ...) -> bool: ...
    def save_content(self, article_id, content, ...) -> bool: ...
    def get_by_id(self, article_id) -> Article | None: ...
    def find_many(self, skip, limit, status, ...) -> tuple[list[Article], int]: ...
    def find_duplicate(self, inputs, user_id, hours) -> Article | None: ...
    def update_status(self, article_id, status) -> bool: ...
    def delete(self, article_id) -> bool: ...
```

#### UserRepository (`port/user_repository.py`)

```python
class UserRepository(Protocol):
    def create(self, email, password_hash, name) -> User | None: ...
    def get_by_email(self, email) -> User | None: ...
    def get_by_id(self, user_id) -> User | None: ...
    def update_last_login(self, user_id) -> bool: ...
```

#### VocabularyRepository (`port/vocabulary_repository.py`)

```python
class VocabularyRepository(Protocol):
    def save(self, article_id, word, lemma, ...) -> str | None: ...
    def get_by_id(self, vocabulary_id) -> Vocabulary | None: ...
    def find(self, article_id, user_id, lemma) -> list[Vocabulary]: ...
    def update_span_id(self, vocabulary_id, span_id) -> None: ...
    def delete(self, vocabulary_id) -> bool: ...
    def count_by_lemma(self, language, user_id, skip, limit) -> list[VocabularyCount]: ...
    def find_lemmas(self, user_id, language, levels, limit) -> list[str]: ...
```

#### TokenUsageRepository (`port/token_usage_repository.py`)

```python
class TokenUsageRepository(Protocol):
    def save(self, user_id, operation, model, ...) -> str | None: ...
    def get_user_summary(self, user_id, days) -> TokenUsageSummary: ...
    def get_by_article(self, article_id) -> list[TokenUsage]: ...
```

**Why Protocol (structural typing)?**
- No need for adapters to explicitly inherit from an ABC
- Any class with matching method signatures satisfies the protocol
- Supports duck typing while providing IDE/mypy type checking

### Adapters (`src/adapter/mongodb/`)

All MongoDB adapters follow the same pattern:
- Receive a `pymongo.Database` instance via constructor injection
- Convert between MongoDB documents and domain objects via `_to_domain()` helper
- All methods return `bool`, domain objects, or `None` (never raw MongoDB documents)
- Each adapter has an `ensure_indexes()` method for index management

| Adapter | File | Collection | Domain Model |
|---------|------|------------|-------------|
| `MongoArticleRepository` | `article_repository.py` | `articles` | `Article` |
| `MongoUserRepository` | `user_repository.py` | `users` | `User` |
| `MongoVocabularyRepository` | `vocabulary_repository.py` | `vocabularies` | `Vocabulary` |
| `MongoTokenUsageRepository` | `token_usage_repository.py` | `token_usage` | `TokenUsage` |

**Shared utilities in `adapter/mongodb/`:**
- `indexes.py`: `create_index_safe()` with conflict resolution, `ensure_all_indexes(db)` called at app startup
- `stats.py`: `get_database_stats(db)` and `get_vocabulary_stats(db)` for the `/stats` endpoint
- `connection.py`: MongoDB client singleton (`get_mongodb_client()`, `DATABASE_NAME`)

#### FakeArticleRepository (`src/adapter/fake/article_repository.py`)

In-memory adapter for unit testing.

- Stores articles in a `dict[str, Article]` (no external dependencies)
- Implements the same `ArticleRepository` protocol as `MongoArticleRepository`
- Enables fast, deterministic tests without database setup

### Composition Root (Dependency Injection)

#### FastAPI (`src/api/dependencies.py`)

The composition root creates all repository instances. A private `_get_db()` helper centralizes the database connection:

```python
def _get_db():
    """Get MongoDB database, raising 503 if unavailable."""
    client = get_mongodb_client()
    if client is None:
        raise HTTPException(status_code=503, detail="Database unavailable")
    return client[DATABASE_NAME]

def get_article_repo() -> ArticleRepository:
    return MongoArticleRepository(_get_db())

def get_user_repo() -> UserRepository:
    return MongoUserRepository(_get_db())

def get_token_usage_repo() -> TokenUsageRepository:
    return MongoTokenUsageRepository(_get_db())

def get_vocab_repo() -> VocabularyRepository:
    return MongoVocabularyRepository(_get_db())
```

Used by FastAPI route handlers via `Depends()`:

```python
@router.get("/{article_id}")
async def get_article_endpoint(
    article_id: str,
    repo: ArticleRepository = Depends(get_article_repo),
):
    article = repo.get_by_id(article_id)
    ...
```

#### Worker (`src/worker/main.py`)

```python
def main():
    client = get_mongodb_client()
    db = client[DATABASE_NAME]
    repo = MongoArticleRepository(db)
    run_worker_loop(repo)  # repo passed as parameter
```

The worker creates the repository once at startup and passes it through the call chain: `main()` -> `run_worker_loop(repo)` -> `process_job(job_data, repo)` -> `JobContext.from_dict(job_data, repo)`.

### Repository Overview

All database entities use hexagonal architecture. MongoDB access goes through Protocol-based repository interfaces.

| Entity | Port | Adapter |
|--------|------|---------|
| Article | `ArticleRepository` | `MongoArticleRepository` |
| User | `UserRepository` | `MongoUserRepository` |
| Vocabulary | `VocabularyRepository` | `MongoVocabularyRepository` |
| Token Usage | `TokenUsageRepository` | `MongoTokenUsageRepository` |

**Additional components added during migration:**
- `adapter/mongodb/indexes.py`: Centralized index management with `ensure_all_indexes(db)`
- `adapter/mongodb/stats.py`: Database statistics for `/stats` endpoint
- `services/vocabulary_service.py`: Business logic for vocabulary operations (duplicate detection, CEFR level filtering)

---

## üí∞ Token Usage Tracking

### Overview
The system tracks LLM API token usage and costs for all API calls, enabling cost monitoring, user billing, and usage analytics.

### Architecture

#### 1. LLM Utility Module (`utils/llm.py`)
Provider-agnostic LLM API calls using LiteLLM with automatic token tracking.

**Functions**:
- `call_llm_with_tracking()`: Makes LLM API calls and returns content + token statistics
- `accumulate_stats()`: Combines multiple `TokenUsageStats` into one (sums tokens and costs)
- `parse_json_from_content()`: Parses JSON from LLM responses (handles markdown code blocks)
- `get_llm_error_response()`: Converts LLM exceptions to HTTP status codes

**TokenUsageStats Dataclass**:
```python
@dataclass
class TokenUsageStats:
    model: str              # Model name (e.g., "gpt-4.1-mini")
    prompt_tokens: int      # Input tokens
    completion_tokens: int  # Output tokens
    total_tokens: int       # Total tokens used
    estimated_cost: float   # Cost in USD (calculated by LiteLLM)
    provider: str | None    # Provider name (openai, anthropic, google)
```

**Supported Providers** (via LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

**Example Usage**:
```python
from utils.llm import call_llm_with_tracking, TokenUsageStats

content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# stats.model = "gpt-4.1-mini"
# stats.prompt_tokens = 8
# stats.completion_tokens = 12
# stats.estimated_cost = 0.000015
```

#### 2. MongoDB Storage (`adapter/mongodb/token_usage_repository.py`)

Token usage persistence is handled by `MongoTokenUsageRepository`, implementing the `TokenUsageRepository` protocol.

**save()**: Save token usage record
```python
def save(
    self,
    user_id: str,
    operation: str,  # "dictionary_search" | "article_generation"
    model: str,
    prompt_tokens: int,
    completion_tokens: int,
    estimated_cost: float,
    article_id: str | None = None,
    metadata: dict | None = None,
) -> str | None:
    """Save token usage record to MongoDB."""
```

**get_user_summary()**: Get user's token usage summary
```python
def get_user_summary(self, user_id: str, days: int = 30) -> TokenUsageSummary:
    """Returns TokenUsageSummary dataclass with aggregated usage data."""
```

**get_by_article()**: Get token usage for specific article
```python
def get_by_article(self, article_id: str) -> list[TokenUsage]:
    """Returns all TokenUsage domain objects for an article."""
```

#### 3. Token Usage Collection Schema (MongoDB)

```json
{
  "_id": "uuid",
  "user_id": "uuid",
  "operation": "dictionary_search | article_generation",
  "model": "string",
  "prompt_tokens": 100,
  "completion_tokens": 50,
  "total_tokens": 150,
  "estimated_cost": 0.00025,
  "article_id": "uuid (optional)",
  "metadata": {
    "query": "...",
    "language": "..."
  },
  "created_at": "datetime"
}
```

**Indexes**:
- `(user_id, created_at)`: User usage queries (descending)
- `article_id`: Article-specific queries (sparse)
- `created_at`: Time-based queries (descending)
- `(operation, created_at)`: Operation-type queries

### Integration

#### Dictionary API (`src/api/routes/dictionary.py`)

```python
@router.post("/search", response_model=SearchResponse)
async def search_word(
    request: SearchRequest,
    current_user: UserResponse = Depends(get_current_user_required),
    service: DictionaryService = Depends(get_dictionary_service),
    token_usage_repo: TokenUsageRepository = Depends(get_token_usage_repo),
):
    # Convert API request to service request
    lookup_request = LookupRequest(
        word=request.word,
        sentence=request.sentence,
        language=request.language,
        article_id=request.article_id
    )

    # Perform lookup via DictionaryService (hybrid LLM + API approach)
    result = await service.lookup(lookup_request)

    # Track token usage via injected repository
    stats = service.last_token_stats
    if stats:
        token_usage_repo.save(
            user_id=current_user.id,
            operation="dictionary_search",
            model=stats.model,
            prompt_tokens=stats.prompt_tokens,
            completion_tokens=stats.completion_tokens,
            estimated_cost=stats.estimated_cost,
            article_id=request.article_id,
            metadata={"word": request.word, "language": request.language}
        )

    return SearchResponse(...)
```

#### Token Usage API Endpoints (`src/api/routes/usage.py`)

**GET /usage/me**: Get current user's token usage summary
```python
@router.get("/me", response_model=TokenUsageSummary)
async def get_my_usage(
    days: int = Query(default=30, ge=1, le=365),
    current_user: UserResponse = Depends(get_current_user_required),
    repo: TokenUsageRepository = Depends(get_token_usage_repo),
):
    # Get aggregated summary via injected repository
    summary = repo.get_user_summary(user_id=current_user.id, days=days)

    # Convert to response models
    by_operation = {
        op_name: OperationUsage(**op_data)
        for op_name, op_data in summary.get('by_operation', {}).items()
    }
    daily_usage = [
        DailyUsage(**day) for day in summary.get('daily_usage', [])
    ]

    return TokenUsageSummary(
        total_tokens=summary.get('total_tokens', 0),
        total_cost=summary.get('total_cost', 0.0),
        by_operation=by_operation,
        daily_usage=daily_usage
    )
```

**GET /usage/articles/{article_id}**: Get token usage for specific article
```python
@router.get("/articles/{article_id}", response_model=list[TokenUsageResponse])
async def get_article_usage(
    article_id: str,
    current_user: UserResponse = Depends(get_current_user_required),
    article_repo: ArticleRepository = Depends(get_article_repo),
    token_usage_repo: TokenUsageRepository = Depends(get_token_usage_repo),
):
    # Verify article ownership via injected ArticleRepository
    article = article_repo.get_by_id(article_id)
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    if article.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="You don't have permission")

    # Get all usage records via injected TokenUsageRepository
    usage_records = token_usage_repo.get_by_article(article_id)

    return [asdict(record) for record in usage_records]
```

### Token Usage Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant WebUI as Web UI
    participant API as FastAPI
    participant Redis
    participant Worker
    participant CrewAI
    participant LiteLLM
    participant MongoDB
    participant LLM as LLM Provider

    Note over User,LLM: Dictionary Search with Token Tracking
    User->>WebUI: Click word
    WebUI->>API: POST /dictionary/search

    alt German language
        API->>API: Step 1a: Stanza NLP (lemma + related_words, ~51ms)
        API->>LLM: Step 1b: CEFR estimation (max_tokens=10)
        LLM-->>API: {level} + stats
    else Other languages
        API->>LLM: Step 1: LLM reduced prompt (max_tokens=200)
        LLM-->>API: {lemma, related_words, level} + stats
    end

    API->>API: Step 2: Free Dictionary API (all entries)
    API->>LLM: Step 3: Sense selection (X.Y.Z, max_tokens=10)
    LLM-->>API: Selected entry.sense.subsense + stats
    API->>MongoDB: save_token_usage() (accumulated stats)
    API-->>WebUI: Definition response

    Note over User,MongoDB: Article Generation with Token Tracking
    User->>WebUI: Generate article
    WebUI->>API: POST /articles/generate
    API->>Redis: Enqueue job
    Redis-->>Worker: Dequeue job
    Worker->>CrewAI: run_crew()
    CrewAI->>LLM: Multiple API calls (agents execute)
    LLM-->>CrewAI: Responses (usage tracked internally)
    CrewAI-->>Worker: CrewResult (with crew_instance)
    Worker->>Worker: result.get_agent_usage()
    Worker->>Worker: calculate_cost() per agent
    Worker->>MongoDB: save_crew_token_usage() (per agent)
    Worker->>MongoDB: Save article

    Note over User,MongoDB: Usage Summary Retrieval
    User->>WebUI: View usage page
    WebUI->>API: GET /usage/me?days=30
    API->>MongoDB: get_user_token_summary()
    MongoDB-->>API: Aggregated summary
    API-->>WebUI: TokenUsageSummary
    WebUI-->>User: Display usage charts

    Note over User,MongoDB: Article Usage Details
    User->>WebUI: View article usage
    WebUI->>API: GET /usage/articles/{id}
    API->>MongoDB: get_article_token_usage()
    MongoDB-->>API: Usage records (all LLM calls)
    API-->>WebUI: List of TokenUsageRecord
    WebUI-->>User: Display usage details
```

---

## üîß Worker Token Tracking

### Overview

The Worker service tracks token usage during CrewAI article generation using **CrewAI's built-in token tracking**, which provides reliable per-agent usage metrics.

### Architecture

**Data Flow:**
```
Worker ‚Üí process_job() ‚Üí run_crew() ‚Üí CrewAI agents execute
                                            ‚Üì
                                     CrewAI tracks usage internally
                                            ‚Üì
                                     CrewResult.get_agent_usage()
                                            ‚Üì
                                     calculate_cost() (LiteLLM pricing)
                                            ‚Üì
                                     save_crew_token_usage() ‚Üí MongoDB
```

**Key Design Decision:**
CrewAI manages LLM calls internally through its agent.llm instances. Each agent has a separate LLM instance with independent usage tracking via `agent.llm.get_token_usage_summary()`. This approach is preferred over LiteLLM callbacks because:
- CrewAI's internal tracking is more reliable for per-agent metrics
- No need to intercept LLM calls at the LiteLLM layer
- Simpler implementation without callback lifecycle management

### Token Usage Module

**File**: `src/utils/token_usage.py`

**Purpose**: Token usage utilities for cost calculation and tracking. Provides functions to calculate LLM costs using LiteLLM's pricing database and save CrewAI agent token usage to MongoDB.

**Functions**:

#### calculate_cost()

```python
def calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    """Calculate estimated cost using LiteLLM's pricing data.

    Note: LiteLLM pricing may become outdated. Costs are estimates only.

    Args:
        model: Model name (e.g., 'gpt-4.1', 'gpt-4.1-mini')
        prompt_tokens: Number of input tokens
        completion_tokens: Number of output tokens

    Returns:
        Estimated cost in USD, or 0.0 if pricing unavailable
    """
```

**Error Handling**:
- Returns 0.0 if model not in LiteLLM pricing database
- Gracefully handles KeyError, ValueError, AttributeError
- Logs unexpected errors at debug level

#### save_crew_token_usage()

```python
def save_crew_token_usage(
    result: CrewResult,
    user_id: str,
    article_id: str | None,
    job_id: str
) -> None:
    """Save token usage for each CrewAI agent to MongoDB.

    Uses CrewAI's built-in token tracking (agent.llm.get_token_usage_summary())
    to get per-agent, per-model usage metrics.
    """
```

**Behavior**:
- Iterates through all agents in CrewResult
- Skips agents with zero token usage
- Calculates cost using `calculate_cost()` with LiteLLM pricing
- Saves each agent's usage as separate MongoDB record
- Non-fatal: failures don't crash job processing

**Data Saved per Agent**:
```json
{
  "user_id": "user-uuid",
  "operation": "article_generation",
  "model": "gpt-4.1",
  "prompt_tokens": 2000,
  "completion_tokens": 1500,
  "estimated_cost": 0.0525,
  "article_id": "article-uuid",
  "metadata": {"job_id": "job-uuid"}
}
```

---

### CrewResult Class

**File**: `src/crew/main.py`

**Purpose**: Container for crew execution result with usage metrics extraction.

```python
class CrewResult:
    """Container for crew execution result and usage metrics."""

    def __init__(self, result, crew_instance):
        self.raw = result.raw
        self.result = result
        self.crew_instance = crew_instance

    def get_agent_usage(self) -> list[dict]:
        """Get token usage per agent with model info.

        Returns:
            List of dicts with agent_role, model, prompt_tokens,
            completion_tokens, total_tokens, successful_requests
        """
```

**Usage Example**:
```python
result = run_crew(inputs=ctx.inputs)
agent_usage = result.get_agent_usage()
# [
#   {'agent_role': 'News Researcher', 'model': 'gpt-4.1', 'prompt_tokens': 500, ...},
#   {'agent_role': 'Content Writer', 'model': 'gpt-4.1', 'prompt_tokens': 2000, ...},
#   ...
# ]
```

**Why CrewAI Built-in Tracking?**
- Each CrewAI agent has its own LLM instance with independent usage tracking
- `agent.llm.get_token_usage_summary()` provides accurate per-agent metrics
- More reliable than intercepting LLM calls at LiteLLM callback layer
- Simpler implementation without callback lifecycle management

---

## üìö Vocabulary-Aware Article Generation

### Overview
The system now supports vocabulary-aware article generation, where CrewAI adjusts content difficulty based on words the user has already learned.

### Vocabulary Features

#### 1. Dictionary API - Word Definition
- **POST /dictionary/search**: Get word definition and lemma using hybrid LLM + Free Dictionary API (entry+sense+subsense selection via X.Y.Z format)
- **Returns**: lemma, definition, related_words, pos, gender, phonetics, conjugations, level, examples
- **Auth**: Required (JWT) to prevent API abuse

#### 2. Vocabulary Storage
- **POST /dictionary/vocabulary**: Add a word to user's vocabulary
- **GET /dictionary/vocabularies**: Get aggregated vocabulary grouped by lemma with counts
- **DELETE /dictionary/vocabularies/{id}**: Delete a vocabulary word
- **Auth**: All vocabulary operations require authentication and are user-specific

#### 3. Article-Specific Vocabularies
- **GET /articles/{article_id}/vocabularies**: Get all vocabularies for a specific article
- **Response**: List of VocabularyResponse objects with word, lemma, definition, context, and metadata
- **Auth**: Users can only access vocabularies from their own articles

### Data Model

#### Vocabulary Collection (MongoDB)
```json
{
  "_id": "ObjectId",
  "article_id": "uuid",
  "user_id": "uuid",
  "word": "string",              // Original word clicked
  "lemma": "string",             // Dictionary form
  "definition": "string",        // Word definition
  "sentence": "string",          // Sentence context
  "language": "string",
  "related_words": ["string"],   // All forms in sentence (e.g., verbs with particles)
  "span_id": "string",           // Span ID from markdown for linking
  "created_at": "datetime",
  "pos": "string",               // Part of speech (noun, verb, adjective, etc.)
  "gender": "string",            // Grammatical gender (der/die/das for German, le/la for French, etc.)
  "phonetics": "string",         // IPA pronunciation (e.g., /h änt/) - English only
  "conjugations": {              // Verb conjugations or noun declensions (null if not applicable)
    "present": "string",
    "past": "string",
    "participle": "string",
    "auxiliary": "string",
    "genitive": "string",
    "plural": "string"
  },
  "level": "string",             // CEFR level (A1, A2, B1, B2, C1, C2)
  "examples": ["string"]         // Example sentences from dictionary
}
```

**Grammatical Metadata Fields:**
- `pos`: Part of speech classification (noun, verb, adjective, adverb, preposition, etc.)
- `gender`: Grammatical gender for nouns in gendered languages (German: der/die/das, French: le/la, Spanish: el/la). Null for non-gendered languages.
- `phonetics`: IPA pronunciation from Free Dictionary API. Only populated for English language lookups due to API accuracy.
- `conjugations`: Verb conjugation forms (present, past, participle, auxiliary) or noun declensions (genitive, plural). Null for other parts of speech.
- `level`: CEFR difficulty level (A1-C2) for vocabulary tracking and adaptive learning.
- `examples`: Example sentences from Free Dictionary API showing word usage in context.

#### GrammaticalInfo Domain Model (`src/domain/model/vocabulary.py`)

The `GrammaticalInfo` dataclass holds optional grammatical metadata for vocabulary entries:

```python
@dataclass
class GrammaticalInfo:
    pos: str | None = None
    gender: str | None = None
    phonetics: str | None = None
    conjugations: dict | None = None
    level: str | None = None
    examples: list[str] | None = None
```

**Usage (via vocabulary_service)**:
```python
from services import vocabulary_service
from domain.model.vocabulary import GrammaticalInfo

grammar = GrammaticalInfo(
    pos='noun',
    gender='der',
    phonetics='/h änt/',
    conjugations={'genitive': 'Hundes', 'plural': 'Hunde'},
    level='A1',
    examples=['Der Hund bellt.', 'Ich habe einen Hund.'],
)

vocabulary_service.save_vocabulary(
    repo,
    article_id=article_id,
    word='Hunde',
    lemma='Hund',
    definition='dog',
    sentence='Die Hunde spielen im Park.',
    language='German',
    user_id=user_id,
    grammar=grammar,
)
```

#### VocabularyCount Model (Aggregated Response)
- Groups vocabularies by lemma
- Returns count of how many times a lemma appears across articles
- Includes most recent definition and example sentence
- Lists all article_ids where lemma appears
- Includes grammatical metadata (pos, gender, conjugations, level) from most recent entry

#### API Model Enhancements

**Conjugations.__bool__()** (`src/api/models.py:18-20`):
- Enables truthiness checking: `if conjugations:` returns False when all fields (present, past, perfect) are None
- Simplifies validation logic by treating empty Conjugations as falsy
- Backend can check conjugation presence without explicit null checks

**VocabularyRequest.field_validator** (`src/api/models.py:106-116`):
- Automatic conversion from Conjugations model to dict before database storage
- Returns None if conjugations object is empty (using `__bool__` check)
- Handles both dict and Conjugations input types
- Prevents storing empty conjugation objects in MongoDB

### Vocabulary-Aware Generation Flow
1. User saves vocabulary words from articles (POST /dictionary/vocabulary)
2. Words stored with article context (sentence, span_id)
3. When generating new article, worker retrieves user's vocabulary list
4. CrewAI receives vocabulary list as constraint for content difficulty
5. Generated article uses different words/complexity for learned vocabulary
6. User can access article-specific vocabularies (GET /articles/{id}/vocabularies)

### Authentication
All vocabulary endpoints require JWT authentication. Users can only:
- Add/delete their own vocabulary
- View their own vocabulary lists
- Access vocabularies from their own articles

### CEFR Vocabulary Level Filtering

**Function**: `get_allowed_vocab_levels(target_level, max_above=1)`

**File**: `src/services/vocabulary_service.py`

**Purpose**: Filters vocabulary words to only include those appropriate for the target CEFR level when generating articles.

**Parameters**:
- `target_level`: Target CEFR level (A1, A2, B1, B2, C1, C2)
- `max_above`: Maximum levels above target to allow (default: 1)

**Returns**: List of allowed CEFR levels

**Examples**:
```python
from services.vocabulary_service import get_allowed_vocab_levels

get_allowed_vocab_levels('A2', max_above=1)  # Returns: ['A1', 'A2', 'B1']
get_allowed_vocab_levels('B1', max_above=1)  # Returns: ['A1', 'A2', 'B1', 'B2']
get_allowed_vocab_levels('C2', max_above=1)  # Returns: ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
```

**Usage in Worker**:
When generating articles, the worker fetches user vocabulary filtered by target level:
```python
from services.vocabulary_service import get_user_lemmas

lemmas = get_user_lemmas(
    repo,
    user_id=ctx.user_id,
    language=ctx.inputs['language'],
    target_level=ctx.inputs.get('level'),  # Filters vocab by CEFR level
    limit=50,
)
```

**Benefits**:
- Prevents too-difficult vocabulary from appearing in beginner articles
- Ensures vocabulary reinforcement matches the target difficulty
- Allows slight challenge with `max_above=1` parameter

---

## ü§ñ CrewAI Pipeline

### Agent Configuration

The CrewAI pipeline uses four specialized agents for article generation. Each agent is configured with a specific role, goal, and LLM model.

**Agents** (`src/crew/config/agents.yaml`):

| Agent | Role | Tools | LLM Model |
|-------|------|-------|-----------|
| `article_finder` | Searches for recent news articles and scrapes full article text | `SerperDevTool(search_type="news")`, `ScrapeWebsiteTool` | `openai/gpt-4.1-mini` |
| `article_picker` | Evaluates and selects the best article using priority-based ranking (topic > level > length); constrained to finder's output only | None (`memory=False`) | `openai/gpt-4.1` |
| `article_rewriter` | Adapts the article to target CEFR level with vocabulary reinforcement and anti-fabrication rules | None | `anthropic/claude-sonnet-4-20250514` |
| `article_reviewer` | Reviews for natural language quality; preserves direct quotes and author style | None | `anthropic/claude-sonnet-4-20250514` |

**Crew Configuration** (`crew.py`):
- **Process**: Sequential (tasks run in order)
- **Memory**: Disabled -- no `ShortTermMemory`, `LongTermMemory`, or `EntityMemory` is used. The `article_picker` agent also has `memory=False` explicitly set to prevent it from referencing previously seen articles across runs.

### Task Pipeline

Tasks execute sequentially, with each task building on the previous output:

```mermaid
graph LR
    A[find_news_articles] --> B[pick_best_article]
    B --> C[adapt_news_article]
    C --> D[review_article_quality]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#e8f5e9
    style D fill:#e8f5e9
```

**Tasks** (`src/crew/config/tasks.yaml`):

#### 1. find_news_articles
- **Agent**: `article_finder`
- **Tools**: `SerperDevTool(search_type="news")` for news-specific search, `ScrapeWebsiteTool` for full article text extraction
- **Description**: Searches for 3-5 recent news articles matching the topic in the target language. Uses the scraping tool to fetch the **full article text** from each URL (search snippets are not accepted as article content). Skips video pages, podcasts, image galleries, and non-text content. Only includes articles with at least 200 words of body text.
- **Output**: `NewsArticleList` (JSON with articles array including full `content` field)
- **Guardrail**: `repair_json_output` for JSON validation

#### 2. pick_best_article
- **Agent**: `article_picker`
- **Description**: Selects the best article **only from the finder's output** (never invents or searches for additional articles). Ranks candidates using priority-based criteria: (1) topic relevance, (2) CEFR difficulty level, (3) approximate word length. Prefers single-topic articles over roundups/compilations. Verifies article existence at source URL and validates author names against the source page before selection.
- **Context**: `find_news_articles`
- **Output**: `SelectedArticle` (JSON with article and selection_rationale)
- **Guardrail**: `repair_json_output` for JSON validation

#### 3. adapt_news_article
- **Agent**: `article_rewriter`
- **Description**: Rewrites the selected article to match the target CEFR level
- **Context**: `pick_best_article`
- **Features**:
  - Vocabulary reinforcement using user's learned words
  - Source attribution (name, URL, date, author if verified)
  - Markdown formatting without word highlighting
  - **Anti-fabrication**: Never fabricates information not present in the original article. If the original is short, keeps the rewrite short rather than padding with invented content.
- **Output**: Markdown text

#### 4. review_article_quality
- **Agent**: `article_reviewer`
- **Description**: Reviews the adapted article for natural language quality and level appropriateness
- **Context**: `adapt_news_article`
- **Features**:
  - Corrects grammar errors and improves sentence flow
  - Simplifies vocabulary that is too difficult for the target CEFR level
  - **Preserves direct quotes** from people in the original article exactly as written
  - **Preserves author style**: only fixes clear errors, does not rephrase stylistic choices
  - Preserves all source information and markdown structure
- **Output**: `ReviewedArticle` (JSON with article_content and replaced_sentences)
- **Guardrail**: `repair_json_output` for JSON validation

### Pydantic Output Models

**File**: `src/crew/models.py`

#### ReviewedArticle
Final output from the review task:
```python
class ReviewedArticle(BaseModel):
    """A reviewed news article with review rationale"""
    article_content: str = Field(description="The final polished article in markdown format")
    replaced_sentences: list[ReplacedSentence] = Field(
        description="List of sentences that were replaced during review",
        default=[]
    )
```

#### ReplacedSentence
Tracks modifications made during review:
```python
class ReplacedSentence(BaseModel):
    """Replaced sentence information"""
    original: str = Field(description="The original sentence before replacement")
    replaced: str = Field(description="The sentence after replacement")
    rationale: str = Field(description="Reason for the replacement")
```

### Worker Integration

**File**: `src/worker/processor.py`

The worker processes the CrewAI result and extracts the reviewed article. It uses the injected `ArticleRepository` (via `ctx.repo`) instead of calling `utils.mongodb` directly:

```python
# Log replaced sentences from review
reviewed = result.pydantic
if isinstance(reviewed, ReviewedArticle) and reviewed.replaced_sentences:
    for change in reviewed.replaced_sentences:
        logger.info(
            f"Sentence replaced: '{change.original}' -> '{change.replaced}' ({change.rationale})",
            extra=ctx.log_extra
        )

# Save to MongoDB via ArticleRepository (injected)
content = reviewed.article_content
if not repo.save_content(ctx.article_id, content, ctx.started_at):
    ctx.mark_failed('Failed to save article to database', 'MongoDB save error')
    return False
```

**Benefits of the Review Step**:
- Catches unnatural expressions that slip through the rewriting phase
- Ensures vocabulary matches the target CEFR level
- Improves sentence flow and readability
- Logs all changes for quality tracking and debugging

---

## üìÅ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞

```
opad/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ domain/           # Domain layer (hexagonal architecture)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ article.py       # Article, ArticleInputs, ArticleStatus
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ user.py          # User
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vocabulary.py    # Vocabulary, GrammaticalInfo, VocabularyCount
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ token_usage.py   # TokenUsage, TokenUsageSummary, OperationUsage, DailyUsage
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ port/             # Port layer (hexagonal architecture)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ article_repository.py       # ArticleRepository Protocol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_repository.py          # UserRepository Protocol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vocabulary_repository.py    # VocabularyRepository Protocol
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ token_usage_repository.py   # TokenUsageRepository Protocol
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ adapter/          # Adapter layer (hexagonal architecture)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mongodb/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection.py              # MongoDB client (get_mongodb_client)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ article_repository.py      # MongoArticleRepository
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_repository.py         # MongoUserRepository
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vocabulary_repository.py   # MongoVocabularyRepository
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ token_usage_repository.py  # MongoTokenUsageRepository
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexes.py                 # Shared index management (ensure_all_indexes)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stats.py                   # Database statistics (get_database_stats)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fake/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ article_repository.py      # FakeArticleRepository (testing)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ user_repository.py         # FakeUserRepository (testing)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vocabulary_repository.py   # FakeVocabularyRepository (testing)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ token_usage_repository.py  # FakeTokenUsageRepository (testing)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/              # API ÏÑúÎπÑÏä§ (FastAPI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py       # FastAPI Ïï± ÏßÑÏûÖÏ†ê (lifespan pattern)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py  # Composition root (all repository factories)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py     # Pydantic Î™®Îç∏ (Article, Job)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/       # API ÏóîÎìúÌè¨Ïù∏Ìä∏
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articles.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jobs.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stats.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dictionary.py  # Dictionary API (word definition)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ job_queue.py  # Redis ÌÅê Í¥ÄÎ¶¨
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ worker/           # Worker ÏÑúÎπÑÏä§ (Python)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py       # Worker ÏßÑÏûÖÏ†ê (composition root)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor.py  # Job Ï≤òÎ¶¨ Î°úÏßÅ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context.py    # JobContext helper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tests/        # Worker tests
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ web/              # Web ÏÑúÎπÑÏä§ (Next.js)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/          # Next.js App Router
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/      # API Routes (ÌîÑÎ°ùÏãú)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articles/ # Article pages
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vocabulary/ # Vocabulary pages
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx  # Î©îÏù∏ ÌéòÏù¥ÏßÄ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/   # React Ïª¥Ìè¨ÎÑåÌä∏
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ArticleCard.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EmptyState.tsx      # Reusable empty state
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ErrorAlert.tsx      # Reusable error alert
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MarkdownViewer.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VocabularyCard.tsx  # Unified vocabulary display component
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ VocabularyList.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/        # Custom React hooks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useAsyncFetch.ts    # Generic fetch with loading/error
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ usePagination.ts    # Pagination calculations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useStatusPolling.ts # Job status polling
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useVocabularyDelete.ts # Vocabulary deletion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/          # Frontend utilities
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.ts           # fetchWithAuth, parseErrorResponse
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.ts          # Auth utilities
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatters.ts    # Date formatting utilities
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styleHelpers.ts  # CEFR color/label helpers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/        # TypeScript type definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.ts # Tailwind config with safelist
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ crew/             # CrewAI Î°úÏßÅ (Í≥µÏú†)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crew.py       # ReadingMaterialCreator ÌÅ¥ÎûòÏä§ (agents + tasks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py       # run() Ìï®Ïàò (CrewAI Ïã§Ìñâ ÏóîÌä∏Î¶¨Ìè¨Ïù∏Ìä∏)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py     # Pydantic Î™®Îç∏ (NewsArticle, ReviewedArticle Îì±)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardrails.py # JSON Ï∂úÎ†• Î≥µÍµ¨ guardrail
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ progress_listener.py  # JobProgressListener (Ïù¥Î≤§Ìä∏ Î¶¨Ïä§ÎÑà)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config/       # YAML ÏÑ§Ï†ï
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agents.yaml  # ÏóêÏù¥Ï†ÑÌä∏ Ï†ïÏùò (article_finder, article_picker, article_rewriter, article_reviewer)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tasks.yaml   # ÌÉúÏä§ÌÅ¨ Ï†ïÏùò (find_news_articles, pick_best_article, adapt_news_article, review_article_quality)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/         # ÏÑúÎπÑÏä§ Í≥ÑÏ∏µ (business logic)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dictionary_service.py  # Dictionary lookup orchestrator (hybrid pipeline)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vocabulary_service.py  # Vocabulary business logic (save, counts, CEFR filtering)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ (Í≥µÏú†)
‚îÇ       ‚îú‚îÄ‚îÄ logging.py    # Structured logging ÏÑ§Ï†ï
‚îÇ       ‚îú‚îÄ‚îÄ llm.py        # OpenAI API Í≥µÌÜµ Ìï®Ïàò
‚îÇ       ‚îú‚îÄ‚îÄ prompts.py    # Full LLM fallback ÌîÑÎ°¨ÌîÑÌä∏Îßå Ìè¨Ìï®
‚îÇ       ‚îú‚îÄ‚îÄ lemma_extraction.py  # Step 1: Lemma extraction (Stanza for German, LLM for others)
‚îÇ       ‚îú‚îÄ‚îÄ sense_selection.py   # Step 3: Sense selection from dictionary entries via LLM
‚îÇ       ‚îú‚îÄ‚îÄ dictionary_api.py    # Free Dictionary API wrapper (uses language_metadata for gender/reflexive/phonetics data)
‚îÇ       ‚îú‚îÄ‚îÄ language_metadata.py # Pure data constants (GENDER_MAP, REFLEXIVE_PREFIXES, REFLEXIVE_SUFFIXES, PHONETICS_SUPPORTED)
‚îÇ       ‚îî‚îÄ‚îÄ token_usage.py # Token usage calculation and tracking
‚îÇ
‚îî‚îÄ‚îÄ Dockerfile.*          # ÏÑúÎπÑÏä§Î≥Ñ Dockerfile (Ïù¥Ïäà #9)
```

### ÏÑúÎπÑÏä§ Íµ¨Î∂Ñ
| Ìè¥Îçî | Ïó≠Ìï† | Îü∞ÌÉÄÏûÑ | Ìè¨Ìä∏ |
|------|------|--------|------|
| `src/domain/` | Domain models (Article, User, Vocabulary, TokenUsage) | - | - |
| `src/port/` | Port definitions (ArticleRepository, UserRepository, VocabularyRepository, TokenUsageRepository) | - | - |
| `src/adapter/` | Infrastructure adapters (MongoDB, Fake) | - | - |
| `src/services/` | Business logic (vocabulary_service, dictionary_service) | - | - |
| `src/api/` | CRUD + Job enqueue + Dictionary API | Python (FastAPI) | 8001 (default) |
| `src/worker/` | CrewAI Ïã§Ìñâ + Job/Token Tracking | Python | - |
| `src/web/` | UI | Node.js (Next.js) | 3000 |
| `src/crew/` | CrewAI Î°úÏßÅ (Í≥µÏú†) | - | - |
| `src/utils/` | Í≥µÌÜµ Ïú†Ìã∏ (Í≥µÏú†) | - | - |

### Worker Î™®Îìà Íµ¨ÏÑ±
| ÌååÏùº | Ïó≠Ìï† | ÏùòÏ°¥ÏÑ± |
|------|------|--------|
| `worker/main.py` | Worker ÏßÑÏûÖÏ†ê + composition root (MongoArticleRepository ÏÉùÏÑ±) | `processor.py`, `adapter/mongodb/` |
| `worker/processor.py` | Job Ï≤òÎ¶¨ Î°úÏßÅ (process_job) -- `ArticleRepository` Ï£ºÏûÖ Î∞õÏùå | `crew/main.py`, `utils/token_usage.py`, `port/article_repository.py` |
| `worker/context.py` | JobContext helper (job data validation) -- `ArticleRepository` Ï£ºÏûÖ Î∞õÏùå | `api/job_queue.py`, `port/article_repository.py` |
| `crew/progress_listener.py` | JobProgressListener (CrewAI event listener) | `api/job_queue.py` |

### CrewAI Î™®Îìà Íµ¨ÏÑ±
| ÌååÏùº | Ïó≠Ìï† | Ï∂úÎ†• Î™®Îç∏ |
|------|------|----------|
| `crew/crew.py` | ReadingMaterialCreator ÌÅ¥ÎûòÏä§ (agents + tasks Ï†ïÏùò) | - |
| `crew/main.py` | `run()` Ìï®Ïàò - CrewAI Ïã§Ìñâ ÏóîÌä∏Î¶¨Ìè¨Ïù∏Ìä∏ | `CrewResult` |
| `crew/models.py` | Pydantic Ï∂úÎ†• Î™®Îç∏ Ï†ïÏùò | `NewsArticleList`, `SelectedArticle`, `ReviewedArticle` |
| `crew/guardrails.py` | JSON Ï∂úÎ†• Î≥µÍµ¨ guardrail | - |
| `crew/config/agents.yaml` | ÏóêÏù¥Ï†ÑÌä∏ ÏÑ§Ï†ï (role, goal, backstory, llm) | - |
| `crew/config/tasks.yaml` | ÌÉúÏä§ÌÅ¨ ÏÑ§Ï†ï (description, expected_output, context) | - |

---

## üîë Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà

### LLM Ïú†Ìã∏Î¶¨Ìã∞ (`utils/llm.py`)
Provider-agnostic LLM API Ìò∏Ï∂ú Î∞è ÌÜ†ÌÅ∞ Ï∂îÏ†ÅÏùÑ ÏúÑÌïú Í≥µÌÜµ Ìï®ÏàòÎì§ (LiteLLM Í∏∞Î∞ò):

- **`call_llm_with_tracking()`**: LLM API Ìò∏Ï∂ú + ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï∂îÏ†Å (Î≤îÏö© Ìï®Ïàò)
  - Î∞òÌôòÍ∞í: `(content: str, stats: TokenUsageStats)`
  - OpenAI, Anthropic, Google Îì± Îã§ÏñëÌïú ÌîÑÎ°úÎ∞îÏù¥Îçî ÏßÄÏõê
  - ÏûêÎèô ÎπÑÏö© Í≥ÑÏÇ∞ (LiteLLM ÎÇ¥Ïû• Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ¨Ïö©)
- **`TokenUsageStats`**: ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ dataclass
  - ÌïÑÎìú: `model`, `prompt_tokens`, `completion_tokens`, `total_tokens`, `estimated_cost`, `provider`
- **`accumulate_stats()`**: Ïó¨Îü¨ `TokenUsageStats`Î•º ÌïòÎÇòÎ°ú Ìï©ÏÇ∞ (ÌÜ†ÌÅ∞ Ïàò Î∞è ÎπÑÏö© Ìï©ÏÇ∞)
- **`parse_json_from_content()`**: LLM ÏùëÎãµÏóêÏÑú JSON ÌååÏã± (Îã§ÏñëÌïú ÌòïÏãù ÏßÄÏõê)
  - ÏùºÎ∞ò JSON, ÎßàÌÅ¨Îã§Ïö¥ ÏΩîÎìú Î∏îÎ°ù (```json), ÌÖçÏä§Ìä∏ ÎÇ¥ JSON Ï∂îÏ∂ú ÏßÄÏõê
- **`get_llm_error_response()`**: LLM Í¥ÄÎ†® ÏòàÏô∏Î•º HTTP ÏÉÅÌÉú ÏΩîÎìúÎ°ú Î≥ÄÌôò

**ÏÇ¨Ïö© ÏòàÏãú:**
```python
from utils.llm import call_llm_with_tracking, parse_json_from_content

# LLM Ìò∏Ï∂ú + ÌÜ†ÌÅ∞ Ï∂îÏ†Å
content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Î°úÍπÖ
logger.info("Token usage", extra=stats.to_dict())

# JSON ÌååÏã±
result = parse_json_from_content(content)
```

**ÏßÄÏõê ÌîÑÎ°úÎ∞îÏù¥Îçî** (LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

### Dictionary Lookup Pipeline Modules

The dictionary lookup uses a 3-step pipeline, split across dedicated modules:

#### Step 1: Lemma Extraction (`utils/lemma_extraction.py`)
Extracts lemma + related_words + CEFR level from a word in context.

- **`extract_lemma()`**: Main entry point. German uses Stanza NLP (local, ~51ms); other languages use LLM reduced prompt (~800ms).
- **`preload_stanza()`**: Eagerly loads the Stanza German pipeline at API startup (~349MB).
- **Stanza path (German)**: Dependency parsing for lemma + related_words, then a tiny LLM call for CEFR level estimation (max_tokens=10).
- **LLM path (other languages)**: Language-specific reduced prompts (`_build_reduced_prompt_en`, `_build_reduced_prompt_de` fallback, `_build_reduced_prompt_generic`).

**ÏÇ¨Ïö© ÏòàÏãú:**
```python
from utils.lemma_extraction import extract_lemma

# German: Stanza NLP + CEFR LLM call
result, stats = await extract_lemma(
    word="h√§ngt",
    sentence="Diese gro√üe Spanne h√§ngt von mehreren Faktoren ab.",
    language="German",
)
# result = {"lemma": "abh√§ngen", "related_words": ["h√§ngt", "ab"], "level": "B1"}

# English: LLM reduced prompt
result, stats = await extract_lemma(
    word="gave",
    sentence="She gave up smoking.",
    language="English",
)
# result = {"lemma": "give up", "related_words": ["gave", "up"], "level": "B1"}
```

#### Step 2: Sense Selection (`utils/sense_selection.py`)
Selects the best entry/sense/subsense from Free Dictionary API entries using LLM.

- **`select_best_sense()`**: Given dictionary entries, selects the best sense matching the word usage in context.
- **Trivial skip**: If only 1 entry with 1 sense and no subsenses, skips LLM call entirely.
- **X.Y.Z format**: LLM responds with entry.sense.subsense index (max_tokens=10).
- **`SenseResult`**: Dataclass with `entry_idx`, `sense_idx`, `subsense_idx`, `definition`, `examples`, `stats`.

**ÏÇ¨Ïö© ÏòàÏãú:**
```python
from utils.sense_selection import select_best_sense

sense = await select_best_sense(
    sentence="I saw the dog in the park.",
    word="saw",
    entries=api_entries,  # from Free Dictionary API
)
# sense.entry_idx = 0, sense.definition = "past tense of see"
```

#### Full LLM Fallback (`utils/prompts.py`)
Only contains the full LLM fallback prompt (used when hybrid pipeline fails).

- **`build_word_definition_prompt()`**: Full LLM fallback prompt generating all fields (lemma, definition, pos, gender, conjugations, level).
- Reduced prompts (lemma extraction) have moved to `utils/lemma_extraction.py`.
- Sense selection prompts have moved to `utils/sense_selection.py`.

**ÏÇ¨Ïö© ÏòàÏãú:**
```python
from utils.prompts import build_word_definition_prompt

# Full LLM fallback: all fields
full_prompt = build_word_definition_prompt(
    language="German",
    sentence="Diese gro√üe Spanne h√§ngt von mehreren Faktoren ab.",
    word="h√§ngt"
)
```

#### Language Metadata (`utils/language_metadata.py`)
Pure data module containing language-specific constants used by `dictionary_api.py`. No logic -- only data definitions.

- **`GENDER_MAP`**: Maps gender keywords to grammatical articles per language (e.g., `"masculine"` -> `"der"` for German, `"le"` for French, `"el"` for Spanish).
- **`REFLEXIVE_PREFIXES`**: Reflexive pronoun prefixes to strip before API lookup (e.g., `"sich "` for German, `"se "` / `"s'"` for French).
- **`REFLEXIVE_SUFFIXES`**: Reflexive verb suffix patterns (e.g., Spanish `"arse"`, `"erse"`, `"irse"` -> strip last 2 chars).
- **`PHONETICS_SUPPORTED`**: Set of language codes that support IPA phonetics from Free Dictionary API (currently only `"en"`).

**ÏÇ¨Ïö© ÏòàÏãú:**
```python
from utils.language_metadata import (
    GENDER_MAP, REFLEXIVE_PREFIXES, REFLEXIVE_SUFFIXES, PHONETICS_SUPPORTED,
)

# Gender lookup
GENDER_MAP["de"]["masculine"]  # "der"
GENDER_MAP["fr"]["feminine"]   # "la"

# Reflexive prefix check
REFLEXIVE_PREFIXES["de"]  # ["sich ", "mich ", "dich ", "uns ", "euch "]

# Phonetics support check
"en" in PHONETICS_SUPPORTED  # True
"de" in PHONETICS_SUPPORTED  # False
```

---

## üì° Dictionary API

### Hybrid Dictionary Lookup Architecture

The dictionary search uses a 3-step pipeline combining Stanza NLP (German), LLM capabilities, and the Free Dictionary API. The pipeline is split across dedicated modules: `utils/lemma_extraction.py` (Step 1), `utils/sense_selection.py` (Step 3), and `services/dictionary_service.py` (orchestrator).

#### Overview

```mermaid
graph TB
    subgraph "Hybrid Dictionary Lookup Pipeline"
        Request[POST /dictionary/search] --> LangCheck{German?}

        LangCheck -->|Yes| Stanza[Step 1a: Stanza NLP<br/>lemma + related_words<br/>~51ms local]
        Stanza --> CEFR[Step 1b: LLM CEFR<br/>gpt-4.1-mini, max_tokens=10]
        CEFR --> DictAPI[Step 2: Free Dictionary API]

        LangCheck -->|No| LLM[Step 1: LLM Reduced Prompt<br/>gpt-4.1-mini, max_tokens=200]
        LLM --> DictAPI

        DictAPI --> |all entries| SenseSelect[Step 3: LLM Sense Selection<br/>X.Y.Z format, max_tokens=10]
        SenseSelect --> Metadata[Step 3b: Extract Metadata<br/>from selected entry]
        Metadata --> Merge[Step 4: Merge Results]

        Merge --> Response[SearchResponse]

        Stanza -.-> |Stanza failure| LLM
        LLM -.-> |LLM failure| Fallback[Full LLM Fallback<br/>gpt-4.1-mini, max_tokens=2000]
        DictAPI -.-> |404 / timeout / no entries| Fallback
        Fallback --> Response
    end

    style Request fill:#2196F3
    style LangCheck fill:#e3f2fd
    style Stanza fill:#9c27b0
    style CEFR fill:#10a37f
    style LLM fill:#10a37f
    style DictAPI fill:#ff9500
    style SenseSelect fill:#10a37f
    style Metadata fill:#9c27b0
    style Merge fill:#9c27b0
    style Response fill:#13aa52
    style Fallback fill:#dc382d
```

#### Data Flow

```mermaid
sequenceDiagram
    participant Client
    participant Service as DictionaryService<br/>(orchestrator)
    participant Stanza as Stanza NLP<br/>(German only)
    participant LLM as LLM<br/>(gpt-4.1-mini)
    participant FreeDict as Free Dictionary API

    Client->>Service: POST {word, sentence, language}

    alt German language
        Note over Service,Stanza: Step 1a: Stanza lemma extraction (~51ms)
        Service->>Stanza: pipeline(sentence)
        Stanza-->>Service: {lemma, related_words}
        Note over Service,LLM: Step 1b: CEFR estimation
        Service->>LLM: CEFR prompt (max_tokens=10)
        LLM-->>Service: {level}
    else Other languages
        Note over Service,LLM: Step 1: LLM reduced prompt
        Service->>LLM: Reduced prompt (max_tokens=200)
        LLM-->>Service: {lemma, related_words, level}
    end

    Note over Service,FreeDict: Step 2: Lookup using lemma
    Service->>FreeDict: GET /api/v1/entries/{lang}/{lemma}
    FreeDict-->>Service: {all_entries}

    alt API returns entries
        alt multiple entries/senses/subsenses
            Note over Service,LLM: Step 3: Select best sense via LLM
            Service->>LLM: X.Y.Z selection prompt (max_tokens=10)
            LLM-->>Service: Selected X.Y.Z (entry.sense.subsense)
        end
        Note over Service: Step 3b: Extract metadata from selected entry
        Note over Service: Step 4: Merge results
        Service-->>Client: SearchResponse (source: hybrid)
    else API failure (404, timeout, no entries)
        Note over Service,LLM: Fallback: Full LLM prompt
        Service->>LLM: Full prompt (max_tokens=2000)
        LLM-->>Service: {lemma, definition, pos, gender, conjugations, level}
        Service-->>Client: SearchResponse (source: llm)
    end
```

#### Pipeline Modules

| Module | File | Responsibility | Output |
|--------|------|----------------|--------|
| **Lemma Extraction** | `utils/lemma_extraction.py` | Step 1: Extract lemma + related_words + CEFR level. German uses Stanza NLP (~51ms), others use LLM reduced prompt (~800ms) | `{"lemma", "related_words", "level"}` + `TokenUsageStats` |
| **Sense Selection** | `utils/sense_selection.py` | Step 3: Select best entry/sense/subsense from dictionary entries via LLM (X.Y.Z format, max_tokens=10) | `SenseResult(entry_idx, sense_idx, subsense_idx, definition, examples, stats)` |
| **Dictionary API** | `utils/dictionary_api.py` | Free Dictionary API wrapper; owns gender extraction, reflexive pronoun stripping, phonetics filtering, and form extraction logic (uses `language_metadata.py` for data constants) | `DictionaryAPIResult` |
| **Language Metadata** | `utils/language_metadata.py` | Pure data constants: `GENDER_MAP`, `REFLEXIVE_PREFIXES`, `REFLEXIVE_SUFFIXES`, `PHONETICS_SUPPORTED` | Data only (no logic) |
| **Dictionary Service** | `services/dictionary_service.py` | Orchestrator: wires Step 1 -> API -> Step 3, handles fallback | `LookupResult` |
| **Full LLM Fallback** | `utils/prompts.py` | Fallback prompt when hybrid pipeline fails (max_tokens=2000) | All fields except `phonetics` and `examples` |

#### Components

| Component | Responsibility | Output Fields |
|-----------|----------------|---------------|
| **Stanza NLP (German)** | Local dependency parsing for lemma + related_words (~51ms, ~349MB model) | `lemma`, `related_words` |
| **LLM (CEFR Estimation)** | Tiny LLM call for CEFR level on Stanza path (max_tokens=10) | `level` |
| **LLM (Reduced Prompt)** | Context-aware lemma extraction for non-German languages (max_tokens=200) | `lemma`, `related_words`, `level` |
| **Free Dictionary API** | Returns all dictionary entries with senses and subsenses | `all_entries` (containing `pos`, `senses`, `phonetics`, `forms` per entry) |
| **LLM (Sense Selection)** | Selects best entry+sense+subsense using X.Y.Z format based on sentence context (max_tokens=10) | `definition`, `examples` (from selected sense/subsense) |
| **Metadata Extraction** | Extracts POS, phonetics, forms, gender from the selected entry (not always entries[0]). Logic lives in `dictionary_api.py`, data constants in `language_metadata.py` | `pos`, `phonetics`, `forms`, `gender` |
| **Merge Function** | Combines results, converts `forms` to `conjugations` format, accumulates token stats | All fields |
| **Full LLM Fallback** | Complete definition when hybrid fails (max_tokens=2000) | All fields except `phonetics` and `examples` |

#### Supported Languages

The Free Dictionary API supports the following languages:

| Language | Code | Example |
|----------|------|---------|
| German | `de` | `/api/v1/entries/de/Hund` |
| English | `en` | `/api/v1/entries/en/dog` |
| French | `fr` | `/api/v1/entries/fr/chien` |
| Spanish | `es` | `/api/v1/entries/es/perro` |
| Italian | `it` | `/api/v1/entries/it/cane` |
| Portuguese | `pt` | `/api/v1/entries/pt/cao` |
| Dutch | `nl` | `/api/v1/entries/nl/hond` |
| Polish | `pl` | `/api/v1/entries/pl/pies` |
| Russian | `ru` | `/api/v1/entries/ru/sobaka` |

**Note**: For unsupported languages, the system automatically falls back to full LLM lookup.

#### Fallback Mechanism

The fallback to full LLM (gpt-4.1-mini) occurs in these scenarios:

1. **Stanza failure (German)**: Stanza pipeline error or token not found -- falls through to LLM reduced prompt
2. **LLM reduced prompt failure**: Failed to parse lemma from reduced prompt
3. **Language not supported**: Language not in `LANGUAGE_CODE_MAP`
4. **Word not found**: Free Dictionary API returns 404
5. **API timeout**: Request exceeds 5-second timeout
6. **API error**: HTTP error or network failure
7. **Missing entries**: API response lacks entries (sense selection requires entries)

**Fallback Chain**:
```
German path:
  Stanza NLP (lemma + related_words)
      |
      v (failure)
  LLM Reduced Prompt (fallback within Step 1)
      |
      v (failure)
  Full LLM Fallback (all fields)

Non-German path:
  LLM Reduced Prompt (lemma + related_words + level)
      |
      v (failure)
  Full LLM Fallback (all fields)

Both paths after Step 1:
  Free Dictionary API (Step 2)
      |
      v (404 / timeout / no entries)
  Full LLM Fallback (all fields)
```

**Note**: Fallback responses do NOT include `phonetics` or `examples` since these are only available from the Free Dictionary API.

#### Phonetics and Examples Data Flow

**Phonetics** (IPA pronunciation) and **examples** (usage sentences) are sourced exclusively from the Free Dictionary API and follow specific rules:

**Data Sources**:
```
Free Dictionary API
    |
    +---> phonetics (IPA string, e.g., "/h änt/")
    |
    +---> examples (array of sentences)
    |
    v
SearchResponse --> VocabularyRequest --> MongoDB
```

**Phonetics Restrictions**:
- Only returned for **English** language lookups
- Reason: Free Dictionary API provides most accurate IPA for English
- Other languages: `phonetics` field is set to `null`

**Implementation** (`src/api/routes/dictionary.py:335-337`):
```python
# Only include phonetics for English
if request.language != "English":
    phonetics = None
```

**Examples Behavior**:
- Available for all supported languages
- Returns up to 3 example sentences from dictionary
- Falls back to empty array if no examples found

**Frontend Display** (`VocabularyCard`):
- Phonetics displayed next to lemma in monospace font
- Examples shown in collapsible section with article sentence first

#### Token Usage Tracking

Token usage is tracked for all dictionary searches:

- **Hybrid success (German)**: CEFR estimation tokens (~10 max) + sense selection tokens (~10 max) accumulated (gpt-4.1-mini). Stanza NLP uses no LLM tokens.
- **Hybrid success (non-German)**: Reduced prompt tokens (~200 max) + sense selection tokens (~10 max) accumulated (gpt-4.1-mini)
- **Fallback**: Full prompt tokens counted (~2000 max, gpt-4.1-mini)
- **Source metadata**: Indicates `"hybrid"` or `"llm"` for cost analysis

```json
{
  "operation": "dictionary_search",
  "metadata": {
    "word": "dog",
    "language": "English",
    "source": "hybrid",
    "phonetics": "/d…îÀê…°/"
  }
}
```

**Note**: `phonetics` is only included for English language lookups.

### Word Definition Endpoint

**Endpoint**: `POST /dictionary/search`

**Î™©Ï†Å**: Î¨∏Ïû• Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú Îã®Ïñ¥Ïùò lemma, Ï†ïÏùò Î∞è Î¨∏Î≤ïÏ†Å Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂ú

**ÏöîÏ≤≠:**
```json
{
  "word": "h√§ngt",
  "sentence": "Diese gro√üe Spanne h√§ngt von mehreren Faktoren ab.",
  "language": "German"
}
```

**ÏùëÎãµ:**
```json
{
  "lemma": "abh√§ngen",
  "definition": "ÏùòÏ°¥ÌïòÎã§, ~Ïóê Îã¨Î†§ÏûàÎã§",
  "related_words": ["h√§ngt", "ab"],
  "pos": "verb",
  "gender": null,
  "phonetics": null,
  "conjugations": {
    "present": "h√§ngt ab",
    "past": "hing ab",
    "participle": "abgehangen",
    "auxiliary": "haben"
  },
  "level": "B1",
  "examples": ["Das h√§ngt vom Wetter ab.", "Es h√§ngt davon ab, ob..."]
}
```

**Note**: `phonetics` is `null` for German (only available for English).

**ÌäπÏßï:**
- **3-Step Pipeline**: Step 1 (lemma extraction) -> Step 2 (Free Dictionary API) -> Step 3 (sense selection)
- **Stanza NLP (German)**: ÎèÖÏùºÏñ¥Îäî Stanza NLPÎ°ú Î°úÏª¨ lemma Ï∂îÏ∂ú (~51ms), CEFRÎßå LLM Ìò∏Ï∂ú
- **LLM Reduced Prompt (non-German)**: Í∏∞ÌÉÄ Ïñ∏Ïñ¥Îäî LLM reduced promptÎ°ú lemma + related_words + level Ï∂îÏ∂ú
- **Î∂ÑÎ¶¨ÎèôÏÇ¨ Ï≤òÎ¶¨**: ÎèÖÏùºÏñ¥ Îì±ÏóêÏÑú ÎèôÏÇ¨Í∞Ä Î∂ÑÎ¶¨Îêú Í≤ΩÏö∞ Ï†ÑÏ≤¥ lemma Î∞òÌôò (Ïòà: `h√§ngt ... ab` ‚Üí `abh√§ngen`)
- **Î≥µÌï©Ïñ¥ Ï≤òÎ¶¨**: Îã®Ïñ¥Í∞Ä Î≥µÌï©Ïñ¥Ïùò ÏùºÎ∂ÄÏù∏ Í≤ΩÏö∞ Ï†ÑÏ≤¥ ÌòïÌÉú Î∞òÌôò
- **related_words**: Î¨∏Ïû•ÏóêÏÑú Í∞ôÏùÄ lemmaÏóê ÏÜçÌïòÎäî Î™®Îì† Îã®Ïñ¥Îì§ÏùÑ Î∞∞Ïó¥Î°ú Î∞òÌôò (Ïòà: Î∂ÑÎ¶¨ ÎèôÏÇ¨Ïùò Í≤ΩÏö∞ Î™®Îì† Î∂ÄÎ∂Ñ Ìè¨Ìï®)
- **Î¨∏Î≤ïÏ†Å Î©îÌÉÄÎç∞Ïù¥ÌÑ∞**: ÌíàÏÇ¨(pos), ÏÑ±(gender), ÎèôÏÇ¨ ÌôúÏö©Ìòï(conjugations), CEFR Î†àÎ≤®(level), IPA Î∞úÏùå(phonetics), ÏòàÎ¨∏(examples) Ï∂îÏ∂ú
- **Î™®Îìà Íµ¨ÏÑ±**: `utils/lemma_extraction.py` (Step 1), `utils/sense_selection.py` (Step 3), `services/dictionary_service.py` (Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞), `utils/prompts.py` (Ìè¥Î∞±Îßå Ìè¨Ìï®)
- **Î≥¥Ïïà**: Regex injection Î∞©ÏßÄÎ•º ÏúÑÌïú `re.escape()` Ï†ÅÏö©

**ÌùêÎ¶Ñ**: ÏúÑ [Hybrid Dictionary Lookup Architecture](#hybrid-dictionary-lookup-architecture) ÏÑπÏÖòÏùò Data Flow Ï∞∏Ï°∞.
Frontend -> Next.js API(`/api/dictionary/search`) -> FastAPI(`/dictionary/search`) -> DictionaryService(orchestrator) -> Step 1(lemma_extraction) -> Step 2(dictionary API) -> Step 3(sense_selection) ÏàúÏúºÎ°ú Ï≤òÎ¶¨Îê®.

---

## üé® Frontend Architecture

### Code Organization

The frontend follows a modular architecture with clear separation of concerns:

```
src/web/
‚îú‚îÄ‚îÄ app/              # Next.js App Router (pages)
‚îú‚îÄ‚îÄ components/       # Reusable React components
‚îú‚îÄ‚îÄ hooks/            # Custom React hooks
‚îú‚îÄ‚îÄ lib/              # Utility functions
‚îî‚îÄ‚îÄ types/            # TypeScript type definitions
```

### Utility Modules

#### API Client (`lib/api.ts`)
Centralized API client utilities for consistent request handling:
- `fetchWithAuth()`: Automatic JWT token injection
- `parseErrorResponse()`: Consistent error message extraction

**Benefits**:
- DRY principle: Authentication logic in one place
- Consistent error handling across all API calls
- Easy to add global request interceptors

#### Date Formatters (`lib/formatters.ts`)
Reusable date formatting functions using Intl.DateTimeFormat:
- `formatDate()`: Customizable date formatting
- `formatDateShort()`: Short date format
- `formatDateTime()`: Date with time

**Benefits**:
- Consistent date display across UI
- Locale-aware formatting
- Single source of truth for date formats

#### Style Helpers (`lib/styleHelpers.ts`)
CEFR level badge styling utilities:
- `getLevelColor()`: Tailwind classes for level badges
- `getLevelLabel()`: Human-readable level labels

**Benefits**:
- Consistent color scheme across UI
- Dynamic class generation for Tailwind
- Easy to update color scheme globally

**Important**: CEFR level colors are safelisted in `tailwind.config.ts` to prevent Tailwind's tree-shaking from removing dynamically-generated classes.

### Custom Hooks

#### useAsyncFetch
Generic hook for async data fetching with automatic state management:
- Loading state
- Error handling
- Automatic 401 redirect
- Type-safe data state

**Use Cases**:
- Fetching article lists
- Loading article details
- Any API data fetching

#### usePagination
Pagination calculations and state management:
- Current page calculation
- Total pages calculation
- Next/previous page navigation
- Skip value computation

**Use Cases**:
- Article list pagination
- Vocabulary list pagination

#### useStatusPolling
Job status polling with automatic interval management:
- Configurable polling interval
- Automatic cleanup on completion/error
- Progress state management
- Callbacks for status changes

**Use Cases**:
- Article generation progress tracking
- Any long-running job monitoring

#### useVocabularyDelete
Vocabulary deletion with error handling:
- DELETE request to API
- Detailed error messages
- Throws errors for caller to handle

**Use Cases**:
- Deleting vocabulary entries from vocabulary list

### Reusable Components

#### ErrorAlert (`src/web/components/ErrorAlert.tsx`)
Consistent error message display:
- Red background with border (`bg-red-50 border-red-200`)
- Optional retry button with hover effect
- Automatic hiding when error is null
- Accessible error messaging

**Props**:
- `error` (string | null): Error message to display
- `onRetry` (optional): Callback function for retry button
- `className` (optional): Additional CSS classes

#### EmptyState (`src/web/components/EmptyState.tsx`)
Consistent empty state display:
- Centered layout with white card background
- Optional icon (emoji or Unicode character)
- Optional action button with blue styling
- Title and description text with gray tones

**Props**:
- `title` (string): Main heading text
- `description` (string): Descriptive subtitle
- `icon` (optional): Emoji or icon character
- `action` (optional): Object with `label` and `onClick` for action button
- `className` (optional): Additional CSS classes

**Benefits**:
- Consistent UX across all pages
- Reduces code duplication
- Easy to update design globally
- Improves maintainability with single source of truth

#### VocabularyCard (`src/web/components/VocabularyCard.tsx`)
Unified vocabulary display component supporting both list and card layouts:
- Displays lemma with gender prefix and IPA phonetics
- Shows part of speech (POS) and CEFR level badges
- Renders verb conjugations or noun declensions
- Collapsible examples section with sentence context
- Optional article link and creation date
- Delete button with callback support

**Props**:
- `id` (string): Unique vocabulary entry ID
- `lemma` (string): Dictionary form of the word
- `word` (string): Original word as clicked
- `definition` (string): Word definition
- `sentence` (string): Context sentence from article
- `gender` (optional): Grammatical gender (der/die/das)
- `phonetics` (optional): IPA pronunciation (e.g., /h änt/)
- `pos` (optional): Part of speech
- `level` (optional): CEFR level (A1-C2)
- `conjugations` (optional): Verb/noun forms (see Conjugations type)
- `examples` (optional): Additional example sentences from dictionary
- `count` (optional): Occurrence count across articles
- `articleId` (optional): Article ID for linking
- `createdAt` (optional): Creation timestamp
- `variant` ('list' | 'card'): Display style (default: 'list')
- `showArticleLink` (boolean): Show "View in Article" link (default: false)
- `onDelete` (optional): Callback for delete button

**Conjugations Type** (`src/web/types/article.ts`):
```typescript
export interface Conjugations {
  present?: string    // Present tense (3rd person singular)
  past?: string       // Past/preterite tense
  participle?: string // Past participle
  auxiliary?: string  // Auxiliary verb (haben/sein)
  genitive?: string   // Genitive form (nouns)
  plural?: string     // Plural form (nouns)
}
```

**Usage**:
```tsx
import VocabularyCard from '@/components/VocabularyCard'

<VocabularyCard
  id="vocab-123"
  lemma="Hund"
  word="Hunde"
  definition="dog"
  sentence="Die Hunde spielen im Park."
  gender="der"
  phonetics="/h änt/"
  pos="noun"
  level="A1"
  conjugations={{ genitive: "Hundes", plural: "Hunde" }}
  examples={["Der Hund bellt.", "Ich habe einen Hund."]}
  variant="card"
  showArticleLink={true}
  articleId="article-456"
  onDelete={(id) => handleDelete(id)}
/>
```

### Refactoring Impact

**Before Refactoring**:
- Duplicate fetch logic in every page
- Inconsistent error handling
- Duplicate date formatting code
- Duplicate pagination calculations
- Duplicate empty state styling

**After Refactoring**:
- Single source of truth for common operations
- Consistent error handling with `useAsyncFetch`
- Reusable date formatters
- Reusable pagination hook
- Reusable UI components (`ErrorAlert`, `EmptyState`)

**Code Reduction**:
- Article detail page: 166 lines reduced
- Articles list page: 27 lines reduced
- Vocabulary page: 43 lines reduced
- Total: 236 lines of code removed through refactoring

### Security Improvements

#### XSS Prevention in MarkdownViewer (`src/web/components/MarkdownViewer.tsx`)

**Issue**: Previous implementation used `innerHTML` to inject vocabulary buttons, creating XSS vulnerability.

**Security Measures Implemented**:

1. **HTML Escaping** (lines 92-96):
   - `escapeHtml()` utility converts text to DOM text node then reads innerHTML
   - Prevents script injection in user-provided content (word, lemma, definition, sentence)
   - Applied to all vocabulary data before rendering

2. **DOM API Methods** (lines 663-713):
   - Replaced `innerHTML` with DOM manipulation (`createElement`, `textContent`)
   - Creates definition spans using `document.createElement()` instead of string templates
   - Uses `textContent` for user data instead of `innerHTML`
   - Parses button HTML in temporary container, then extracts element reference

3. **Event Delegation** (lines 514-534):
   - Single event listener on container instead of per-word listeners
   - Prevents stale closure issues with dynamic content
   - Ref-based callback storage (`handleWordClickRef`) avoids outdated state

4. **Data Attribute Escaping** (lines 112-131):
   - All data attributes escaped before setting: `data-word="${wordEscaped}"`
   - JSON strings escaped with `.replace(/"/g, '&quot;')` for attribute safety
   - Prevents attribute injection attacks

**Before (Vulnerable)**:
```typescript
// Direct innerHTML injection - XSS risk
defSpan.innerHTML = `<strong>${lemma}</strong>: ${meaning} <button>...</button>`
```

**After (Secure)**:
```typescript
// DOM API - XSS safe
const strong = document.createElement('strong')
strong.textContent = displayLemma
defSpan.appendChild(strong)
defSpan.appendChild(document.createTextNode(': ' + meaning))
```

**Impact**: Prevents malicious script execution from vocabulary data, protects against DOM-based XSS attacks.

#### React Component Remounting Pattern

**Purpose**: Prevent React hydration mismatches when article content changes.

**Implementation** (`src/web/app/articles/[id]/page.tsx:266`):
```typescript
<MarkdownViewer
  key={`${articleId}-${content.length}`}
  content={content}
  language={article?.language}
  articleId={articleId}
  vocabularies={vocabularies}
  onAddVocabulary={handleAddVocabulary}
/>
```

**Key Prop Strategy**:
- Pattern: `${articleId}-${content.length}`
- Forces complete component remount when content changes
- Triggers reset of `data-processed` attribute (line 456)
- Clears all previous DOM state and event listeners

**Processing State Check** (`src/web/components/MarkdownViewer.tsx:456-458`):
```typescript
// Skip if already processed (component remounts on content change via key prop)
if (containerRef.current.getAttribute('data-processed') === 'true') {
  return
}
```

**Why This Pattern**:
- Without key: React reuses DOM nodes, causing hydration mismatches
- Manual cleanup: Error-prone and complex to maintain
- useEffect on content: Risk of double-processing
- Remounting: Clean state guaranteed, simple lifecycle

**Benefits**:
- Eliminates React DOM mismatch errors
- Prevents stale event listeners
- Simplifies component update logic
- Ensures consistent behavior across content changes

#### Sentence Extraction via DOM Offset Matching (`src/web/components/MarkdownViewer.tsx`)

**Purpose**: Accurately extract the sentence containing a clicked word for dictionary lookup context, even when the same word appears in multiple sentences.

**Problem with Previous Approach**:
The previous `extractSentence` implementation normalized whitespace up front, then used `String.includes()` to find the first sentence containing the clicked word. This was inaccurate when the same word appeared in multiple sentences -- the function always returned the first occurrence regardless of which instance the user actually clicked.

**Current Approach -- DOM Offset-Based Matching**:

1. **`getTextOffset(parent, target)`**: A helper function that calculates the character offset of a target node (the clicked word span) within a parent element's `textContent`. Uses `TreeWalker` with `NodeFilter.SHOW_TEXT` to iterate through all text nodes in DOM order, accumulating character lengths until the target node is found.

2. **`extractSentence(wordSpan)`**: Uses the computed offset to match against sentence-splitter's range metadata:
   - Retrieves raw `textContent` from the parent element (no whitespace normalization, to preserve accurate character offsets)
   - Calls `getTextOffset()` to determine where the clicked span sits in the text
   - Passes the raw text to `sentence-splitter`'s `split()` function, which returns sentence nodes with `range: [start, end]` metadata
   - Finds the sentence whose range contains the span offset (`spanOffset >= range[0] && spanOffset < range[1]`)
   - Applies whitespace normalization (`replace(/\s+/g, ' ').trim()`) only at return-time on the matched sentence

**Fallback Chain**:
```
DOM offset + sentence-splitter range matching
    |
    v (if getTextOffset returns -1)
includes-based first-match (original behavior)
    |
    v (if sentence-splitter throws)
Simple regex split on punctuation
```

**Implementation** (`src/web/components/MarkdownViewer.tsx:256-319`):
```typescript
/** Calculate the character offset of a target node within a parent's textContent. */
const getTextOffset = (parent: Node, target: Node): number => {
  let offset = 0
  const walker = document.createTreeWalker(parent, NodeFilter.SHOW_TEXT)
  let node: Text | null
  while ((node = walker.nextNode() as Text | null)) {
    if (target.contains(node)) {
      return offset
    }
    offset += (node.textContent || '').length
  }
  return -1
}

const extractSentence = (wordSpan: HTMLElement): string => {
  // ...parent resolution...
  const text = parent.textContent || ''  // Raw text, no normalization
  const spanOffset = getTextOffset(parent, wordSpan)

  const result = split(text)
  const sentences = result.filter(node => node.type === 'Sentence')

  if (spanOffset >= 0) {
    const found = sentences.find(node =>
      spanOffset >= node.range[0] && spanOffset < node.range[1]
    )
    if (found) {
      return found.raw.replace(/\s+/g, ' ').trim()  // Normalize only at return
    }
  }
  // ...fallback logic...
}
```

**Key Design Decisions**:
- **No early whitespace normalization**: Raw `textContent` is used for offset calculation and sentence-splitter input so that character positions remain aligned between `getTextOffset` and the splitter's range metadata
- **Whitespace normalization at return-time only**: The matched sentence's `raw` value is normalized before being returned, ensuring clean output for the dictionary API
- **Graceful degradation**: If `getTextOffset` returns `-1` (target node not found in parent), the function falls back to the original `includes`-based matching with a console warning

**Benefits**:
- Correctly identifies the sentence for duplicate words (e.g., "die" appearing in multiple German sentences)
- Preserves offset accuracy by deferring whitespace normalization
- Maintains backward compatibility through the fallback chain

#### Dictionary Lookup Caching with Sentence Context (`src/web/components/MarkdownViewer.tsx`)

**Purpose**: Cache dictionary lookup results so that the same word in the same sentence context returns the cached lemma and definition without making duplicate API calls. Sentence context is included in cache keys to correctly handle context-dependent words (e.g., German "sich" mapping to different lemmas in different sentences).

**Cache Refs**:

| Ref | Key Format | Value | Purpose |
|-----|-----------|-------|---------|
| `lemmaCacheRef` | `language:word:sentence` | JSON string of `{lemma, definition, related_words, ...}` | Full LLM response cache keyed by language, word, and sentence context |
| `wordToLemmaRef` | `word:sentence` | `lemma` string | Quick word-to-lemma mapping for finding same-lemma variants within a sentence |

**Key Format Examples**:
```
lemmaCacheRef:  "German:h√§ngt:diese gro√üe spanne h√§ngt von mehreren faktoren ab." -> '{"lemma":"abh√§ngen","definition":"..."}'
wordToLemmaRef: "h√§ngt:diese gro√üe spanne h√§ngt von mehreren faktoren ab." -> "abh√§ngen"
```

**Why Sentence Context in Cache Keys**:
The same word can have different meanings (and different lemmas) depending on the sentence. For example, German "sich" could map to "sich befinden" (to be located) in one sentence and "sich freuen" (to be happy) in another. Using word-only keys (`language:word`) would incorrectly return the cached result from the first lookup for all subsequent lookups of the same word, regardless of context. The sentence-context key format (`language:word:sentence`) ensures each unique word+sentence combination gets its own cache entry.

**Helper Functions**:

- **`getWordMeaning(word, sentence)`**: Retrieves the cached definition and display lemma for a word within a specific sentence context. Checks `wordToLemmaRef` first (fast path), then `lemmaCacheRef`, then falls back to `wordDefinitions` state by word key.

- **`getRelatedWords(word, sentence)`**: Retrieves cached related words (e.g., separable verb particles) for a word within a specific sentence context from `lemmaCacheRef`.

**`handleWordClick` Cache Lookup Flow**:
```
handleWordClick(spanId, word)
    |
    v
extractSentence(spanElement)         <-- Called BEFORE any cache checks
    |
    v
Build sentenceCacheKey = "language:word:sentence"
    |
    v
Check lemmaCacheRef[sentenceCacheKey]
    |-- hit + definition exists --> return (no API call)
    |
    v
Check wordToLemmaRef["word:sentence"]
    |-- hit + definition exists --> return (no API call)
    |
    v
Check wordDefinitions[word]          <-- Fallback for old cache format
    |-- hit --> return (no API call)
    |
    v
Check loadingWordsRef (prevent duplicate fetches)
    |
    v
getWordDefinitionFromLLM(word, sentence)
    |
    v
Store results:
  - wordToLemmaRef["word:sentence"] = lemma
  - wordToLemmaRef["relatedWord:sentence"] = lemma (for each related word)
  - lemmaCacheRef["language:word:sentence"] = JSON({lemma, definition, ...})
```

### Bug Fixes

#### 1. Conjugations Type Conversion
**Issue**: Frontend expected conjugations as object, but backend returned null for non-verbs, causing type mismatches.

**Fix**: Ensure conjugations field is properly typed and handled as nullable in TypeScript types.

#### 2. Tailwind Safelist for Dynamic Classes
**Issue**: CEFR level color classes (generated dynamically by `getLevelColor()`) were being purged by Tailwind's tree-shaking.

**Fix**: Added safelist to `tailwind.config.ts` to preserve dynamic color classes:
```typescript
safelist: [
  'bg-gray-100', 'text-gray-600',   // Unknown level
  'bg-green-100', 'text-green-700', // A1-A2
  'bg-yellow-100', 'text-yellow-700', // B1-B2
  'bg-red-100', 'text-red-700',     // C1-C2
]
```

**Impact**: CEFR level badges now display correctly with proper colors

### Token Usage Display

The article detail page (`src/web/app/articles/[id]/page.tsx`) displays token usage with smart aggregation and auto-refresh functionality.

#### TokenUsageSection Component

**Location**: `src/web/app/articles/[id]/page.tsx:46-181`

**Purpose**: Displays token usage breakdown for an article with aggregation logic.

**Features**:
- Displays total tokens, prompt tokens, completion tokens, and estimated cost
- Shows detailed breakdown per operation with agent names
- Aggregates dictionary searches into cumulative totals
- Uses expandable details for viewing individual operations

#### Aggregation Logic

```typescript
// dictionary_search: aggregate by operation+model
// article_generation: keep separate using record id
const key = record.operation === 'dictionary_search'
  ? `op:dictionary_search:${record.model}`
  : `id:${record.id}`
```

**Rationale**:
- **Article generation records**: Kept separate to show individual agent contributions (Article Search, Article Selection, Article Rewrite)
- **Dictionary search records**: Aggregated into cumulative totals per model since individual lookups are less meaningful

#### Agent Names

Article generation records include `agent_name` in metadata:
- `Article Search`: Research agent that finds relevant articles
- `Article Selection`: Agent that selects the best article for adaptation
- `Article Rewrite`: Agent that rewrites content for the target level

The `formatOperationName()` helper prioritizes `agent_name` over the raw operation type for display.

#### Auto-Refresh Mechanism

**Flow**:
```
User clicks word in MarkdownViewer
    |
    v
extractSentence(spanElement)       <-- Sentence extracted early
    |
    v
Cache check (lemmaCacheRef, wordToLemmaRef) using sentence-context key
    |-- cache hit --> show cached definition (no API call, no token usage update)
    |
    v (cache miss)
Dictionary API call (POST /dictionary/search)
    |
    v
Token usage record saved to MongoDB
    |
    v
onTokenUsageUpdate callback triggered
    |
    v
fetchTokenUsage(true) called with isRefresh=true
    |
    v
Token usage section updates without loading spinner
```

**Implementation** (`src/web/app/articles/[id]/page.tsx`):

1. **MarkdownViewer receives callback**: `onTokenUsageUpdate?: () => void`
2. **After dictionary search**: Callback is invoked in `handleWordClick` (line 328)
3. **Article page passes handler**: `onTokenUsageUpdate={() => fetchTokenUsage(true)}`
4. **Silent refresh**: `isRefresh=true` skips loading state to prevent UI flicker

```typescript
const fetchTokenUsage = useCallback(async (isRefresh = false) => {
  if (article?.status !== 'completed') return

  if (!isRefresh) {
    setTokenUsageLoading(true)  // Only show loading on initial fetch
  }
  // ... fetch logic
}, [article?.status, articleId])
```

**Benefits**:
- Users see updated token costs immediately after word lookups
- No full page reload required
- Silent refresh prevents disruptive UI changes

### Vocabulary Management Endpoints

**Endpoints:**
- `POST /dictionary/vocabularies` - Add vocabulary
- `GET /dictionary/vocabularies` - Get vocabulary list (optionally filtered by article_id)
- `DELETE /dictionary/vocabularies/{id}` - Delete vocabulary
- `GET /dictionary/stats` - Get vocabulary statistics (word counts by language)

**Vocabulary Ï†ÄÏû•:**
- MongoDB `vocabularies` Ïª¨Î†âÏÖòÏóê Ï†ÄÏû•
- `related_words` Î∞∞Ïó¥ Ìè¨Ìï® (Î∂ÑÎ¶¨ ÎèôÏÇ¨ Îì± Î≥µÏû°Ìïú Ïñ∏Ïñ¥ Íµ¨Ï°∞ ÏßÄÏõê)
- ArticleÎ≥ÑÎ°ú Í∑∏Î£πÌôîÌïòÏó¨ Í¥ÄÎ¶¨

**Vocabulary ÌëúÏãú:**
- Ï†ÄÏû•Îêú Îã®Ïñ¥(`span_id`Î°ú ÏãùÎ≥Ñ)Îäî Ìï≠ÏÉÅ Ï¥àÎ°ùÏÉâÏúºÎ°ú ÌïòÏù¥ÎùºÏù¥Ìä∏ (`related_words` Ï°¥Ïû¨ Ïó¨Î∂ÄÏôÄ Î¨¥Í¥Ä). ÌÅ¥Î¶≠Ìïú Îã®Ïñ¥Ïùò ÌïòÏù¥ÎùºÏù¥Ìä∏Îäî `related_words` Í≤ÄÏÉâÎ≥¥Îã§ Î®ºÏ†Ä Ïã§ÌñâÎêòÏñ¥, ÌïúÍµ≠Ïñ¥ Îì± `related_words`Í∞Ä ÏóÜÎäî Ïñ∏Ïñ¥ÏóêÏÑúÎèÑ Ï†ïÏÉÅ ÎèôÏûëÌï®.
- `related_words`Í∞Ä Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞, Ìï¥Îãπ Îã®Ïñ¥Îì§ÎèÑ Ìï®Íªò Ï¥àÎ°ùÏÉâ ÌëúÏãú (Î∂ÑÎ¶¨ ÎèôÏÇ¨ Îì± Î≥µÌï© ÌëúÌòÑ ÏßÄÏõê)
- Ïòà: "h√§ngt" Ï†ÄÏû• Ïãú "ab"ÎèÑ ÏûêÎèôÏúºÎ°ú Ï¥àÎ°ùÏÉâ ÌëúÏãú (`related_words: ["h√§ngt", "ab"]`)

---

## üß™ Testing Infrastructure

### Web Testing (Vitest)

**Configuration**: `src/web/vitest.config.ts`

**Test Environment**:
- **Framework**: Vitest 4.0.18 with jsdom for DOM simulation
- **UI**: @vitest/ui for interactive test running
- **Testing Library**: @testing-library/react for component testing
- **Test Matchers**: @testing-library/jest-dom for DOM assertions

**Coverage Settings**:
- Provider: v8 (Node.js native coverage)
- Reporters: text, json, html
- Thresholds: 80% for lines, functions, branches, statements
- Excludes: node_modules, test files, test directories

**Test Location**:
- Pattern: `**/__tests__/**/*.test.ts` and `**/__tests__/**/*.test.tsx`
- Current test files:
  - `hooks/__tests__/usePagination.test.ts`
  - `hooks/__tests__/useStatusPolling.test.ts`
  - `lib/__tests__/api.test.ts`
  - `lib/__tests__/formatters.test.ts`
  - `lib/__tests__/styleHelpers.test.ts`

**Run Commands** (`src/web/package.json`):
```bash
npm test         # Run all tests once
npm run test:watch   # Watch mode for development
npm run test:ui      # Interactive UI for test exploration
```

**Alias Resolution**:
- `@` alias resolves to `src/web/` directory
- Matches Next.js path configuration for consistency

**Benefits**:
- Fast test execution with Vitest (ESM-native)
- Interactive UI for debugging
- Coverage reporting for quality assurance
- Type-safe testing with TypeScript
