# ì•„í‚¤í…ì²˜ ë¬¸ì„œ: 3-Service ë¶„ë¦¬

## ğŸ“Š í˜„ì¬ êµ¬ì¡° (Before)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Railway Container (ë‹¨ì¼)         â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Next.js     â”‚                       â”‚
â”‚  â”‚  (Port 3000) â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â”‚                               â”‚
â”‚         â”‚ spawn('python3', main.py)     â”‚
â”‚         â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Python       â”‚                       â”‚
â”‚  â”‚ CrewAI ì‹¤í–‰   â”‚                       â”‚
â”‚  â”‚              â”‚                       â”‚
â”‚  â”‚ - status.json íŒŒì¼ ì“°ê¸°                â”‚
â”‚  â”‚ - MongoDBì— ì—…ë¡œë“œ                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                         â”‚
â”‚  ë¬¸ì œì :                                  â”‚
â”‚  - Next.jsì™€ Pythonì´ ê°™ì€ ì»¨í…Œì´ë„ˆ          â”‚
â”‚  - ë¦¬ì†ŒìŠ¤ ê²½ìŸ (CPU/ë©”ëª¨ë¦¬)                  â”‚
â”‚  - í™•ì¥ ë¶ˆê°€ëŠ¥ (ë‘˜ ë‹¤ í•¨ê»˜ ìŠ¤ì¼€ì¼)             â”‚
â”‚  - ì¥ì•  ê²©ë¦¬ ë¶ˆê°€ (í•˜ë‚˜ ì£½ìœ¼ë©´ ì „ì²´)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í˜„ì¬ íë¦„:
1. ì‚¬ìš©ìê°€ "Generate" í´ë¦­
2. Next.js `/api/generate` â†’ `spawn('python3', main.py)` ì‹¤í–‰
3. Pythonì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ CrewAI ì‹¤í–‰
4. Pythonì´ `status.json` íŒŒì¼ì— ì§„í–‰ìƒí™© ê¸°ë¡
5. Next.jsê°€ `/api/status`ë¡œ í´ë§ (2ì´ˆë§ˆë‹¤)
6. ì™„ë£Œë˜ë©´ `/api/article`ë¡œ MongoDBì—ì„œ íŒŒì¼ ê°€ì ¸ì˜´

---

## ğŸ¯ ëª©í‘œ êµ¬ì¡° (After)

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    Web[Web<br/>Next.js] -->|HTTP| API[API<br/>FastAPI]
    API -->|RPUSH| Redis[(Redis<br/>Queue + Status)]
    Redis -->|BLPOP| Worker[Worker<br/>Python]
    Worker -->|Execute| CrewAI[CrewAI]
    Worker -->|Save| MongoDB[(MongoDB<br/>Article + Vocabulary)]
    
    API -.->|SET/GET| Redis
    Worker -.->|SET| Redis
    
    API -.->|utils/llm.py<br/>utils/prompts.py| OpenAI
    
    Web -->|HTTP<br/>Proxy| API
    API -->|Save/Query| MongoDB
    
    style Web fill:#2196F3
    style API fill:#2196F3
    style Worker fill:#2196F3
    style CrewAI fill:#10a37f
    style Redis fill:#dc382d
    style MongoDB fill:#13aa52
    style OpenAI fill:#10a37f
    
    linkStyle 0 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 1 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 2 stroke:#4a90e2,stroke-width:2px,color:#4a90e2
    linkStyle 5 stroke:#ff9500,stroke-width:2px,color:#ff9500
    linkStyle 6 stroke:#13aa52,stroke-width:2px,color:#13aa52
    linkStyle 7 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
    linkStyle 8 stroke:#9c27b0,stroke-width:2px,color:#9c27b0
```

### Article Generation íë¦„

```mermaid
sequenceDiagram
    participant Web
    participant API
    participant Redis
    participant Worker
    participant CrewAI
    participant MongoDB
    
    Web->>API: POST /articles/generate
    API->>MongoDB: Check duplicates
    API->>MongoDB: Save article metadata
    API->>Redis: RPUSH job
    API-->>Web: Return job_id + article_id
    
    Worker->>Redis: BLPOP
    Redis-->>Worker: job_data
    
    Worker->>CrewAI: Execute crew.kickoff()
    CrewAI-->>Worker: Return article
    
    Worker->>MongoDB: Save article content
    Worker->>Redis: Update status (completed)
```

**íŠ¹ì§•:**
- **ì‹¤ì‹œê°„ ì‘ë‹µ**: ì‚¬ìš©ìê°€ ë‹¨ì–´ë¥¼ í´ë¦­í•˜ë©´ ì¦‰ì‹œ ì •ì˜ ë°˜í™˜ (ë¹„ë™ê¸° í ì‚¬ìš© ì•ˆ í•¨)
- **í”„ë¡ì‹œ íŒ¨í„´**: Next.js API routeê°€ FastAPIë¡œ ìš”ì²­ì„ í”„ë¡ì‹œ
- **ê³µí†µ ìœ í‹¸ ì‚¬ìš©**: `utils/llm.py`ì™€ `utils/prompts.py`ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
- **ì—ëŸ¬ ì²˜ë¦¬**: `get_llm_error_response()`ë¡œ ì¼ê´€ëœ ì—ëŸ¬ ì‘ë‹µ

### ì„œë¹„ìŠ¤ ê°„ í†µì‹ 

| From | To | Method | Purpose |
|------|-----|--------|---------|
| **Web** | **API** | HTTP | Article ìƒì„±, Job enqueue, Token usage ì¡°íšŒ |
| **Web** | **Next.js API** | HTTP | Dictionary API ìš”ì²­ (í”„ë¡ì‹œ), Vocabulary CRUD ìš”ì²­ (í”„ë¡ì‹œ), Dictionary Stats ìš”ì²­ (í”„ë¡ì‹œ) |
| **Next.js API** | **API** | HTTP | Dictionary API í”„ë¡ì‹œ ìš”ì²­, Vocabulary CRUD í”„ë¡ì‹œ ìš”ì²­, Dictionary Stats í”„ë¡ì‹œ ìš”ì²­ |
| **API** | **MongoDB** | (via utils.mongodb) | ì¤‘ë³µ ì²´í¬, Article metadata ì €ì¥/ì¡°íšŒ, Vocabulary ì €ì¥/ì¡°íšŒ, Token usage ì €ì¥/ì¡°íšŒ |
| **API** | **Redis** | `RPUSH` | Jobì„ íì— ì¶”ê°€ |
| **API** | **Redis** | `SET/GET` | Job ìƒíƒœ ì €ì¥/ì¡°íšŒ (ê³µí†µ ëª¨ë“ˆ `api.job_queue` ì‚¬ìš©) |
| **API** | **LLM** | HTTP (via utils.llm) | Dictionary APIìš© LLM í˜¸ì¶œ (lemma, definition, related_words) + Token tracking |
| **API** | **API** | Internal | Token usage endpoints (`/usage/me`, `/usage/articles/{id}`) |
| **Worker** | **Redis** | `BLPOP` | Jobì„ íì—ì„œ êº¼ëƒ„ (blocking) |
| **Worker** | **Redis** | `SET` | Job ìƒíƒœ ì—…ë°ì´íŠ¸ (ê³µí†µ ëª¨ë“ˆ `api.job_queue` ì‚¬ìš©) |
| **Worker** | **CrewAI** | Function Call | Article ìƒì„± |
| **Worker** | **MongoDB** | (via utils.mongodb) | Article content ì €ì¥, Token usage ì €ì¥ |

**ì°¸ê³ **: APIì™€ Worker ëª¨ë‘ `api.job_queue` ëª¨ë“ˆì„ í†µí•´ Redisì— ì ‘ê·¼í•©ë‹ˆë‹¤. MongoDB ì ‘ê·¼ì€ `utils.mongodb` ëª¨ë“ˆì„ í†µí•´ í•©ë‹ˆë‹¤.

### Redis ë°ì´í„° êµ¬ì¡°

#### 1. Job Queue (List) - `opad:jobs`

**ìš©ë„**: Workerê°€ ì²˜ë¦¬í•  jobë“¤ì„ FIFO ìˆœì„œë¡œ ì €ì¥

```
Queue: opad:jobs (List)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [oldest] â† ... â† [newest]       â”‚
â”‚    â†‘                    â†‘       â”‚
â”‚  BLPOP              RPUSH       â”‚
â”‚ (Worker)             (API)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë°ì´í„° í˜•ì‹**:
```json
{
  "job_id": "uuid",
  "article_id": "uuid",
  "inputs": {
    "language": "Korean",
    "level": "B1",
    "length": "300",
    "topic": "Climate Change"
  },
  "created_at": "2025-01-08T12:34:56.789Z"
}
```

#### 2. Job Status (String) - `opad:job:{job_id}`

**ìš©ë„**: ê° jobì˜ í˜„ì¬ ìƒíƒœì™€ ì§„í–‰ë¥  ì¶”ì 

**TTL**: 24ì‹œê°„ (ìë™ ì‚­ì œ)

**ë°ì´í„° í˜•ì‹**:
```json
{
  "id": "job-uuid",
  "article_id": "article-uuid",
  "status": "running",
  "progress": 45,
  "message": "Adapting article...",
  "error": null,
  "created_at": "2025-01-08T12:34:56.789Z",
  "updated_at": "2025-01-08T12:35:12.345Z"
}
```

**ì ‘ê·¼ íŒ¨í„´**:
- **API**: ìƒíƒœ ì´ˆê¸°í™” (queued), ì¡°íšŒ (GET)
- **Worker**: ìƒíƒœ ì—…ë°ì´íŠ¸ (running, completed, failed)
- **Progress Listener**: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (0-100%) - CrewAI ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

---

## ğŸ”‘ í•µì‹¬ ê°œë…

### 1. **ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ (Async Job Processing)**
- **ë¬¸ì œ**: CrewAI ì‹¤í–‰ì€ 2-5ë¶„ ê±¸ë¦¼ â†’ HTTP ìš”ì²­ì´ íƒ€ì„ì•„ì›ƒ
- **í•´ê²°**: Job Queue íŒ¨í„´
  - ìš”ì²­ ì¦‰ì‹œ `jobId` ë°˜í™˜
  - ì‹¤ì œ ì‘ì—…ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
  - í´ë¼ì´ì–¸íŠ¸ëŠ” job ìƒíƒœë¥¼ í´ë§

### 2. **ì„œë¹„ìŠ¤ ë¶„ë¦¬ (Service Separation)**
- **ì›ì¹™**: "í•œ ì»¨í…Œì´ë„ˆ = í•œ ì—­í• "
- **ì¥ì **:
  - ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ (workerë§Œ ëŠ˜ë¦¬ë©´ ë¨)
  - ì¥ì•  ê²©ë¦¬ (worker ì£½ì–´ë„ web/apiëŠ” ì •ìƒ)
  - ë°°í¬ ë¶„ë¦¬ (apië§Œ ìˆ˜ì •í•´ë„ worker ì˜í–¥ ì—†ìŒ)

### 3. **Job Queue (Redis)**
- **ì—­í• **: ì‘ì—… ìš”ì²­ì„ íì— ë„£ê³ , workerê°€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
- **ìƒíƒœ**: `queued` â†’ `running` â†’ `completed` / `failed`
- **ì¥ì **: ë¶€í•˜ ë¶„ì‚°, ì¬ì‹œë„ ê°€ëŠ¥, ìš°ì„ ìˆœìœ„ ì„¤ì • ê°€ëŠ¥

### 4. **ë°ì´í„° ì €ì¥ì†Œ**

#### MongoDB: Article Storage
- **Article metadata ë° content ì €ì¥** (`articles` ì»¬ë ‰ì…˜)
  - ì¤‘ë³µ ì²´í¬ (24ì‹œê°„ ë‚´ ë™ì¼ ì…ë ¥ íŒŒë¼ë¯¸í„°)
  - Article ì¡°íšŒ ë° ë¦¬ìŠ¤íŠ¸

- **Vocabulary ì €ì¥** (`vocabularies` ì»¬ë ‰ì…˜)
  - ë‹¨ì–´, lemma, ì •ì˜, ë¬¸ì¥ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
  - `related_words` ë°°ì—´ í¬í•¨ (ë¶„ë¦¬ ë™ì‚¬ ë“± ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡° ì§€ì›)
  - Articleë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê´€ë¦¬

- **Token Usage ì¶”ì ** (`token_usage` ì»¬ë ‰ì…˜)
  - LLM API í˜¸ì¶œ ì‹œ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì 
  - ì‚¬ìš©ìë³„, ì‘ì—…ë³„ (dictionary_search, article_generation) ì§‘ê³„
  - ì¼ë³„ ì‚¬ìš©ëŸ‰ í†µê³„ ë° ë¹„ìš© ë¶„ì„
  
**Article Status** (MongoDB, ì˜êµ¬ ì €ì¥):
- `running`: Article ìƒì„± ì‹œ ì´ˆê¸° ìƒíƒœ (ì²˜ë¦¬ ì¤‘)
- `completed`: Article ìƒì„± ì™„ë£Œ
- `failed`: Article ìƒì„± ì‹¤íŒ¨
- `deleted`: Article ì‚­ì œ (soft delete)

**Status Flow:**
```
ìƒì„± ì‹œ: running
   â†“
ì™„ë£Œ: completed
ì‹¤íŒ¨: failed
```

#### Redis: Job Queue & Status
- **Queue**: `opad:jobs` (List) - Workerê°€ ì²˜ë¦¬í•  jobë“¤ì„ FIFO ìˆœì„œë¡œ ì €ì¥
- **Status**: `opad:job:{job_id}` (String, 24h TTL) - Jobì˜ ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì 

**Job Status** (Redis, 24ì‹œê°„ TTL):
- `queued`: Jobì´ íì— ì¶”ê°€ë¨ (Workerê°€ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
- `running`: Workerê°€ Jobì„ ì²˜ë¦¬ ì¤‘
- `completed`: Job ì²˜ë¦¬ ì™„ë£Œ
- `failed`: Job ì²˜ë¦¬ ì‹¤íŒ¨

**Status Flow:**
```
queued â†’ running â†’ completed / failed
```

**Article Status vs Job Status:**
- **Article Status (MongoDB)**: Articleì˜ ìµœì¢… ìƒíƒœ (ì˜êµ¬ ì €ì¥)
- **Job Status (Redis)**: Job ì²˜ë¦¬ì˜ ì‹¤ì‹œê°„ ìƒíƒœ (24ì‹œê°„ í›„ ìë™ ì‚­ì œ)
- Articleì€ `running` ìƒíƒœë¡œ ìƒì„±ë˜ê³ , Jobì´ ì™„ë£Œë˜ë©´ `completed` ë˜ëŠ” `failed`ë¡œ ì—…ë°ì´íŠ¸ë¨

---

## ğŸ’° Token Usage Tracking

### Overview
The system tracks LLM API token usage and costs for all API calls, enabling cost monitoring, user billing, and usage analytics.

### Architecture

#### 1. LLM Utility Module (`utils/llm.py`)
Provider-agnostic LLM API calls using LiteLLM with automatic token tracking.

**Functions**:
- `call_llm_with_tracking()`: Makes LLM API calls and returns content + token statistics
- `parse_json_from_content()`: Parses JSON from LLM responses (handles markdown code blocks)
- `get_llm_error_response()`: Converts LLM exceptions to HTTP status codes

**TokenUsageStats Dataclass**:
```python
@dataclass
class TokenUsageStats:
    model: str              # Model name (e.g., "gpt-4.1-mini")
    prompt_tokens: int      # Input tokens
    completion_tokens: int  # Output tokens
    total_tokens: int       # Total tokens used
    estimated_cost: float   # Cost in USD (calculated by LiteLLM)
    provider: str | None    # Provider name (openai, anthropic, google)
```

**Supported Providers** (via LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

**Example Usage**:
```python
from utils.llm import call_llm_with_tracking, TokenUsageStats

content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# stats.model = "gpt-4.1-mini"
# stats.prompt_tokens = 8
# stats.completion_tokens = 12
# stats.estimated_cost = 0.000015
```

#### 2. MongoDB Storage (`utils/mongodb.py`)

**save_token_usage()**: Save token usage record
```python
def save_token_usage(
    user_id: str,
    operation: str,  # "dictionary_search" | "article_generation"
    model: str,
    prompt_tokens: int,
    completion_tokens: int,
    estimated_cost: float,
    article_id: Optional[str] = None,
    metadata: Optional[dict] = None
) -> Optional[str]:
    """Save token usage record to MongoDB."""
```

**get_user_token_summary()**: Get user's token usage summary
```python
def get_user_token_summary(user_id: str, days: int = 30) -> dict:
    """
    Returns:
    {
        'total_tokens': int,
        'total_cost': float,
        'by_operation': {
            'operation_type': {'tokens': int, 'cost': float, 'count': int}
        },
        'daily_usage': [
            {'date': 'YYYY-MM-DD', 'tokens': int, 'cost': float}
        ]
    }
    """
```

**get_article_token_usage()**: Get token usage for specific article
```python
def get_article_token_usage(article_id: str) -> list[dict]:
    """Returns all token usage records for an article."""
```

#### 3. Token Usage Collection Schema (MongoDB)

```json
{
  "_id": "uuid",
  "user_id": "uuid",
  "operation": "dictionary_search | article_generation",
  "model": "string",
  "prompt_tokens": 100,
  "completion_tokens": 50,
  "total_tokens": 150,
  "estimated_cost": 0.00025,
  "article_id": "uuid (optional)",
  "metadata": {
    "query": "...",
    "language": "..."
  },
  "created_at": "datetime"
}
```

**Indexes**:
- `(user_id, created_at)`: User usage queries (descending)
- `article_id`: Article-specific queries (sparse)
- `created_at`: Time-based queries (descending)
- `(operation, created_at)`: Operation-type queries

### Integration

#### Dictionary API (`src/api/routes/dictionary.py`)

```python
@router.post("/search", response_model=SearchResponse)
async def search_word(request: SearchRequest, current_user: User = Depends(get_current_user_required)):
    # Build prompt
    prompt = build_word_definition_prompt(
        language=request.language,
        sentence=request.sentence,
        word=request.word
    )

    # Call LLM with tracking
    content, stats = await call_llm_with_tracking(
        messages=[{"role": "user", "content": prompt}],
        model="gpt-4.1-mini",
        max_tokens=200
    )

    # Log token usage
    logger.info("Token usage for dictionary search", extra=stats.to_dict())

    # Save to database (Phase 2)
    save_token_usage(
        user_id=current_user.id,
        operation="dictionary_search",
        model=stats.model,
        prompt_tokens=stats.prompt_tokens,
        completion_tokens=stats.completion_tokens,
        estimated_cost=stats.estimated_cost,
        metadata={"query": request.word, "language": request.language}
    )

    # Parse and return response
    result = parse_json_from_content(content)
    return SearchResponse(**result)
```

#### Token Usage API Endpoints (`src/api/routes/usage.py`)

**GET /usage/me**: Get current user's token usage summary
```python
@router.get("/me", response_model=TokenUsageSummary)
async def get_my_usage(
    days: int = Query(default=30, ge=1, le=365),
    current_user: User = Depends(get_current_user_required)
):
    # Get aggregated summary from MongoDB
    summary = get_user_token_summary(user_id=current_user.id, days=days)

    # Convert to response models
    by_operation = {
        op_name: OperationUsage(**op_data)
        for op_name, op_data in summary.get('by_operation', {}).items()
    }
    daily_usage = [
        DailyUsage(**day) for day in summary.get('daily_usage', [])
    ]

    return TokenUsageSummary(
        total_tokens=summary.get('total_tokens', 0),
        total_cost=summary.get('total_cost', 0.0),
        by_operation=by_operation,
        daily_usage=daily_usage
    )
```

**GET /usage/articles/{article_id}**: Get token usage for specific article
```python
@router.get("/articles/{article_id}", response_model=list[TokenUsageRecord])
async def get_article_usage(
    article_id: str,
    current_user: User = Depends(get_current_user_required)
):
    # Verify article ownership
    article = get_article(article_id)
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    if article.get('user_id') != current_user.id:
        raise HTTPException(status_code=403, detail="You don't have permission")

    # Get all usage records for article
    usage_records = get_article_token_usage(article_id)

    return [TokenUsageRecord(**record) for record in usage_records]
```

### Token Usage Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant WebUI as Web UI
    participant API as FastAPI
    participant Redis
    participant Worker
    participant CrewAI
    participant LiteLLM
    participant MongoDB
    participant LLM as LLM Provider

    Note over User,LLM: Dictionary Search with Token Tracking
    User->>WebUI: Click word
    WebUI->>API: POST /dictionary/search
    API->>LLM: Call LLM API (via utils/llm.py)
    LLM-->>API: Response + usage stats
    API->>MongoDB: save_token_usage()
    API-->>WebUI: Definition response

    Note over User,MongoDB: Article Generation with Token Tracking
    User->>WebUI: Generate article
    WebUI->>API: POST /articles/generate
    API->>Redis: Enqueue job
    Redis-->>Worker: Dequeue job
    Worker->>Worker: Set litellm.callbacks = [ArticleGenerationTokenTracker]
    Worker->>CrewAI: run_crew()
    CrewAI->>LiteLLM: LLM API calls
    LiteLLM->>LLM: Multiple API calls
    LLM-->>LiteLLM: Responses
    LiteLLM->>Worker: Callback: log_success_event()
    Worker->>MongoDB: save_token_usage() (per LLM call)
    CrewAI-->>Worker: Article result
    Worker->>MongoDB: Save article
    Worker->>Worker: litellm.callbacks = [] (cleanup)

    Note over User,MongoDB: Usage Summary Retrieval
    User->>WebUI: View usage page
    WebUI->>API: GET /usage/me?days=30
    API->>MongoDB: get_user_token_summary()
    MongoDB-->>API: Aggregated summary
    API-->>WebUI: TokenUsageSummary
    WebUI-->>User: Display usage charts

    Note over User,MongoDB: Article Usage Details
    User->>WebUI: View article usage
    WebUI->>API: GET /usage/articles/{id}
    API->>MongoDB: get_article_token_usage()
    MongoDB-->>API: Usage records (all LLM calls)
    API-->>WebUI: List of TokenUsageRecord
    WebUI-->>User: Display usage details
```

### Future Enhancements

**Phase 1** (Completed):
- âœ… LiteLLM integration with token tracking
- âœ… TokenUsageStats dataclass
- âœ… MongoDB storage functions
- âœ… Dictionary API integration with logging

**Phase 2** (Completed):
- âœ… Database storage of token usage records
- âœ… User token summary endpoint
- âœ… Article token usage tracking

**Phase 3** (Completed):
- âœ… API endpoints for token usage (`/usage/me`, `/usage/articles/{id}`)
- âœ… Authentication and authorization for usage endpoints
- âœ… Usage summary with daily breakdown and operation filtering

**Phase 4** (Completed):
- âœ… CrewAI article generation token tracking via LiteLLM callbacks
- âœ… ArticleGenerationTokenTracker class for worker integration
- âœ… Automatic token tracking for authenticated users during article generation

**Phase 5** (Completed):
- âœ… JobTracker coordinator pattern for unified tracking
- âœ… Context manager protocol for automatic setup/cleanup
- âœ… Proper LiteLLM callback lifecycle management
- âœ… Nested context support with callback state preservation

**Phase 6** (Planned):
- Usage analytics dashboard (Frontend)
- Cost alerts and limits
- Per-user billing reports

---

## ğŸ¯ JobTracker Architecture (Phase 5)

### Overview

JobTracker is a coordinator pattern that unifies two parallel tracking systems during article generation:
1. **JobProgressListener** - Tracks CrewAI task progress (task-level events)
2. **ArticleGenerationTokenTracker** - Tracks LLM token usage (API-level callbacks)

### Architecture Diagram

```mermaid
graph TB
    subgraph Worker["Worker Process"]
        ProcessJob[process_job]
        JobTracker[JobTracker Context Manager]

        subgraph Trackers["Tracking Components"]
            ProgressListener[JobProgressListener<br/>CrewAI Events]
            TokenTracker[ArticleGenerationTokenTracker<br/>LiteLLM Callbacks]
        end

        CrewAI[run_crew<br/>Article Generation]
    end

    subgraph Storage["Data Storage"]
        Redis[(Redis<br/>Job Status)]
        MongoDB[(MongoDB<br/>Token Usage)]
    end

    subgraph Events["Event Sources"]
        CrewAIEvents[CrewAI Event Bus<br/>Task Start/Complete]
        LiteLLMCallbacks[LiteLLM Callbacks<br/>API Success/Failure]
    end

    ProcessJob -->|with JobTracker| JobTracker
    JobTracker -->|__enter__<br/>creates| ProgressListener
    JobTracker -->|__enter__<br/>creates| TokenTracker
    JobTracker -->|runs| CrewAI

    CrewAI -->|emits events| CrewAIEvents
    CrewAI -->|makes LLM calls| LiteLLMCallbacks

    CrewAIEvents -->|triggers| ProgressListener
    LiteLLMCallbacks -->|triggers| TokenTracker

    ProgressListener -->|update_job_status| Redis
    TokenTracker -->|save_token_usage| MongoDB

    JobTracker -->|__exit__<br/>cleanup| ProgressListener
    JobTracker -->|__exit__<br/>restore callbacks| TokenTracker

    style JobTracker fill:#4a90e2,stroke:#2563eb,color:#fff
    style ProgressListener fill:#10a37f,stroke:#0d8a6a,color:#fff
    style TokenTracker fill:#10a37f,stroke:#0d8a6a,color:#fff
    style Redis fill:#dc382d,stroke:#a12822,color:#fff
    style MongoDB fill:#13aa52,stroke:#0f8a43,color:#fff
```

### Context Manager Lifecycle

```python
# Worker code
with crewai_event_bus.scoped_handlers():
    with JobTracker(job_id, user_id, article_id) as tracker:
        # 1. __enter__() runs automatically
        #    - Creates JobProgressListener
        #    - Creates ArticleGenerationTokenTracker (if user_id)
        #    - Saves litellm.callbacks to _original_callbacks
        #    - Sets litellm.callbacks = [token_tracker]

        result = run_crew(inputs)

        # 2. During execution
        #    - CrewAI emits task events â†’ JobProgressListener â†’ Redis
        #    - CrewAI makes LLM calls â†’ LiteLLM â†’ TokenTracker â†’ MongoDB

        # 3. Check for failures
        if tracker.listener.task_failed:
            return False

    # 4. __exit__() runs automatically (even on exception)
    #    - Restores litellm.callbacks = _original_callbacks
    #    - Prevents callback leakage between jobs
```

### Key Design Decisions

**Why Context Manager?**
- Automatic setup and cleanup (no manual litellm.callbacks management)
- Exception-safe (cleanup always runs via `__exit__`)
- Pythonic and readable (`with` statement)

**Why Save/Restore Callbacks Instead of Clearing?**
- Supports nested JobTracker contexts (future-proof)
- Prevents accidentally clearing callbacks from outer scope
- More defensive for long-running worker processes

**Why Two Separate Trackers?**
- Different abstraction levels (task-level vs API-level)
- Different storage backends (Redis vs MongoDB)
- Different lifetimes (ephemeral vs permanent)
- Independent failure modes (one can fail without affecting other)

**Why Coordinate Them?**
- Shared lifecycle (both track same job)
- Shared context (job_id, user_id, article_id)
- Single entry point for worker code
- Consistent error handling

### Data Flow Comparison

| Aspect | JobProgressListener | ArticleGenerationTokenTracker |
|--------|---------------------|-------------------------------|
| **Trigger** | CrewAI task events | LLM API calls |
| **Frequency** | ~4-5 times per job | ~5-10 times per job |
| **Granularity** | Coarse (task start/end) | Fine (per API call) |
| **Data** | Task name, progress % | Model, tokens, cost |
| **Storage** | Redis (job status) | MongoDB (usage records) |
| **TTL** | 24 hours (auto-expire) | Permanent |
| **Consumer** | Frontend polling | Analytics/billing |

### Error Handling

**Non-Fatal Tracking**:
- Both trackers catch all exceptions internally
- Failures logged as warnings but never crash worker
- Article generation continues even if tracking fails
- Rationale: Tracking is observability, not critical path

**Cleanup Guarantees**:
```python
def __exit__(self, exc_type, exc_val, exc_tb) -> None:
    """Always runs, even on exception."""
    if self.token_tracker is not None:
        litellm.callbacks = self._original_callbacks  # Always restore
```

### Testing Considerations

**Unit Tests Should Cover**:
- `JobTracker.__enter__()` creates both trackers
- `JobTracker.__exit__()` restores callbacks
- Anonymous users skip token tracking
- Callback state preserved across nested contexts
- Cleanup runs even on exceptions

**Integration Tests Should Cover**:
- Full article generation with both trackers active
- Progress updates appear in Redis
- Token usage records appear in MongoDB
- Multiple jobs don't interfere with each other

---

## ğŸ”§ Worker Token Tracking (Phase 4-5)

### Overview

The Worker service tracks token usage during CrewAI article generation using LiteLLM's callback system. Phase 5 introduced the **JobTracker coordinator pattern** that unifies job progress tracking and token usage tracking into a single context manager with proper lifecycle management.

### Architecture

**Data Flow:**
```
Worker â†’ process_job() â†’ JobTracker.__enter__() â†’ Set litellm.callbacks â†’ run_crew() â†’ CrewAI agents
                                â†“                                                            â†“
                    JobProgressListener (task events)                            Multiple LLM calls
                    ArticleGenerationTokenTracker (token tracking)                         â†“
                                                                            LiteLLM intercepts each call
                                                                                        â†“
                                                            ArticleGenerationTokenTracker.log_success_event()
                                                                                        â†“
                                                                            save_token_usage() â†’ MongoDB
                                                                                        â†“
                                                                    JobTracker.__exit__() â†’ Cleanup
```

### JobTracker Coordinator (Phase 5)

**File**: `src/worker/job_tracker.py`

**Purpose**: Unified coordinator that manages both job progress tracking (CrewAI events) and token usage tracking (LiteLLM callbacks) with proper lifecycle management.

**Architecture**:
```python
class JobTracker:
    """Coordinator for job progress and token tracking during article generation.

    Unifies:
    1. JobProgressListener - Real-time task progress updates via CrewAI events
    2. ArticleGenerationTokenTracker - LLM token usage tracking via LiteLLM callbacks
    """
```

**Key Features**:
- **Context Manager Protocol**: Automatic setup (`__enter__`) and cleanup (`__exit__`)
- **Callback Lifecycle Management**: Saves and restores LiteLLM callbacks for nested contexts
- **Conditional Tracking**: Only tracks tokens for authenticated users (user_id exists)
- **Memory Leak Prevention**: Always clears callbacks in `__exit__`, even on exceptions

**Usage Pattern** (`src/worker/processor.py`):

```python
def process_job(job_data: dict) -> bool:
    ctx = JobContext.from_dict(job_data)

    try:
        from crewai.events.event_bus import crewai_event_bus

        with crewai_event_bus.scoped_handlers():
            with JobTracker(ctx.job_id, ctx.user_id, ctx.article_id) as tracker:
                # Fetch vocabulary for personalized generation
                if ctx.user_id and ctx.inputs.get('language'):
                    vocab = get_user_vocabulary_for_generation(ctx.user_id, ctx.inputs['language'], 50)
                    if vocab:
                        ctx.inputs['vocabulary_list'] = vocab

                # Execute CrewAI (all LLM calls automatically tracked)
                result = run_crew(inputs=ctx.inputs)

                # Check for task failures
                if tracker.listener.task_failed:
                    logger.warning("Task failed during execution")
                    if ctx.article_id:
                        update_article_status(ctx.article_id, 'failed')
                    return False

        # Save to MongoDB
        save_article(ctx.article_id, result.raw, ctx.started_at)
        ctx.update_status('completed', 100, 'Article generated successfully!')
        return True

    except Exception as e:
        logger.error(f"Job failed: {e}")
        ctx.mark_failed(translate_error(e), f"{type(e).__name__}: {str(e)[:200]}")
        return False
```

**Lifecycle**:

1. **`__enter__()`** (lines 80-121):
   - Creates `JobProgressListener` for CrewAI task progress events
   - Creates `ArticleGenerationTokenTracker` for token tracking (if user_id exists)
   - Saves original `litellm.callbacks` to `_original_callbacks`
   - Sets `litellm.callbacks = [tracker]` to enable token tracking
   - Returns self for use in context manager

2. **`__exit__()`** (lines 123-150):
   - Restores original `litellm.callbacks` from `_original_callbacks`
   - Prevents callback interference between jobs
   - Always executes, even on exceptions (cleanup guaranteed)

**Benefits**:
- **Single Responsibility**: Each tracker handles one concern (progress vs tokens)
- **Proper Cleanup**: Context manager ensures callbacks never leak between jobs
- **Nested Context Support**: Saves/restores callbacks for nested JobTracker usage
- **Fail-Safe**: Cleanup happens even on exceptions

### ArticleGenerationTokenTracker Class

**File**: `src/worker/token_tracker.py`

**Purpose**: LiteLLM callback handler that intercepts all LLM API calls during article generation and records token usage to MongoDB.

**Key Methods**:

1. `log_success_event(kwargs, response_obj, start_time, end_time)` (lines 67-157)
   - Called by LiteLLM after each successful LLM API call
   - Extracts model name, prompt_tokens, completion_tokens from response
   - Calculates estimated cost using `litellm.completion_cost()`
   - Saves token usage to MongoDB via `save_token_usage()`
   - Never crashes worker on tracking failures (catch-all exception handler)

2. `log_failure_event(kwargs, response_obj, start_time, end_time)` (lines 179-216)
   - Called when LLM API call fails
   - Logs failure for observability (does not save to MongoDB)
   - Truncates error messages to 200 chars to prevent log bloat

**Integration**: Now managed by `JobTracker` coordinator (see above)

### How LiteLLM Callbacks Work

**LiteLLM Integration**:
- LiteLLM provides a `CustomLogger` base class for callback handlers
- When `litellm.callbacks = [tracker]` is set, all LLM API calls are intercepted
- CrewAI agents use LiteLLM internally, so all agent LLM calls are tracked
- Callbacks are invoked automatically:
  - `log_success_event()` on successful API call
  - `log_failure_event()` on failed API call

**Per-Call Tracking**:
- Each LLM API call during article generation generates a separate token usage record
- An article generation may involve 5-10 LLM calls (research, writing, editing, etc.)
- Total article cost = sum of all individual call costs
- All records share the same `article_id` for aggregation

**Callback Lifecycle with JobTracker**:
```
1. Worker starts job
2. JobTracker.__enter__():
   - Save original callbacks: _original_callbacks = litellm.callbacks.copy()
   - Register new callback: litellm.callbacks = [tracker]
3. CrewAI agent 1 makes LLM call â†’ tracker.log_success_event() â†’ save to MongoDB
4. CrewAI agent 2 makes LLM call â†’ tracker.log_success_event() â†’ save to MongoDB
5. ... (multiple calls)
6. CrewAI completes
7. JobTracker.__exit__():
   - Restore callbacks: litellm.callbacks = _original_callbacks
   - Prevents interference with next job or nested JobTracker
```

**Why Save/Restore Callbacks?**
- Supports nested JobTracker contexts (if needed in future)
- Prevents accidentally clearing callbacks set by outer scope
- More robust than simply setting to empty list
- Defensive programming for long-running worker processes

### Token Usage Record Structure

**Saved to MongoDB** (via `save_token_usage()`):
```json
{
  "_id": "usage-uuid",
  "user_id": "user-uuid",
  "operation": "article_generation",
  "model": "gpt-4.1-mini",
  "prompt_tokens": 2000,
  "completion_tokens": 1500,
  "total_tokens": 3500,
  "estimated_cost": 0.0525,
  "article_id": "article-uuid",
  "metadata": {
    "job_id": "job-uuid"
  },
  "created_at": "2026-01-30T10:00:00Z"
}
```

**Multiple Records Per Article**:
- Each LLM call during generation creates a separate record
- Aggregation query sums all records with same `article_id`
- Example: Research agent (500 tokens) + Writing agent (3000 tokens) + Editing agent (800 tokens) = 4300 total tokens

### Error Handling

**Non-Fatal Failures**:
- Token tracking failures never crash the worker
- All exceptions caught and logged as warnings
- Article generation continues even if tracking fails
- Rationale: Tracking is observability, not critical functionality

**Cost Calculation Fallback**:
- If `litellm.completion_cost()` fails, cost is set to 0.0
- Tokens are still tracked (tokens always available in response)
- Allows cost calculation to be updated retroactively from token counts

### Anonymous User Handling

**Conditional Tracking** (`src/worker/processor.py:54-63`):
```python
if ctx.user_id:
    token_tracker = ArticleGenerationTokenTracker(...)
    litellm.callbacks = [token_tracker]
```

- Only tracks for authenticated users (user_id exists)
- Anonymous users (no user_id) skip token tracking entirely
- Prevents null user_id records in MongoDB
- Aligns with dictionary API behavior (requires authentication)

### Security Considerations

**User ID Validation**:
- `user_id` passed from job queue (validated during job creation in API)
- Worker trusts job queue data (internal system boundary)
- No additional validation needed in worker

**Memory Leaks Prevention**:
- `finally` block always clears `litellm.callbacks = []`
- Prevents callbacks from affecting subsequent jobs
- Critical in long-running worker processes

### Testing

**Test File**: `src/worker/tests/test_token_tracker.py`

**Coverage**:
- Callback registration and cleanup
- Successful LLM call tracking
- Failed LLM call handling
- Cost calculation fallback
- MongoDB save failures (non-fatal)

### Retrieval API

**Get Article Usage**:
```python
# API endpoint: GET /usage/articles/{article_id}
records = get_article_token_usage(article_id)
total_cost = sum(r['estimated_cost'] for r in records)
```

**Returns**:
- All token usage records for article (multiple LLM calls)
- Sorted by `created_at` ascending (chronological order)
- Includes metadata with `job_id` for traceability

---

## ğŸ“š Vocabulary-Aware Article Generation

### Overview
The system now supports vocabulary-aware article generation, where CrewAI adjusts content difficulty based on words the user has already learned.

### Vocabulary Features

#### 1. Dictionary API - Word Definition
- **POST /dictionary/search**: Get word definition and lemma using OpenAI API
- **Returns**: lemma, definition, and related_words (for complex structures like separable verbs)
- **Auth**: Required (JWT) to prevent API abuse

#### 2. Vocabulary Storage
- **POST /dictionary/vocabulary**: Add a word to user's vocabulary
- **GET /dictionary/vocabularies**: Get aggregated vocabulary grouped by lemma with counts
- **DELETE /dictionary/vocabularies/{id}**: Delete a vocabulary word
- **Auth**: All vocabulary operations require authentication and are user-specific

#### 3. Article-Specific Vocabularies
- **GET /articles/{article_id}/vocabularies**: Get all vocabularies for a specific article
- **Response**: List of VocabularyResponse objects with word, lemma, definition, context, and metadata
- **Auth**: Users can only access vocabularies from their own articles

### Data Model

#### Vocabulary Collection (MongoDB)
```json
{
  "_id": "ObjectId",
  "article_id": "uuid",
  "user_id": "uuid",
  "word": "string",              // Original word clicked
  "lemma": "string",             // Dictionary form
  "definition": "string",        // Word definition
  "sentence": "string",          // Sentence context
  "language": "string",
  "related_words": ["string"],   // All forms in sentence (e.g., verbs with particles)
  "span_id": "string",           // Span ID from markdown for linking
  "created_at": "datetime",
  "pos": "string",               // Part of speech (noun, verb, adjective, etc.)
  "gender": "string",            // Grammatical gender (der/die/das for German, le/la for French, etc.)
  "conjugations": {              // Verb conjugations (null for non-verbs)
    "present": "string",
    "past": "string",
    "perfect": "string"
  },
  "level": "string"              // CEFR level (A1, A2, B1, B2, C1, C2)
}
```

**New Grammatical Metadata Fields:**
- `pos`: Part of speech classification (noun, verb, adjective, adverb, preposition, etc.)
- `gender`: Grammatical gender for nouns in gendered languages (German: der/die/das, French: le/la, Spanish: el/la). Null for non-gendered languages.
- `conjugations`: Verb conjugation forms across tenses (present, past, perfect). Null for non-verbs.
- `level`: CEFR difficulty level (A1-C2) for vocabulary tracking and adaptive learning.

#### VocabularyCount Model (Aggregated Response)
- Groups vocabularies by lemma
- Returns count of how many times a lemma appears across articles
- Includes most recent definition and example sentence
- Lists all article_ids where lemma appears
- Includes grammatical metadata (pos, gender, conjugations, level) from most recent entry

#### API Model Enhancements

**Conjugations.__bool__()** (`src/api/models.py:18-20`):
- Enables truthiness checking: `if conjugations:` returns False when all fields (present, past, perfect) are None
- Simplifies validation logic by treating empty Conjugations as falsy
- Backend can check conjugation presence without explicit null checks

**VocabularyRequest.field_validator** (`src/api/models.py:106-116`):
- Automatic conversion from Conjugations model to dict before database storage
- Returns None if conjugations object is empty (using `__bool__` check)
- Handles both dict and Conjugations input types
- Prevents storing empty conjugation objects in MongoDB

### Vocabulary-Aware Generation Flow
1. User saves vocabulary words from articles (POST /dictionary/vocabulary)
2. Words stored with article context (sentence, span_id)
3. When generating new article, worker retrieves user's vocabulary list
4. CrewAI receives vocabulary list as constraint for content difficulty
5. Generated article uses different words/complexity for learned vocabulary
6. User can access article-specific vocabularies (GET /articles/{id}/vocabularies)

### Authentication
All vocabulary endpoints require JWT authentication. Users can only:
- Add/delete their own vocabulary
- View their own vocabulary lists
- Access vocabularies from their own articles

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
opad/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # API ì„œë¹„ìŠ¤ (FastAPI)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py       # FastAPI ì•± ì§„ì…ì 
â”‚   â”‚   â”œâ”€â”€ models.py     # Pydantic ëª¨ë¸ (Article, Job)
â”‚   â”‚   â”œâ”€â”€ routes/       # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ articles.py
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints.py
â”‚   â”‚   â”‚   â”œâ”€â”€ stats.py
â”‚   â”‚   â”‚   â””â”€â”€ dictionary.py  # Dictionary API (word definition)
â”‚   â”‚   â””â”€â”€ job_queue.py  # Redis í ê´€ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ worker/           # Worker ì„œë¹„ìŠ¤ (Python)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py       # Worker ì§„ì…ì 
â”‚   â”‚   â”œâ”€â”€ processor.py  # Job ì²˜ë¦¬ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ job_tracker.py      # JobTracker coordinator (Phase 5)
â”‚   â”‚   â”œâ”€â”€ token_tracker.py    # ArticleGenerationTokenTracker
â”‚   â”‚   â””â”€â”€ context.py    # JobContext helper
â”‚   â”‚
â”‚   â”œâ”€â”€ web/              # Web ì„œë¹„ìŠ¤ (Next.js)
â”‚   â”‚   â”œâ”€â”€ app/          # Next.js App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ api/      # API Routes (í”„ë¡ì‹œ)
â”‚   â”‚   â”‚   â”œâ”€â”€ articles/ # Article pages
â”‚   â”‚   â”‚   â”œâ”€â”€ vocabulary/ # Vocabulary pages
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx  # ë©”ì¸ í˜ì´ì§€
â”‚   â”‚   â”œâ”€â”€ components/   # React ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ ArticleCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EmptyState.tsx      # Reusable empty state
â”‚   â”‚   â”‚   â”œâ”€â”€ ErrorAlert.tsx      # Reusable error alert
â”‚   â”‚   â”‚   â”œâ”€â”€ MarkdownViewer.tsx
â”‚   â”‚   â”‚   â””â”€â”€ VocabularyList.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/        # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useAsyncFetch.ts    # Generic fetch with loading/error
â”‚   â”‚   â”‚   â”œâ”€â”€ usePagination.ts    # Pagination calculations
â”‚   â”‚   â”‚   â”œâ”€â”€ useStatusPolling.ts # Job status polling
â”‚   â”‚   â”‚   â””â”€â”€ useVocabularyDelete.ts # Vocabulary deletion
â”‚   â”‚   â”œâ”€â”€ lib/          # Frontend utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts           # fetchWithAuth, parseErrorResponse
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts          # Auth utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ formatters.ts    # Date formatting utilities
â”‚   â”‚   â”‚   â””â”€â”€ styleHelpers.ts  # CEFR color/label helpers
â”‚   â”‚   â”œâ”€â”€ types/        # TypeScript type definitions
â”‚   â”‚   â”œâ”€â”€ tailwind.config.ts # Tailwind config with safelist
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”‚
â”‚   â”œâ”€â”€ opad/             # CrewAI ë¡œì§ (ê³µìœ )
â”‚   â”‚   â”œâ”€â”€ crew.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/            # ê³µí†µ ìœ í‹¸ë¦¬í‹° (ê³µìœ )
â”‚       â”œâ”€â”€ mongodb.py    # MongoDB ì—°ê²° ë° ì‘ì—…
â”‚       â”œâ”€â”€ logging.py    # Structured logging ì„¤ì •
â”‚       â”œâ”€â”€ llm.py        # OpenAI API ê³µí†µ í•¨ìˆ˜
â”‚       â””â”€â”€ prompts.py    # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚
â””â”€â”€ Dockerfile.*          # ì„œë¹„ìŠ¤ë³„ Dockerfile (ì´ìŠˆ #9)
```

### ì„œë¹„ìŠ¤ êµ¬ë¶„
| í´ë” | ì—­í•  | ëŸ°íƒ€ì„ | í¬íŠ¸ |
|------|------|--------|------|
| `src/api/` | CRUD + Job enqueue + Dictionary API | Python (FastAPI) | 8001 (default) |
| `src/worker/` | CrewAI ì‹¤í–‰ + Job/Token Tracking | Python | - |
| `src/web/` | UI | Node.js (Next.js) | 3000 |
| `src/crew/` | CrewAI ë¡œì§ (ê³µìœ ) | - | - |
| `src/utils/` | ê³µí†µ ìœ í‹¸ (ê³µìœ ) | - | - |

### Worker ëª¨ë“ˆ êµ¬ì„± (Phase 5)
| íŒŒì¼ | ì—­í•  | ì˜ì¡´ì„± |
|------|------|--------|
| `worker/main.py` | Worker ì§„ì…ì  (infinite loop) | `processor.py` |
| `worker/processor.py` | Job ì²˜ë¦¬ ë¡œì§ (process_job) | `job_tracker.py`, `crew/main.py` |
| `worker/job_tracker.py` | JobTracker coordinator (context manager) | `token_tracker.py`, `crew/progress_listener.py` |
| `worker/token_tracker.py` | ArticleGenerationTokenTracker (LiteLLM callback) | `utils/mongodb.py` |
| `worker/context.py` | JobContext helper (job data validation) | `api/job_queue.py` |
| `crew/progress_listener.py` | JobProgressListener (CrewAI event listener) | `api/job_queue.py` |

---

## ğŸ”‘ ê³µí†µ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

### LLM ìœ í‹¸ë¦¬í‹° (`utils/llm.py`)
Provider-agnostic LLM API í˜¸ì¶œ ë° í† í° ì¶”ì ì„ ìœ„í•œ ê³µí†µ í•¨ìˆ˜ë“¤ (LiteLLM ê¸°ë°˜):

- **`call_llm_with_tracking()`**: LLM API í˜¸ì¶œ + í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (ë²”ìš© í•¨ìˆ˜)
  - ë°˜í™˜ê°’: `(content: str, stats: TokenUsageStats)`
  - OpenAI, Anthropic, Google ë“± ë‹¤ì–‘í•œ í”„ë¡œë°”ì´ë” ì§€ì›
  - ìë™ ë¹„ìš© ê³„ì‚° (LiteLLM ë‚´ì¥ ê°€ê²© ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
- **`TokenUsageStats`**: í† í° ì‚¬ìš©ëŸ‰ í†µê³„ dataclass
  - í•„ë“œ: `model`, `prompt_tokens`, `completion_tokens`, `total_tokens`, `estimated_cost`, `provider`
- **`parse_json_from_content()`**: LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹± (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
  - ì¼ë°˜ JSON, ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ (```json), í…ìŠ¤íŠ¸ ë‚´ JSON ì¶”ì¶œ ì§€ì›
- **`get_llm_error_response()`**: LLM ê´€ë ¨ ì˜ˆì™¸ë¥¼ HTTP ìƒíƒœ ì½”ë“œë¡œ ë³€í™˜

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from utils.llm import call_llm_with_tracking, parse_json_from_content

# LLM í˜¸ì¶œ + í† í° ì¶”ì 
content, stats = await call_llm_with_tracking(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4.1-mini",
    max_tokens=200
)

# í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
logger.info("Token usage", extra=stats.to_dict())

# JSON íŒŒì‹±
result = parse_json_from_content(content)
```

**ì§€ì› í”„ë¡œë°”ì´ë”** (LiteLLM):
- OpenAI: `"gpt-4.1-mini"`, `"gpt-4.1"`
- Anthropic: `"anthropic/claude-4.5-sonnet"`
- Google: `"gemini/gemini-2.0-flash"`

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (`utils/prompts.py`)
ì¬ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡¬í”„íŠ¸ ë¹Œë” í•¨ìˆ˜ë“¤:

- **`build_word_definition_prompt()`**: Dictionary APIìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (lemma ë° definition ì¶”ì¶œ)

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from utils.prompts import build_word_definition_prompt

prompt = build_word_definition_prompt(
    language="German",
    sentence="Diese groÃŸe Spanne hÃ¤ngt von mehreren Faktoren ab.",
    word="hÃ¤ngt"
)
```

---

## ğŸ“¡ Dynamic Endpoint Discovery

### Tag-Based Endpoint Grouping

The `/endpoints` endpoint dynamically generates an HTML page listing all registered API routes, grouped by their tags. This system requires no code changes when new routes are addedâ€”they automatically appear in the listing.

**Endpoint**: `GET /endpoints`

**File**: `src/api/routes/endpoints.py`

**How It Works**:

1. **Route Introspection**: Scans `app.routes` to collect all routes with methods and paths
2. **Tag Extraction**: Reads tags from route definitions (e.g., `tags=["articles"]`, `tags=["usage"]`)
3. **Dynamic Grouping**: Groups endpoints by tag using `group_endpoints_by_tag()`
4. **HTML Generation**: Renders grouped endpoints as styled HTML page

**Helper Functions**:

- **`group_endpoints_by_tag(endpoints)`**: Groups endpoints by tag (first tag if multiple), returns `dict[tag, list[endpoints]]`
  - Uses `defaultdict(list)` to avoid KeyError
  - Filters out `EXCLUDED_TAGS` (e.g., "meta")
  - Sorts endpoints within each group by `(path, method)`
  - Assigns "other" tag if endpoint has no tags

- **`get_sorted_tags(grouped)`**: Returns alphabetically sorted tag list with "other" at end

- **`format_endpoint(ep)`**: Formats single endpoint as HTML with method badge, path, and summary

- **`format_tag_title(tag)`**: Converts tag name to display title (e.g., "articles" â†’ "Articles Endpoints")

**Configuration**:

```python
# src/api/routes/endpoints.py:13-14
EXCLUDED_TAGS = {"meta"}  # Tags to hide from listing
EXCLUDED_PATHS = {"/docs", "/openapi.json", "/redoc"}  # Paths to skip
```

**Example Route Definition**:

```python
# src/api/routes/usage.py
router = APIRouter(tags=["usage"])  # Tag used for grouping

@router.get("/me", summary="Get current user's token usage summary")
async def get_my_usage(...):
    """Detailed description..."""
```

**Benefits**:

- **Zero-maintenance**: New routes automatically appear in listing
- **Tag-based organization**: Routes grouped by domain (articles, usage, dictionary)
- **Clean HTML output**: Color-coded HTTP methods (GET=blue, POST=green, DELETE=red)
- **No hardcoding**: Eliminates manual endpoint lists

**Usage**:
Visit `/endpoints` in browser to see all available API routes grouped by tag.

---

## ğŸ“¡ Dictionary API

### Word Definition Endpoint

**Endpoint**: `POST /dictionary/search`

**ëª©ì **: ë¬¸ì¥ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ì˜ lemma, ì •ì˜ ë° ë¬¸ë²•ì  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ

**ìš”ì²­:**
```json
{
  "word": "hÃ¤ngt",
  "sentence": "Diese groÃŸe Spanne hÃ¤ngt von mehreren Faktoren ab.",
  "language": "German"
}
```

**ì‘ë‹µ:**
```json
{
  "lemma": "abhÃ¤ngen",
  "definition": "ì˜ì¡´í•˜ë‹¤, ~ì— ë‹¬ë ¤ìˆë‹¤",
  "related_words": ["hÃ¤ngt", "ab"],
  "pos": "verb",
  "gender": null,
  "conjugations": {
    "present": "hÃ¤ngt ab",
    "past": "hing ab",
    "perfect": "hat abgehangen"
  },
  "level": "B1"
}
```

**íŠ¹ì§•:**
- **ë¶„ë¦¬ë™ì‚¬ ì²˜ë¦¬**: ë…ì¼ì–´ ë“±ì—ì„œ ë™ì‚¬ê°€ ë¶„ë¦¬ëœ ê²½ìš° ì „ì²´ lemma ë°˜í™˜ (ì˜ˆ: `hÃ¤ngt ... ab` â†’ `abhÃ¤ngen`)
- **ë³µí•©ì–´ ì²˜ë¦¬**: ë‹¨ì–´ê°€ ë³µí•©ì–´ì˜ ì¼ë¶€ì¸ ê²½ìš° ì „ì²´ í˜•íƒœ ë°˜í™˜
- **related_words**: ë¬¸ì¥ì—ì„œ ê°™ì€ lemmaì— ì†í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ë“¤ì„ ë°°ì—´ë¡œ ë°˜í™˜ (ì˜ˆ: ë¶„ë¦¬ ë™ì‚¬ì˜ ê²½ìš° ëª¨ë“  ë¶€ë¶„ í¬í•¨)
- **ë¬¸ë²•ì  ë©”íƒ€ë°ì´í„°**: í’ˆì‚¬(pos), ì„±(gender), ë™ì‚¬ í™œìš©í˜•(conjugations), CEFR ë ˆë²¨(level) ìë™ ì¶”ì¶œ
- **ê³µí†µ ìœ í‹¸ ì‚¬ìš©**: `utils/llm.py`ì˜ `call_openai_chat()` í•¨ìˆ˜ í™œìš©
- **í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬**: `utils/prompts.py`ì˜ `build_word_definition_prompt()` ì‚¬ìš©
- **ë³´ì•ˆ**: Regex injection ë°©ì§€ë¥¼ ìœ„í•œ `re.escape()` ì ìš©

**íë¦„:**

```mermaid
sequenceDiagram
    participant Frontend as Frontend<br/>(MarkdownViewer)
    participant NextAPI as Next.js API<br/>(/api/dictionary/search)
    participant FastAPI as FastAPI<br/>(/dictionary/search)
    participant Utils as Utils<br/>(prompts.py + llm.py)
    participant OpenAI as OpenAI API

    Frontend->>NextAPI: POST /api/dictionary/search<br/>{word, sentence, language}
    NextAPI->>FastAPI: POST /dictionary/search<br/>{word, sentence, language}

    FastAPI->>Utils: build_word_definition_prompt()
    Utils-->>FastAPI: prompt string

    FastAPI->>Utils: call_openai_chat(prompt)
    Utils->>OpenAI: POST /v1/chat/completions
    OpenAI-->>Utils: {lemma, definition, related_words, pos, gender, conjugations, level}
    Utils->>Utils: parse_json_from_content()
    Utils-->>FastAPI: {lemma, definition, related_words, pos, gender, conjugations, level}

    FastAPI-->>NextAPI: SearchResponse<br/>{lemma, definition, related_words, pos, gender, conjugations, level}
    NextAPI-->>Frontend: {lemma, definition, related_words, pos, gender, conjugations, level}
```

---

## ğŸ¨ Frontend Architecture

### Code Organization

The frontend follows a modular architecture with clear separation of concerns:

```
src/web/
â”œâ”€â”€ app/              # Next.js App Router (pages)
â”œâ”€â”€ components/       # Reusable React components
â”œâ”€â”€ hooks/            # Custom React hooks
â”œâ”€â”€ lib/              # Utility functions
â””â”€â”€ types/            # TypeScript type definitions
```

### Utility Modules

#### API Client (`lib/api.ts`)
Centralized API client utilities for consistent request handling:
- `fetchWithAuth()`: Automatic JWT token injection
- `parseErrorResponse()`: Consistent error message extraction

**Benefits**:
- DRY principle: Authentication logic in one place
- Consistent error handling across all API calls
- Easy to add global request interceptors

#### Date Formatters (`lib/formatters.ts`)
Reusable date formatting functions using Intl.DateTimeFormat:
- `formatDate()`: Customizable date formatting
- `formatDateShort()`: Short date format
- `formatDateTime()`: Date with time

**Benefits**:
- Consistent date display across UI
- Locale-aware formatting
- Single source of truth for date formats

#### Style Helpers (`lib/styleHelpers.ts`)
CEFR level badge styling utilities:
- `getLevelColor()`: Tailwind classes for level badges
- `getLevelLabel()`: Human-readable level labels

**Benefits**:
- Consistent color scheme across UI
- Dynamic class generation for Tailwind
- Easy to update color scheme globally

**Important**: CEFR level colors are safelisted in `tailwind.config.ts` to prevent Tailwind's tree-shaking from removing dynamically-generated classes.

### Custom Hooks

#### useAsyncFetch
Generic hook for async data fetching with automatic state management:
- Loading state
- Error handling
- Automatic 401 redirect
- Type-safe data state

**Use Cases**:
- Fetching article lists
- Loading article details
- Any API data fetching

#### usePagination
Pagination calculations and state management:
- Current page calculation
- Total pages calculation
- Next/previous page navigation
- Skip value computation

**Use Cases**:
- Article list pagination
- Vocabulary list pagination

#### useStatusPolling
Job status polling with automatic interval management:
- Configurable polling interval
- Automatic cleanup on completion/error
- Progress state management
- Callbacks for status changes

**Use Cases**:
- Article generation progress tracking
- Any long-running job monitoring

#### useVocabularyDelete
Vocabulary deletion with error handling:
- DELETE request to API
- Detailed error messages
- Throws errors for caller to handle

**Use Cases**:
- Deleting vocabulary entries from vocabulary list

### Reusable Components

#### ErrorAlert (`src/web/components/ErrorAlert.tsx`)
Consistent error message display:
- Red background with border (`bg-red-50 border-red-200`)
- Optional retry button with hover effect
- Automatic hiding when error is null
- Accessible error messaging

**Props**:
- `error` (string | null): Error message to display
- `onRetry` (optional): Callback function for retry button
- `className` (optional): Additional CSS classes

#### EmptyState (`src/web/components/EmptyState.tsx`)
Consistent empty state display:
- Centered layout with white card background
- Optional icon (emoji or Unicode character)
- Optional action button with blue styling
- Title and description text with gray tones

**Props**:
- `title` (string): Main heading text
- `description` (string): Descriptive subtitle
- `icon` (optional): Emoji or icon character
- `action` (optional): Object with `label` and `onClick` for action button
- `className` (optional): Additional CSS classes

**Benefits**:
- Consistent UX across all pages
- Reduces code duplication
- Easy to update design globally
- Improves maintainability with single source of truth

### Refactoring Impact

**Before Refactoring**:
- Duplicate fetch logic in every page
- Inconsistent error handling
- Duplicate date formatting code
- Duplicate pagination calculations
- Duplicate empty state styling

**After Refactoring**:
- Single source of truth for common operations
- Consistent error handling with `useAsyncFetch`
- Reusable date formatters
- Reusable pagination hook
- Reusable UI components (`ErrorAlert`, `EmptyState`)

**Code Reduction**:
- Article detail page: 166 lines reduced
- Articles list page: 27 lines reduced
- Vocabulary page: 43 lines reduced
- Total: 236 lines of code removed through refactoring

### Security Improvements

#### XSS Prevention in MarkdownViewer (`src/web/components/MarkdownViewer.tsx`)

**Issue**: Previous implementation used `innerHTML` to inject vocabulary buttons, creating XSS vulnerability.

**Security Measures Implemented**:

1. **HTML Escaping** (lines 92-96):
   - `escapeHtml()` utility converts text to DOM text node then reads innerHTML
   - Prevents script injection in user-provided content (word, lemma, definition, sentence)
   - Applied to all vocabulary data before rendering

2. **DOM API Methods** (lines 663-713):
   - Replaced `innerHTML` with DOM manipulation (`createElement`, `textContent`)
   - Creates definition spans using `document.createElement()` instead of string templates
   - Uses `textContent` for user data instead of `innerHTML`
   - Parses button HTML in temporary container, then extracts element reference

3. **Event Delegation** (lines 514-534):
   - Single event listener on container instead of per-word listeners
   - Prevents stale closure issues with dynamic content
   - Ref-based callback storage (`handleWordClickRef`) avoids outdated state

4. **Data Attribute Escaping** (lines 112-131):
   - All data attributes escaped before setting: `data-word="${wordEscaped}"`
   - JSON strings escaped with `.replace(/"/g, '&quot;')` for attribute safety
   - Prevents attribute injection attacks

**Before (Vulnerable)**:
```typescript
// Direct innerHTML injection - XSS risk
defSpan.innerHTML = `<strong>${lemma}</strong>: ${meaning} <button>...</button>`
```

**After (Secure)**:
```typescript
// DOM API - XSS safe
const strong = document.createElement('strong')
strong.textContent = displayLemma
defSpan.appendChild(strong)
defSpan.appendChild(document.createTextNode(': ' + meaning))
```

**Impact**: Prevents malicious script execution from vocabulary data, protects against DOM-based XSS attacks.

#### React Component Remounting Pattern

**Purpose**: Prevent React hydration mismatches when article content changes.

**Implementation** (`src/web/app/articles/[id]/page.tsx:266`):
```typescript
<MarkdownViewer
  key={`${articleId}-${content.length}`}
  content={content}
  language={article?.language}
  articleId={articleId}
  vocabularies={vocabularies}
  onAddVocabulary={handleAddVocabulary}
/>
```

**Key Prop Strategy**:
- Pattern: `${articleId}-${content.length}`
- Forces complete component remount when content changes
- Triggers reset of `data-processed` attribute (line 456)
- Clears all previous DOM state and event listeners

**Processing State Check** (`src/web/components/MarkdownViewer.tsx:456-458`):
```typescript
// Skip if already processed (component remounts on content change via key prop)
if (containerRef.current.getAttribute('data-processed') === 'true') {
  return
}
```

**Why This Pattern**:
- Without key: React reuses DOM nodes, causing hydration mismatches
- Manual cleanup: Error-prone and complex to maintain
- useEffect on content: Risk of double-processing
- Remounting: Clean state guaranteed, simple lifecycle

**Benefits**:
- Eliminates React DOM mismatch errors
- Prevents stale event listeners
- Simplifies component update logic
- Ensures consistent behavior across content changes

### Bug Fixes

#### 1. Conjugations Type Conversion
**Issue**: Frontend expected conjugations as object, but backend returned null for non-verbs, causing type mismatches.

**Fix**: Ensure conjugations field is properly typed and handled as nullable in TypeScript types.

#### 2. Tailwind Safelist for Dynamic Classes
**Issue**: CEFR level color classes (generated dynamically by `getLevelColor()`) were being purged by Tailwind's tree-shaking.

**Fix**: Added safelist to `tailwind.config.ts` to preserve dynamic color classes:
```typescript
safelist: [
  'bg-gray-100', 'text-gray-600',   // Unknown level
  'bg-green-100', 'text-green-700', // A1-A2
  'bg-yellow-100', 'text-yellow-700', // B1-B2
  'bg-red-100', 'text-red-700',     // C1-C2
]
```

**Impact**: CEFR level badges now display correctly with proper colors

### Vocabulary Management Endpoints

**Endpoints:**
- `POST /dictionary/vocabularies` - Add vocabulary
- `GET /dictionary/vocabularies` - Get vocabulary list (optionally filtered by article_id)
- `DELETE /dictionary/vocabularies/{id}` - Delete vocabulary
- `GET /dictionary/stats` - Get vocabulary statistics (word counts by language)

**Vocabulary ì €ì¥:**
- MongoDB `vocabularies` ì»¬ë ‰ì…˜ì— ì €ì¥
- `related_words` ë°°ì—´ í¬í•¨ (ë¶„ë¦¬ ë™ì‚¬ ë“± ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡° ì§€ì›)
- Articleë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê´€ë¦¬

**Vocabulary í‘œì‹œ:**
- ì €ì¥ëœ ë‹¨ì–´ëŠ” ì´ˆë¡ìƒ‰ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸
- `related_words`ì— í¬í•¨ëœ ë‹¨ì–´ë“¤ë„ í•¨ê»˜ ì´ˆë¡ìƒ‰ í‘œì‹œ
- ì˜ˆ: "hÃ¤ngt" ì €ì¥ ì‹œ "ab"ë„ ìë™ìœ¼ë¡œ ì´ˆë¡ìƒ‰ í‘œì‹œ

---

## ğŸ§ª Testing Infrastructure

### Web Testing (Vitest)

**Configuration**: `src/web/vitest.config.ts`

**Test Environment**:
- **Framework**: Vitest 4.0.18 with jsdom for DOM simulation
- **UI**: @vitest/ui for interactive test running
- **Testing Library**: @testing-library/react for component testing
- **Test Matchers**: @testing-library/jest-dom for DOM assertions

**Coverage Settings**:
- Provider: v8 (Node.js native coverage)
- Reporters: text, json, html
- Thresholds: 80% for lines, functions, branches, statements
- Excludes: node_modules, test files, test directories

**Test Location**:
- Pattern: `**/__tests__/**/*.test.ts` and `**/__tests__/**/*.test.tsx`
- Current test files:
  - `hooks/__tests__/usePagination.test.ts`
  - `hooks/__tests__/useStatusPolling.test.ts`
  - `lib/__tests__/api.test.ts`
  - `lib/__tests__/formatters.test.ts`
  - `lib/__tests__/styleHelpers.test.ts`

**Run Commands** (`src/web/package.json`):
```bash
npm test         # Run all tests once
npm run test:watch   # Watch mode for development
npm run test:ui      # Interactive UI for test exploration
```

**Alias Resolution**:
- `@` alias resolves to `src/web/` directory
- Matches Next.js path configuration for consistency

**Benefits**:
- Fast test execution with Vitest (ESM-native)
- Interactive UI for debugging
- Coverage reporting for quality assurance
- Type-safe testing with TypeScript
